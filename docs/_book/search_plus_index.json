{"./":{"url":"./","title":"MLSQL Stack","keywords":"","body":"MLSQL Stack 使用入门 MLSQL Console是MLSQL Stack 套件中非常重要的一款直接面向普通用户的产品。 MLSQL Console支持无编码所见即所得的交互性数据分析，同时也提供了面向大数据和AI的语言MLSQL的支持，拥有诸如脚本管理，代码补全，执行等功能。 项目官网 MLSQL官网 不会SQL的看这里 分析工坊文档地址 会SQL的看这里 控制台文档地址 欢迎捐赠 我们欢迎企业和个人对项目进行捐赠，用以维持服务器等费用。            该文件修订时间： 2021-01-12 10:27:46 "},"release-notes/":{"url":"release-notes/","title":"所有版本","keywords":"","body":"所有版本 本节会介绍MLSQL Stack的： 版本发布策略 每个版本发布信息            该文件修订时间： 2021-01-12 10:27:47 "},"release-notes/version.html":{"url":"release-notes/version.html","title":"MLSQL Stack版本管理策略","keywords":"","body":"MLSQL Stack版本管理策略 从2.0.0版本开始，我们对MLSQL Console/MLSQL Engine的版本策略进行了修改。 版本规划 Bug修复都会在小版本里增加。比如2.0.1,2.0.2 新特性都会在中间版本增加， 如2.1.0,2.2.0 Console/Engine版本兼容规则 大版本之间互不兼容。但是在小版本中，MLSQL Console高版本会兼容MLSQL Engine 低版本。比如 Console 2.0.2 会兼容 Engine 2.0.0/2.0.1, 新特性版本（通常为大版本），一定会同步发布。比如Console 2.1.0 发布，那么Engine也一定会发布2.1.0。            该文件修订时间： 2021-01-12 10:27:47 "},"release-notes/2.0.1.html":{"url":"release-notes/2.0.1.html","title":"MLSQL Stack 2.0.1版发布","keywords":"","body":"MLSQL Stack 2.0.1 发布 MLSQL Stack 2.0.0发布后，重点修正累计的bug，同时添加一些小feature。 MLSQL Console 累计6个bug修正 MLSQL Engine 累计8个bug修正以及小新特性。 具体可以查看： MLSQL Stack 2.0.0 Bug 合集 用户可以关注新特性[Engine][2.0.0][Feature] 增加engine sparkcontext监控接口, 解决SparkContext挂掉后，但是Engine依然存活，导致监控程序无法很好的识别而拉起服务的问题。 在2.0.1开发期间，我们也发布了analysis.mlsql.tech, 允许用户轻松将MLSQL Engine部署到阿里云，分析自己在OSS上的数据。该功能目前还处于测试阶段。感兴趣的可以尝试。预期在2.2.0版本 会达到稳定。 我们下一个版本会是2.1.0. 在这个版本，分支功能将会得到支持，大家可以查看这个Issue: [Engine][2.0.0][Feature] 增加 if/else 分支流程控制 注意，该Issue下的语法形态并非最终形态。最终形态会在2.1.0版本发布的时候定型。我们到时会有博客详细介绍。 对于MLSQL Console，到2.1.0版本，我们预期主要精力会投入在云原生支持，并且会增加团队与角色的功能，让用户可以更好的管理自己的分析团队。            该文件修订时间： 2021-01-12 10:27:47 "},"release-notes/2.0.0.html":{"url":"release-notes/2.0.0.html","title":"MLSQL Stack 2.0.0版发布","keywords":"","body":"MLSQL Stack 2.0.0 发布 经过两月的努力，我们终于发布了2.0.0版本。原先是打算发布1.7.0的，经过慎重考虑，我们决定改成大版本发布。主要原因有： MLSQL Cluster（Proxy） 暂时被移除。 MLSQL Console得到了极大的改进，除了之前Console，现在也支持AnalysisWorkshop这种无SQL化工作平台 并且我们启动了权限服务的开发，可以将所有资源（数仓，数据湖，文件，数据库等各种到列级别的资源得到有效权限控制） MLSQL Engine添加了MLSQL代码提示插件 MLSQL Console 重写了大部分代码，推出无SQL化工作平台： 演示视频 新文档 MLSQL代码提示插件 演示视频 MLSQL Engine MLSQL Engine 接口增加了Token以及自定义接口访问权限控制。 新文档(正在撰写) 老文档 总结 MLSQL Engine 内核代码已经非常稳定，大部分功能都是以插件形式提供。 MLSQL Console则在积极开发中，给用户更好的交互体验。 为了简化搭建成本，MLSQL Cluster(Proxy) 短期内会停止维护。 新组建如权限服务等会开始开发，我们也会提供免费版本供使用。            该文件修订时间： 2021-01-12 10:27:47 "},"release-notes/2.0.1-bug.html":{"url":"release-notes/2.0.1-bug.html","title":"MLSQL Stack 2.0.1 Bug 合集","keywords":"","body":"MLSQL Stack 2.0.1 Bug 合集            该文件修订时间： 2021-01-12 10:27:47 "},"release-notes/2.0.0-bug.html":{"url":"release-notes/2.0.0-bug.html","title":"MLSQL Stack 2.0.0 Bug 合集","keywords":"","body":"MLSQL Stack 2.0.0 Bug 合集 该文档主要记录2.0.0版本已知bug,包括Console/Engine. 我们会在2.0.1版本修复。用户如果需要也可以直接使用 2.0.1-SNAPSHOT。 MLSQL Console [Console][2.0.0][Bug] PacketTooBigException [Console][2.0.0][Bug][AanalysisWorkshop] 修改字段名时，不能修改后和原来的名称相同 [Console][2.0.0][Bug] 添加Engine无法删除， 在Console里无法正确选择Engine [Console][2.0.0][Bug] 控制台脚本目录 expand/collapse 的问题 [Console][2.0.0][Bug] 上传文件时，使用配置而非数据库主目录配置 [Console][2.0.0][Bug] 下载个人目录文件提示没有权限 MLSQL Engine [Engine][2.0.0][Bug][Ray] 结合Ray运行长任务时，会出现Connection Refused问题 [Engine][2.0.0][Bug] 使用!ray,!python命令行权限控制问题 [Engine][2.0.1][Feature] 支持读取二进制文件 [Engine][2.0.1][Feature] 支持保存二进制数组到HDFS文件系统 [Engine][2.0.1][Bug] 解决在spark-3.0中运行!ray 会出现jackson-scala版本问题 [Engine][2.0.1][Bug] !ray指令中如果配置运行地址为driver时会出现socket相关问题 [Engine][2.0.1][Bug] WowRowEncoder 导致Python获取Spark数据非常慢 [Engine][2.0.0][Feature] 增加engine sparkcontext监控接口            该文件修订时间： 2021-01-12 10:27:47 "},"howtouse/":{"url":"howtouse/","title":"基本安装","keywords":"","body":"基本安装 MLSQL Stack套件（包含Console/Engine）安装配置说明。            该文件修订时间： 2021-01-12 10:27:46 "},"howtouse/install.html":{"url":"howtouse/install.html","title":"MLSQL Console 安装","keywords":"","body":"MLSQL Console 安装 下载 下载地址： 2.0.0 MLSQL Engine 和 MLSQL Console需要配套。MLSQL Engine的安装，请参考MLSQL Stack 安装介绍中的 MLSQL Engine部分 下载解压后，会看到一个mlsql-console-2.0.0文件夹，进入该目录： Step1 数据库配置 首先创建数据库，假设你数据库名称为 mlsql-console， 使用该目录下的 console-db.sql 文件进行表创建。 修改application.docker.yml中标记有 MYSQL_HOST 字样的部分，修改连接地址，账号密码。 Step2 修改start-default.sh 在start-default.sh中有个MY_URL，请将127.0.0.1转换成你的实际机器IP（内网）。如果你要修改端口，你需要同时修改application.docker.yml以及start-default.sh 中的MY_URL的端口。 Step3 启动服务 现在可以运行 ./start-default.sh 启动MLSQL Console了。默认端口为9002. 恭喜，你已经完成安装部分。请访问 http://127.0.0.1:9002 完成剩下配置。我们在下一个章节会告诉你如何配置。            该文件修订时间： 2021-01-12 10:27:46 "},"howtouse/setup.html":{"url":"howtouse/setup.html","title":"MLSQL Console 初始配置","keywords":"","body":"MLSQL Console 初始配置 完成上个章节的安装后，你应该可以访问 http://127.0.0.1:9002了。此时你应该看到一个设置页面： 设置管理员账号和密码 该步骤是设置管理员账号和密码。 添加计算引擎 MLSQL Engine Engine URL 请填写实际Engine内网地址。 请注意你需要添加 http:// 地址前缀。 恭喜完成 此时可以进入控制台了。如果你是管理员，应该可以看到Setting标签： 进入该标签,你可以开启注册和登录功能： 这样其他用户知道地址后，就可以注册和登录了。 默认没有开启Console界面，用户可以将Enable Console选项打开： 这样才会有控制台选项： 到目前为止，我们已经可以正常使用Console了。            该文件修订时间： 2021-01-12 10:27:46 "},"howtouse/engine/":{"url":"howtouse/engine/","title":"MLSQL Engine 安装与配置","keywords":"","body":"基本安装 目前经过测试的兼容版本如下： Spark 2.4.3 Spark 3.0.0 该章节为 【MLSQL Engine】 多种部署方式说明。 MLSQL Engine复用了Spark的部署工具，使用spark-submit命令进行部署。 这意味着： 需要有Spark发行包 支持Spark支持的所有环境 MLSQL Engine 的发行包可以在 MLSQL官方下载站点知道。目前最新版本为2.0.0. 在对应版本的下载目录里，你应该会看到两个发行包: mlsql-engine_2.4-2.0.0.tar.gz mlsql-engine_3.0-2.0.0.tar.gz 我们以第一个为例，解释包名的含义： 2.4以及3.0 都表示依赖的Spark版本 2.0.0 表示MLSQL Engine的自身的版本            该文件修订时间： 2021-01-12 10:27:46 "},"howtouse/engine/configuration.html":{"url":"howtouse/engine/configuration.html","title":"启动参数详解","keywords":"","body":"启动参数详解 一个典型的启动命令： $SPARK_HOME/bin/spark-submit --class streaming.core.StreamingApp \\ --driver-memory ${DRIVER_MEMORY} \\ --jars ${JARS} \\ --master local[*] \\ --name mlsql \\ --conf \"spark.scheduler.mode=FAIR\" \\ [1] ${MLSQL_HOME}/libs/${MAIN_JAR} \\ -streaming.name mlsql \\ -streaming.platform spark \\ -streaming.rest true \\ -streaming.driver.port 9003 \\ -streaming.spark.service true \\ -streaming.thrift false \\ -streaming.enableHiveSupport true 以位置[1]为分割点，前面主要都是Spark相关配置，后面部分则是MLSQL相关配置。也有另外一个区别点，spark配置都以两个横杠开头，而MLSQL配置则以一个横杠开头。 通过在这种方式，我们可以将MLSQL Engine运行在包括K8s,Yarn,Mesos以及Local等各种环境之上。 下面表格是MLSQL Engine的参数说明。 MLSQL也使用到了很多以spark开头的参数，他们必须使用 --conf 来进行配置，而不是 - 配置。这个务必要注意。 常用参数 参数 说明 示例值 streaming.master 等价于--master 如果在spark里设置了，就不需要设置这个 streaming.name 应用名称 streaming.platform 平台 目前只有spark streaming.rest 是否开启http接口 布尔值，需要设置为true streaming.driver.port HTTP服务端口 一般设置为9003 streaming.spark.service 配置streaming.rest 参数 一般设置为true streaming.job.cancel 支持运行超时设置 一般设置为true streaming.datalake.path 数据湖基目录 一般为HDFS 需要设置，否则很多功能会不可用，比如插件等。 接口权限控制 参数 说明 示例值 spark.mlsql.auth.access_token 如果设置了，那么会开启token验证，任何访问引擎的接口都需要带上这个token才会被授权 默认不开启 spark.mlsql.auth.custom 设置接口访问授权的自定义实现类 默认无 用户可以将实现 {def auth(params: Map[String, String]): (Boolean, String) 的类使用--jars带上，然后通过 --conf spark.mlsql.auth.custom= YOUR CLASS NAME 来设置自定义的接口权限访问。 二层通讯参数 MLSQL Engine会在Spark之上构建一个二层通讯，方便driver直接控制executor. 参数 说明 示例值 streaming.ps.cluster.enable 是否开启二层通讯 默认为true spark.ps.cluster.driver.port 二层通讯driver端口 默认为7777 streaming.ps.ask.timeout 通讯超时 默认为3600秒 streaming.ps.network.timeout 通讯超时 默认为28800秒 Hive支持参数 参数 说明 示例值 streaming.enableHiveSupport 是否开启hive支持 默认为false streaming.hive.javax.jdo.option.ConnectionURL 用来配置hive.javax.jdo.option.ConnectionURL 默认为空 自定义UDF jar包注册 如果我们将自己的UDF打包进Jar包里，我们需要在启动的时候告诉系统对应的UDF 类名称。 UDF的编写需要符合MLSQL的规范。我们推荐直接在Console里动态编写UDF/UDAF。 参数 说明 示例值 streaming.udf.clzznames 支持多个class,用逗号分隔 离线插件安装 确保插件的jar包都是用--jars带上。并且目前只支持app插件。 参数 说明 示例值 streaming.plugin.clzznames 支持多个class,用逗号分隔 可通过如下地址下载插件(填写插件名和版本)： http://store.mlsql.tech/run?action=downloadPlugin&pluginType=MLSQL_PLUGIN&pluginName=mlsql-excel-2.4&version=0.1.0-SNAPSHOT session设置 MLSQL支持用户级别Session,请求级别Session。每个Session相当于构建了一个沙盒，避免不同请求之间发生冲突。默认是用户级别Session,如果希望使用请求级别Session，需要在请求上带上 sessionPerRequest 参数。对此参看Rest接口详解。 参数 说明 示例值 spark.mlsql.session.idle.timeout session一直不使用的超时时间 30分钟 spark.mlsql.session.check.interval session超时检查周期 5分钟 分布式日志收集 MLSQL Engine支持将部分任务的日志发送到Driver。 参数 说明 示例值 streaming.executor.log.in.driver 是否在driver启动日志服务 默认为true 权限校验 参数 说明 示例值 spark.mlsql.enable.runtime.directQuery.auth 开启directQuery语句运行时权限校验 默认为false spark.mlsql.enable.runtime.select.auth 开启select语句运行时权限校验 默认为false spark.mlsql.enable.datasource.rewrite 开启数据源加载时动态删除非授权列 默认为false spark.mlsql.datasource.rewrite.implClass 设置自定义数据源列控制的实现类            该文件修订时间： 2021-01-12 10:27:46 "},"howtouse/engine/deploy.html":{"url":"howtouse/engine/deploy.html","title":"Local","keywords":"","body":"Local部署 Local部署适合开发使用。如果企业服务器单机够强劲，也可以Local部署，性能会好于同等配置的集群。 安装条件 下载Spark发行包 下载MLSQL发行包 注意: mlsql-engine_3.0-x.x.x.tar.gz 对应Spark 3.0.x mlsql-engine_2.4-x.x.x.tar.gz 对应spark 2.4.x 启动 分别解压两个发行包. 解压后的MLSQL Engine包如下： 按如下方式启动start-default.sh 即可运行： export SPARK_HOME=... && ./start-default.sh 其中SPARK_HOME 是你Spark的解压目录。如果没有设置，脚本会提示你需要设置。 如果一切正常,你应该会看到如下的启动信息： 额外配置 可以打开 start-local.sh脚本，里面的配置都可以修改： 具体可配置参数参看启动参数详解            该文件修订时间： 2021-01-12 10:27:46 "},"howtouse/engine/k8s_deploy.html":{"url":"howtouse/engine/k8s_deploy.html","title":"K8s部署","keywords":"","body":"K8S部署 目前实测Spark 3.0 通过。建议使用3.0 进行K8s部署。 目前K8s部署有两种方式： Spark官方的 spark-submit方式 Google 开发的Operater方式 两种方式的差别是，Google是裸露K8s的命令来进行部署的，而Spark官方则是通过Spark-Submit透明完成K8s部署的。另外，Google的目前比较适合作为一次性任务提交，Spark官方的更适合将Spark部署成服务。而MLSQL Engine主要以Service方式部署，所以我们推荐官方的部署模式。 在K8s部署MLSQL Engine的基本思路是,将spark-submit命令作为一个K8s Pod的启动命令，就像我们平时启动一个web服务一样。启动完成后，spark-submit会自动向K8s 申请executor Pods. 如果是手工操作，就是启动一个pod,然后在里面运行spark-submit命令，spark-submit会完成如下两件事： 启动9003端口，也就是MLSQL Engine的HTTP端口,外部可以通过该端口访问。 向K8s申请 Executor Pods 最后，我们将第一个Pod增加service以及ingress配置后就可以对外提供服务了。 配置文件示例 下面是Deployment 的template: template: metadata: labels: app: spark-mlsql-3 spec: containers: - name: spark-mlsql-3 args: - >- echo \"/opt/spark/bin/spark-submit --master k8s://https://xxx.xxx.xxx.xxx:xxxx --deploy-mode client --jars local:///tmp/streamingpro-mlsql-spark_3.0_2.12-1.7.0-SNAPSHOT.jar --class streaming.core.StreamingApp --conf spark.kubernetes.container.image=xxxxx/dev/spark-jdk-slim-14:v3.0.0 --conf spark.kubernetes.container.image.pullPolicy=Always --conf spark.kubernetes.namespace=dev --conf spark.kubernetes.authenticate.driver.serviceAccountName=xxxxx --conf spark.kubernetes.container.image.pullSecrets=xxxxx --conf spark.kubernetes.driver.request.cores=1 --conf spark.kubernetes.driver.limit.cores=1 --conf spark.kubernetes.executor.request.cores=1 --conf spark.kubernetes.executor.limit.cores=1 --conf spark.executor.instances=1 --conf spark.driver.host=$POD_IP --conf spark.sql.cbo.enabled=true --conf spark.sql.adaptive.enabled=true --conf spark.sql.cbo.joinReorder.enabled=true --conf spark.sql.cbo.planStats.enabled=true --conf spark.sql.cbo.starSchemaDetection=true --conf spark.driver.maxResultSize=2g --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.kryoserializer.buffer.max=200m --conf \"\\\"spark.executor.extraJavaOptions=-XX:+UnlockExperimentalVMOptions -XX:+UseZGC -XX:+UseContainerSupport -Dio.netty.tryReflectionSetAccessible=true\\\"\" --conf \"\\\"spark.driver.extraJavaOptions=-XX:+UnlockExperimentalVMOptions -XX:+UseZGC -XX:+UseContainerSupport -Dio.netty.tryReflectionSetAccessible=true\\\"\" local:///tmp/streamingpro-mlsql-spark_3.0_2.12-1.7.0-SNAPSHOT.jar -streaming.name spark-mlsql-3.0 -streaming.rest true -streaming.thrift false -streaming.platform spark -streaming.enableHiveSupport true -streaming.spark.service true -streaming.job.cancel true -streaming.datalake.path /data/mlsql/datalake -streaming.driver.port 9003\" | bash command: - /bin/sh - '-c' 需要注意的配置点-POD_IP spark.driver.host 需要配置成 $POD_IP,具体方式如下： env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: NODE_IP valueFrom: fieldRef: fieldPath: status.hostIP 需要注意的配置点-serviceAccountName 通常在K8s里需要配置--conf spark.kubernetes.authenticate.driver.serviceAccountName=xxxx, 目的是为了过K8s相关的权限。 需要注意的配置点-Java参数配置 如果你使用JDK8的镜像，那么需要移除spark.executor.extraJavaOptions,spark.driver.extraJavaOptions等两个配置选项。他们是为JDK14准备的。 如何获取配置中的镜像 其中，上面设计的镜像xxxxx/dev/spark-jdk-slim-14:v3.0.0可以使用官方提供的镜像工具来获得。具体方式如下(以Spark2.4.3版本为例)： cd $SPARK_HOME && ./bin/docker-image-tool.sh -t v2.4.3 build 上面执行完会打三个images：spark:v2.4.3 / spark-py:v2.4.3 / spark-r:v2.4.3 你如果只需要spark基本镜像包可执行： cd $SPARK_HOME && docker build -t spark:latest -f kubernetes/dockerfiles/spark/Dockerfile . MLSQL Engine Jar包如何放到镜像里 可以把MLSQL Engine Jar包使用K8s的Volume功能挂载到pod里去。同理--jars 目录。Executor的Pod不需要挂载。 Hadoop相关配置如何放到镜像里 同样可以使用K8s Volumn进行挂载，然后设置环境变量。也可以将相关配置放到SPARK_HOME/conf里之后再执行镜像构建命令。 Spark 3.0版本如何兼容hive 1.2 Spark3.0默认不兼容 hive 1.2 需要自己从Spark源码重新打包。譬如下面的编译参数就指定了hive-1.2的支持，然后再使用这个发行包构建镜像。 git checkout -b v3.0.0 v3.0.0 ./dev/make-distribution.sh \\ --name hadoop2.7 \\ --tgz \\ -Pyarn -Phadoop-2.7 -Phive-1.2 -Phive-thriftserver -Pkubernetes \\ -DskipTests LZO/Snappy的问题 LZO的jar包直接丢SPARK_HOME/jars里即可。 Snappy需要通过--conf java参数指定： --conf spark.executor.extraJavaOptions=-Djava.library.path=/opt/.../lib/native/ 请确保所有的镜像里都有配置的路径。 如何配置Service实现外部访问 我们可以通过配置service/ingress来实现外部访问Engine. 首先是service.yaml示例： apiVersion: v1 kind: Service metadata: annotations: {} name: spark-mlsql namespace: dev spec: ports: - name: mlsql port: 9003 targetPort: 9003 - name: spark port: 4040 targetPort: 4040 selector: app: spark-mlsql-3 接着ingress.yaml示例： apiVersion: extensions/v1beta1 kind: Ingress metadata: name: spark-mlsql-ingress namespace: dev spec: rules: - host: xxxxxxxxx http: paths: - path: / backend: serviceName: spark-mlsql servicePort: 9003 - host: xxxxxxxx http: paths: - path: / backend: serviceName: spark-mlsql servicePort: 4040            该文件修订时间： 2021-01-12 10:27:46 "},"howtouse/engine/yarn_deploy.html":{"url":"howtouse/engine/yarn_deploy.html","title":"Yarn部署","keywords":"","body":"Yarn部署 Yarn部署非常简单。我们推荐使用yarn-client模式部署。根据Local部署的需求，下载Spark/Engine，然后按如下步骤即可启动。 将hdfs/yarn/hive相关xml配置文件放到 SPARK_HOME/conf目录下。 修改Engine的start-local.sh 修改Engine的start-local.sh 找到文件里如下代码片段： $SPARK_HOME/bin/spark-submit --class streaming.core.StreamingApp \\ --driver-memory ${DRIVER_MEMORY} \\ --jars ${JARS} \\ --master local[*] \\ --name mlsql \\ --conf \"spark.sql.hive.thriftServer.singleSession=true\" \\ 将--master的local[*] 换成 yarn-client, 然后添加excuctor配置,最后大概如下面的样子： $SPARK_HOME/bin/spark-submit --class streaming.core.StreamingApp \\ --driver-memory ${DRIVER_MEMORY} \\ --jars ${JARS} \\ --master yarn \\ --deploy-mode client \\ --executor-memory 2g \\ --executor-cores 1 \\ --num-executors 1 \\ --name mlsql \\ --conf \"spark.sql.hive.thriftServer.singleSession=true\" \\ 然后指定SPARK_HOME运行即可。            该文件修订时间： 2021-01-12 10:27:46 "},"analysis-workshop/":{"url":"analysis-workshop/","title":"Analysis Workshop使用","keywords":"","body":"Analysis Workshop使用 1.7.0-SNAPSHOT/1.7.0 及以上版本可用 Analysis Workshop 提供了非常强大的数据分析功能,用户无需任何编程和SQL方面的知识，即可实现对数据分析。 其特点有： 支持数仓（Hive）,数据湖(HDFS),文件系统(HDFS)等各种主流数据源。 支持海量数据分析 无SQL，无编码 边修改边预览，你所看到的数据就是你要修改的数据 支持操作回滚 用户可以在 Analysis Workshop上实现等价任意复杂度SQL操作 Analysis Workshop的设计者相信，用户关心的是数据的变化而非处理逻辑的变化，通过创新性的引入暂存区和以及[应用/回滚]动作解决用户 感知数据形态变化需求。            该文件修订时间： 2021-01-12 10:27:46 "},"analysis-workshop/interface.html":{"url":"analysis-workshop/interface.html","title":"软件界面介绍","keywords":"","body":"界面简介 该章节我们简单介绍Analysis Workshop的一些基础概念。 先看交互界面的介绍： 最右侧的[Table Workhsop]是 暂存区，我们在操作数据后，可以将当前数据状态保存成一张新表，方便后续使用。同时也能起到保存操作的作用，避免 对数据操作的丢失。 左侧上边部分是数据操作区，在数据操作区里有大量操作单元，允许我们可以对数据展示区里的数据进行处理。处理点击Apply后会实时 在 数据展示区得到回显。 整个界面的核心是由上面三部分组成： 暂存区 数据操作区 数据展示区 我们可以通过 数据操作区 对 数据展示区 里的数据进行处理，我们可以随时将数据展示区里的数据形态保存到暂存区。            该文件修订时间： 2021-01-12 10:27:46 "},"analysis-workshop/admin-engine.html":{"url":"analysis-workshop/admin-engine.html","title":"Admin管理后台之Engine配置","keywords":"","body":"Admin管理后台之Engine配置 MLSQL Console后端的执行引擎是 MLSQL Engine.所以我们需要配置了Engine才能真正的数据分析。 MLSQL Console在启动脚本中可以配置Engine相关信息，同时也会配置一些参数高速Engine Console的一些信息，从而实现双方互通。 我们在第一次启动Console的时候，初始化配置里，也会有一个简单的Engine配置，保证配置完成后可以执行最基础的Console功能。如果想更完善的配置，则需要以Admin身份进入Console. 如果你是管理员，那么进入方式界面之后可以看到【管理员选项卡】： 点击进入后再点击左侧Engines选项卡： 打开【编辑模式】即可编辑或者删除已有的Engine配置，或者新增新的Engine配置。 每个字段的含义： 参数 说明 示例 name 引擎名称，需要唯一性 default这样的字符串 url 引擎地址 http://127.0.0.1:9003 home 主目录 /data/mlsql/homes consoleUrl console的地址。考虑到很多次场景系统自动获得IP地址是错误的，用户需要主动填写，避免Engine无法连接Console http://127.0.0.1:9002 fileServerUrl 文件服务器地址 默认填写 consoleUrl authServerUrl 权限校验服务器地址 默认填写 consoleUrl skipAuth 是否跳过权限验证 建议跳过；如果设置为false表示开启权限验证，Engine会去authServerUrl校验信息 extraOpts json字段。如果是新增，填写 {} accessToken Engine如果开启了token验证，需要得到token才能访问 Engine配置完成之后还有啥 配置完成之后，你需要在两个地方用到Engine,第一个是在Console写脚本的时候： 这里你可以选择已经配置的Engine去执行你的代码。 第二是 【设置】下面的 给【分析工坊】设置一个默认的执行引擎。 另外，你可能还需要给【分析工坊】设置一个合理的超时时间，取决于你的Engine规模大小以及数据大小：            该文件修订时间： 2021-01-12 10:27:46 "},"analysis-workshop/simple-example.html":{"url":"analysis-workshop/simple-example.html","title":"简单使用示例","keywords":"","body":"简单使用示例 我们会通过一个简单csv格式数据处理来介绍Analysis Workshop的功能。 上传数据 假设用户有一个在本地的CSV文件，我们通过 Console->Tools/Dashboard -> Upload 进行上传： 将文件拖拽到上传框后，就可以了。 系统提示你上传成功，你可以通过命令行查看。不过我们可以到直接进入 Analysis Workshop -> FileSystem 查看： 可以看到，我们上传的文件已经在 /tmp/upload里了。 打开文件 右键单击文件，会弹出一个配置框： 他会询问打开的类型以及需要配置的参数。我们只需要配置header等于true,这表示我们会将csv的第一行作为字段名称。 其他的无需配置，点击Ok，此时 数据操作台，数据展示台 会出现： 查看文件记录条数 我们可以在数据展示台里点击下拉框， 然后点击count即可： 此时我们看到，CSV文件里有接近7万条数据。 如果希望回到前面的状态里，只需要点击操作台里的 【Rollback】 按钮即可。 它会撤销掉刚才的count操作。 过滤产品名 如果我们想过滤 Product Name 为 Consumer Loan 数据单独看看，可以点击 数据展示台的搜索标记： 点击运行后： 过滤完成后，我们希望把这个结果保存下来，但是保存之前，我们并不需要这么多列，这个时候我们可以选择我们需要的列，并且修改列名。 选择我们需要的列 点击上面的 【Apply】 按钮，他会将你的选择实时反馈出来。当然，如果你选择错误，你可以使用 【Rollback】进行返回。 修改列名 因为带有空格的列无法保存到Table Workshop里，所以我们先来修改个名字： 点击Apply之后，你会发现原来的列还被保留： 我们再去选择一次即可。 保存我们的工作成果 点击 【Save As】 按钮后，会弹出如下框： 我们取名为 table1 并且，我们希望能够把数据持久化下来，那么可以勾选 Persit Table. 点击Ok之后，就可以立刻在 Table Workshop里看到它： 因为我们对它进行了持久化，所以根据数据大小，我们在适当的时候可以在FileSystem里看到它： 然而，能不能在FileSystem中看到，并不影响你对它的使用。 重新打开原来的csv文件并且关联table1 我们按前面提到的方式，重新打开csv文件。接着，我们希望和和已经处理的table1进行关联。点击Join标签，会出现一个向导： 在下拉框中，你可以看到刚才我们保存的table1表。接着我们根据DataReceived字段关联： 最后一步，我们选择两个表里需要的字段： 执行Apply: 可以看到效果了。如果需要，我们可以继续保存这个结果为table2. 打开 Table Workshop里的表 方式是相同的，右键单击表名即可。我们这里打开 table1. 统计SubProduct不同种类的个数 打开table1后，点击 Agg标签对 SubProduct 的groupBy进行勾选 点击 【Choose Function】, 输入count（会自动提示）。然后在New FieldName 里填写num,点击ok即可。 如果你对函数熟悉，你也可以直接在Mannual Transform里自己填写。 现在点击 Apply: 如果SubProduct 种类特别多，你只想看其中某个产品的数目，你可以使用前面提到的搜索功能，直接在展示控制台里查看，也可以点击filter标签，填写你要的产品名称，： 点击Apply: 如果你发现过滤错了，你可以点击 【Rollback】 撤回。 对于任何结果你都可以使用【Save As】进行保存。 Filter的控制能力很强，你可以任意对条件使用嵌套的【且】和【或】组合进行过滤。 小结 从上面我们可以看到，使用Analysis Workshop已经可以实现非常复杂的数据处理了。            该文件修订时间： 2021-01-12 10:27:46 "},"analysis-workshop/pivot.html":{"url":"analysis-workshop/pivot.html","title":"透视表","keywords":"","body":"透视表 这可能是最简单的透视表使用了。 假设我们有个产品用户反馈表： 显然，这个表的记录模式是流水账，记录了某天某个用户对某个产品的一个反馈（Issue）。现在我们想要查看的是，以某天为单位每个产品的反馈数量。 根据分析需求，我们只需要关心时间，产品名称，反馈三个字段。所以第一步我们先做字段筛选，进入【操作区】，选择这三个列： 点击 【Apply】. 现在，进入透视表菜单： 将Date Received 拖拽到左边的【行列】，将 Product Name拖拽到【表头列】，而 Issue 则是作为统计列: 现在选择下统计方法，在当前场景里是选择 count: 点击 【Apply】: 这样，就可以看到每天每个产品的Issue情况了。            该文件修订时间： 2021-01-12 10:27:46 "},"analysis-workshop/dash.html":{"url":"analysis-workshop/dash.html","title":"可视化","keywords":"","body":"可视化 在Analysis Workshop中，用户也可以选择报表插件绘制报表。下面是制作一张报表的示意图： 报表插件一般使用MLSQL 脚本开发，之后会发布到MLSQL插件商店。 准备数据 我们需要准备一些测试数据，我们可以直接使用插件完成数据的准备。首先进入 Console，输入如下代码： include store.`tech/mlsql/console/example/SimpleData`; load delta.`example.simpleData` as output; 点击 【Run】，效果如下： 如果你需要自定义存储表，可以设置【targetTableName】,如下示例： set targetTableName=\"example.simpleData2\"; include store.`tech/mlsql/console/example/SimpleData`; load delta.`example.simpleData2` as output; 插件【tech/mlsql/console/example/SimpleData】默认会在你的delta的example库下，建立一个simpleData表。 simpleData有三个字段，分别是部门名，雇员名，以及薪水。我们现在的需求是，对按部门分组，统计每个雇员的薪水排名。 绘制报表 打开Analysis Workshop,点击右侧DeltaLake标签下的 example.simpleData表： 进入操作台的 Dash: 在 【Visualization Plugin】中选择一个合适的插件，点击后右侧【Plugin Parameters】中会出现一些配置参数以及样例图： 接着在【Generic】中选择X,Y 字段： 最后点击【Apply】可以看到渲染结果： 鼠标移动后，可视化区域会有一些菜单选项 可根据需求使用。 注意： 如果对于效果不满意，需要先点击【Rollback】，再修改配置后点击Apply.            该文件修订时间： 2021-01-12 10:27:46 "},"analysis-workshop/window.html":{"url":"analysis-workshop/window.html","title":"使用窗口进行数据分析","keywords":"","body":"使用窗口进行数据分析 窗口(Window)函数是SQL中的术语，亦是高阶分析必须的。尽管在Analysis Workshop中我们会以向导的方式帮住你去使用，但是主动了解Window窗口函数必要的基础知识依然是有必要的。 传统我们可以对数据进行分组聚合亦或是简单的对字段进行操作。但是很多场景，我们希望对每条数据的前后N条数据（或者特定区间）的数据进行某种计算，这就像用一个框每次滑动一条记录，每次计算的时候计算整个窗口包含的数据。 在Analysis Workshop中，我们需要四步完成一个或者多个窗口一致的统计。 对数据分组（这是性能上的要求） 对每个分组的数据进行排序（不是必须的） 设置窗口大小（不是必须的） 应用窗口计算函数 准备数据 我们需要准备一些测试数据，这里直接使用插件完成数据的准备。首先进入 Console，输入如下代码： include store.`tech/mlsql/console/example/SimpleData`; load delta.`example.simpleData` as output; 点击 【Run】，效果如下： 如果你需要自定义存储表，可以设置【targetTableName】,如下示例： set targetTableName=\"example.simpleData2\"; include store.`tech/mlsql/console/example/SimpleData`; load delta.`example.simpleData2` as output; 插件【tech/mlsql/console/example/SimpleData】默认会在你的delta的example库下，建立一个simpleData表。 simpleData有三个字段，分别是部门名，雇员名，以及薪水。我们现在的需求是，对按部门分组，统计每个雇员的薪水排名。 窗口分析 在Analysis Workshop中打开 example.simpleData表，然后进入操作台 【Window】 选显卡， 第一步选择分组字段，可以是多个，我们选择部门： 接着按薪水排序： 选择窗口大小，Preceding和Following 都不填写的话，那么默认Preceding 是unbouded, Following是current row. 正常大家填写数字即可。 ： 我们对窗口计算两个值，一个是部门薪水总和，一个是雇员薪水排名。 先看如何计算薪水综合： 填写完成后，点击【Add】 按钮，下侧会显示添加结果： 在上面搜索框中你可以搜索一个函数的具体用法： 接着我们进行排名，使用rank函数： 添加完成后，点击 【Apply】即可看到结果： 如果需要可视化该图的话，进入【Dash】标签填写必要信息即可：            该文件修订时间： 2021-01-12 10:27:46 "},"analysis-workshop/complex-filter.html":{"url":"analysis-workshop/complex-filter.html","title":"复杂组合条件过滤","keywords":"","body":"复杂组合条件过滤 Analysis Workshop 引入【组】、【条件】两个基础概念实现复杂的过滤条件组合。 条件， 基本的比较表达式，比如 等于，Like等。 组， 只包含基本条件或者只包含其他组，他们之间只有 【且/and】 或者 【或/or】的关系。 说起来可能比较抽象。我们来看一个具体实例。 在 Analysis Workshop里打开一张表simpleData, 他包含三个字段： 分别是员工，部门和薪资。 现在我们希望过滤出符合如下条件的内容： 部门为 Sales 且 员工薪资大于 1000 部门为Finance且员工薪资小于 3000 1，2的关系为或的关系。 这里，【部门为 Sales】 我们叫做一个基本【条件】，所以第一个规则其实就是一个组，这个组里面有两个条件，分别是部门为 Sales 和员工薪资大于1000。 同理2. 1，2两个组构成一个新组，我们假设叫 【最终组】，最终组包含这两个组的关系是或。 我们来看看如何使用Analysis Workshop来描述这种关系。 构建组 先构建组【部门为 Sales 且 员工薪资大于 1000】， 勾选条件，并且点击 【Add Selected conditions to group】 将条件添加到组里。 添加完成后，可以在 Apply Group 标签栏看到新添加的组： 我们可以点击Apply 看下效果： Rollback数据，接着继续构建组【部门为Finance且员工薪资小于 3000】，构建完成后，可以在 Add Groups to Group 标签栏看到新建的两个组： 在这里，我们可以进一步组合组，得到最终组， 现在可以切到 Apply Group里，选择最终组 点击Apply,即可看到结果：            该文件修订时间： 2021-01-12 10:27:46 "},"analysis-workshop/ds-mysql.html":{"url":"analysis-workshop/ds-mysql.html","title":"MySQL数据库配置","keywords":"","body":"MySQL数据库配置 【Analysis Workshop】 支持用户自己添加MySQL数据库，从而能够对数据库里的数据进行。使用【Analysis Workshop】对MySQL分析最大的优势是，可以和其他存储器的数据做关联分析，比如数仓，数据湖，文件系统等。 本文将教会读者配置MySQL数据库，以及一些简单的分析用法。 配置 登录进 MLSQL Console 后，进入 设置 -> 数据源 -> 连接MySQL: 之后填写MySQL信息，点击应用。系统提示成功后，就可以在 【MySQL列表】里看到刚才的配置： Note: 别名如果没有配置，默认和数据库名是一致的。 进入 【Analysis Workshop】，就可以看到【MySQL】标签下，多了一个数据库： 使用 双击MySQL标签下的任何一张表，系统会弹出一个打开选项，此时会出现两种情况： 可以配置参数加速表的打开 不可以配置参数，只能单线程打开表 两种情况哪个会出现的依据是表里面是否有数字类型的字段。【Analysis Workshop】 为了能够多线程的去拉取MySQL数据，需要按数字列按区段对数据进行切割。 下面是不能使用的提示： 这个时候直接点击确认，就可以看到数据了： 如果能使用的话，用户需要选择下切割字段： 我这里选择 id字段（自增id字段是最佳的切割字段），系统会自动补全最大值，最小值， 然后就是分区数，默认是10。你也可以调整。 切分方式就比较简单的，最大值减去最小值，然后除以10，就得到区间的id范围，这样可以并行的去MySQL获取数据。 如果系统自动填充超时（可能MySQL表很慢），用户自己手动填写下就好。 之后点击确认就可以继续对数据做分析了。 如何将将MySQL表数据导出到文件系统，数据湖或者数仓里？ 可以点击[Save As]按钮，将表保存到【暂存区】， 然后利用暂存区的导出功能导出到其他数据源里： 稍后你就可以在这里看到数据了： 如果你一直没看到数据，可以进入【控制台】 的 Tools/Dashboard/History里看进度：            该文件修订时间： 2021-01-12 10:27:46 "},"analysis-workshop/json-fields.html":{"url":"analysis-workshop/json-fields.html","title":"Json字段展开","keywords":"","body":"Json字段展开 实际业务场景中，我们很多字段会是Json文本。AnalysisWorkshop 提供了两种方式供用户使用。一种是需要学习函数，一种则不需要。 无需函数展开Json字段 在AnalysisWorkshop里打开表，然后进入【操作区】的 【字段】/【Json字段展开】栏目，选择包含json格式的字段： 勾选后，会出现一个 添加 符号： 点击后，系统会罗列出json字段里所有的路径。 假设我们选择 web_paltform字段，然后取一个新的名字,点击Apply，这个时候你操作的数据就多了一列 web_platform了： 使用函数 用户也可以使用get_json_object函数，进入【操作区】的 【字段】/【转换字段】栏目，填写函数和字段名称： 点击Apply后也能得到和前面相同的效果。不过这里需要用户掌握get_json_object函数的用法以及XPath的使用规则。            该文件修订时间： 2021-01-12 10:27:46 "},"analysis-workshop/aw-plugin.html":{"url":"analysis-workshop/aw-plugin.html","title":"用MLSQL为Analysis Workshop开发插件","keywords":"","body":"用MLSQL为Analysis Workshop开发插件 前言 我们知道 【Analysis Workshop】 的目标是无SQL化，但最终可以做到等价于任意复杂度SQL。尽管它现在理论上 应该非常接近这个目标了，但是我们认为我们可以通过插件，进一步简化操作，让一个可能需要很多步骤的东西，变成可能只需要 一个步骤就可以完成。 依托于面向大数据和AI语言MLSQL强大的能力，我们可以很轻松的为【Analysis Workshop】添加插件。 示例插件：模糊过滤插件 在这篇文章里，我们会开发一个非常简单的插件，该插件可以让用户按如下步骤完成模糊过滤功能： 从当前表中选择需要过滤的字段 填写过滤值 点击Apply运行插件代码，得到新结果集 插件构建的用户界面如下： 用户选择 【模糊匹配过滤插件.mlsql】,然后就会出现一个表单，按要求填写表单即可完成对应字段的过滤。 这个插件的功能等价于字段检索功能： 开发流程之表单构建 首先开发者需要到 【Console】中新建一个脚本： 接着就可以开始用MLSQL编写插件了。 首先插件需要一些参数，这些参数会以表单的形式供用户填写。在【模糊过滤插件】中，我们需要用户填写待搜索字段以及对应的搜索值。一个select 控件，一个input控件。在MLSQL中可以按如下方式表达： set field=\"\" where type=\"defaultParam\" and formType=\"select\" and label=\"过滤字段\" and required=\"true\" -- 这个字段是必须的 and optionTable=\"__current_table_fields__\" ; set field_value=\"\" where type=\"defaultParam\" formType=\"input\" and label=\"过滤值\"; 其中 field/field_value 为对应的表单空间名称，也是我们的参数名称。 type=\"defaultParam\" 表示这个set变量可以被前面的变量覆盖。 formType表示控件类型。目前只支持 input/select. label 是显示给用户的表单名称 required 指定该字段是否是必须填写 对于select 空间，还有几个特有的配置字段： 参数名称 参数描述 optionStr 下拉框的值集合，用分号分割。 optionTable 你可以将下拉框的值提前写到一个hive表或者delta表里，然后在这里指定表名。注意表必须包含name,value两个字段。 optionSql 你可以写一条简单的SQL，同样最后结果里必须包含name,value字段 selectMode select空间是单选还是多选。 multiple/tags optionTable 有一个内置的表，叫__current_table_fields__,可以获取当前显示数据的schema. 开发流程之数据处理逻辑 在前面，我们用set语法构建了表单，现在，我们可以开始处理数据了。在当前插件中，处理的逻辑比较简单，只用到了select语法，没有用到ET组件等更强大的功能。 select * from `${__current_table__}` where `${field}` like \"%${field_value}%\" as `${__new_table__}`; 这里有两个内置变量： __currenttable\\_ 当前用户正在操作的数据的名称。 __newtable\\_ 经过插件处理完成后数据的名称 field/field_value 则是前面我们自己设置的两个变量。 开发流程之调试 你可以在自己的脚本前面添加 set __current_table__ = \"***\" where type=\"defaultParam\"; set __new_table__ = \"****\" where type=\"defaultParam\"; 这样就可以直接在脚本里调试运行了。 开发流程之发布 最后调试没啥问题了，就可以发布了： 点击Publish即可。现在就可以进入【Analysis Workshop】里使用了。 总结 【Analysis Workshop】 插件开发基本分成两步： 构建表单 书写业务逻辑 因为MLSQL支持内嵌Python，以及通过Scala/Java 开发UDF/UDAF等，所以可以实现非常复杂的需求，从而定制化出一些在公司内部非常有价值的插件。            该文件修订时间： 2021-01-12 10:27:46 "},"console/":{"url":"console/","title":"MLSQL语言控制台使用","keywords":"","body":"MLSQL语言控制台使用 MLSQL Console也提供了编码模式，方便用户直接在Web Console上编程。 支持的语言为MLSQL, MLSQL 是一个面向大数据和AI的语言。            该文件修订时间： 2021-01-12 10:27:46 "},"console/interface.html":{"url":"console/interface.html","title":"界面简介","keywords":"","body":"界面简介 Console的主题界面如下： 在Tools/Dashboard里还有很多功能，比如执行历史，上传文件等： 在脚本编辑区，核心动作有四个： Run 运行脚本 Cancel 取消运行 Save 保存当前脚本 脚本执行超时设置 下侧会显示进度以及已经执行的时间。用户如果离开当前页面，可以到 Tools/Dashboard 的History标签页查看任务。            该文件修订时间： 2021-01-12 10:27:46 "},"console/simple-example.html":{"url":"console/simple-example.html","title":"简单使用示例","keywords":"","body":"简单使用示例 该章节我们会简单介绍如何通过MLSQL脚本来处理一个CSV文件。 上传数据 假设用户有一个在本地的CSV文件，我们通过 Console->Tools/Dashboard -> Upload 进行上传： 将文件拖拽到上传框后，就可以了。 按照提示，使用 !hdfs 命令查看： 加载数据 MLSQL 使用load加载数据， 书写时系统会自动做一些提示： 书写完成后，就可以点击运行查看了： 使用Select语句进行数据处理 MLSQL完全兼容SQL的select语句。我们可以使用Select语句对数据进行处理 保存数据 MLSQL使用Save语句保存数据。 这里把数据保存成了json格式，在FileSytem中也可以看到： 注意事项 如果需要保存脚本，需要在右侧新建脚本，然后在对应的脚本里进行保存。            该文件修订时间： 2021-01-12 10:27:46 "},"lang/":{"url":"lang/","title":"MLSQL语法手册","keywords":"","body":"MLSQL语法手册 正如本gitbook开篇说的，MLSQL首先是一个专门为大数据和机器学习而生的语言。那么既然是语言，就有其语法。 不过MLSQL的语法学习成本极低，基本只要原先已经掌握了SQL,那么将毫不费力的学会MLSQL. 通常而言，整个学习过程只需要十几分钟到几个小时，具体看大家对SQL的掌握程度。整个MLSQL大概有如下几类语句： set 语法，主要是进行变量设置。 load 语法，主要是用于加载数据表。 select 语法，传统的SQL select语句。 train/run/predict 语法，他们句式完全一致，只是语义上略有区别。 register 语法，将模型注册成函数，或者将python/java/scala的代码注册成函数。 save 语法，主要用于存储数据。 ! 宏语法,可以将任意一段MLSQL代码包装成一个命令使用。 MLSQL 内置了大量的!宏语法，比!hdfs, !python等。 include语法，类似c里面的include,我们可以将其他MLSQL文件里的内容包含到当前脚本里。 在下面的章节我们会进行详细的介绍。            该文件修订时间： 2021-01-12 10:27:46 "},"lang/set.html":{"url":"lang/set.html","title":"Set语法","keywords":"","body":"Set 语法 变量设置广泛的存在于各种语言中。MLSQL通过Set语法来完成变量设置。 基础应用 set hello=\"world\"; 此时你运行后不会有任何输出结果。因为他仅仅是设置了一个变量。那么怎么使用呢？ 你可以在后续的语法中使用，比如： set hello=\"world\"; select \"hello ${hello}\" as title as output; 这里，我们引入了一个下个章节才会介绍的select语法，不过因为select语法相当简单，本质就是就是传统的SQL [select语句 + as + 表名]。大家现在只要记住这个规则就可以了。我们通过SQL语句将变量输出,得到结果如下： title hello world 通常， 变量可以用于任何语句的任何部分。甚至可以是结果输出表名，比如下面的例子 set hello=\"world\"; select \"hello William\" as title as `${hello}`; select * from world as output; 我们看到，我们并没有显式的定义world表，但是我们在最后一句依然可以使用，这是因为系统正确的解析了前面的${hello}变量。 因为表名是被语法解析格式限制的，所以我们需要用``将其括起来，避免语法解析错误。 值得一提的是，set语法当前的生命周期是request级别的，也就是每次请求有效。通常在MLSQL中，生命周期分成三个部分： request （当前执行请求有效） session （当前会话周期有效） application （全局应用有效） 我们后续章节会详细描述。 request级别表示什么含义呢？ 如果你先执行 set hello=\"world\"; 然后再单独执行 select \"hello William\" as title as `${hello}`; select * from world as output; 系统会提示报错： Illegal repetition near index 17 ((?i)as)[\\s|\\n]+`${hello}` ^ java.util.regex.PatternSyntaxException: Illegal repetition near index 17 ((?i)as)[\\s|\\n]+`${hello}` ^ java.util.regex.Pattern.error(Pattern.java:1955) java.util.regex.Pattern.closure(Pattern.java:3157) java.util.regex.Pattern.sequence(Pattern.java:2134) java.util.regex.Pattern.expr(Pattern.java:1996) java.util.regex.Pattern.compile(Pattern.java:1696) java.util.regex.Pattern.(Pattern.java:1351) java.util.regex.Pattern.compile(Pattern.java:1028) java.lang.String.replaceAll(String.java:2223) tech.mlsql.dsl.adaptor.SelectAdaptor.analyze(SelectAdaptor.scala:49) 系统找不到${hello}这个变量,然后原样输出，最后导致语法解析错误。 set语法类型 set 是一个相当灵活的语法。 在MLSQL中，根据作用的不同，我们将set区分为五种类型： text conf shell sql defaultParam 第一种text也就是我们前面已经演示过的： set hello=\"world\"; 第二种conf表示这是一个配置选项，通常用于配置系统的行为，比如 set spark.sql.shuffle.partitions=200 where type=\"conf\"; 表示将底层spark引擎的shuffle默认分区数设置为200. 那么如何制定这是一个配置呢？ 答案就是加上 where type=\"conf\" 条件子句。 第三种是shell,也就是set后的key最后是由shell执行生成的。 我们已经不推荐使用该方式。典型的例子比如： set date=`date` where type=\"shell\"; select \"${date}\" as dt as output; 注意，这里需要使用`` 括住该命令。 输出结果为： dt Mon Aug 19 10:28:10 CST 2019 第四种是sql类型，这意味着set后的key最后是由sql引擎执行生成的。下面的例子可以让大家看出其特点和用法： set date=`select date_add(1) as set date=`select date_sub(CAST(current_timestamp() as DATE), 1) as dt` where type=\"sql\"; select \"${date}\" as dt as output; 注意这里也需要使用`` 括住命令。 最后结果输出为： dt 2019-08-18 最后一种是defaultParam，我们先看示例。 set hello=\"foo\"; set hello=\"bar\"; select \"${hello}\" as name as output; 最后输出是 name bar 这符合大家直觉，下面的会覆盖上面的。那如果我想达到这么一种效果，如果变量已经设置了，我就不设置，如果变量没有被设置过，我就作为默认值。为了达到 这个效果，MLSQL引入了defaultParam类型： set hello=\"foo\"; set hello=\"bar\" where type=\"defaultParam\"; select \"${hello}\" as name as output; 最后输出结果是： name foo 如果前面没有设置过hello=\"foo\", set hello=\"bar\" where type=\"defaultParam\"; select \"${hello}\" as name as output; 则输出结果为 name bar set语法编译时和运行时 MLSQL有非常完善的权限体系，可以轻松控制任何数据源到列级别的访问权限，而且创新性的提出了编译时权限， 也就是通过静态分析MLSQL脚本从而完成表级别权限的校验（列级别依然需要运行时完成）。 但是编译期权限最大的挑战在于set变量的解析，比如： select \"foo\" as foo as foo_table; set hello=`select foo from foo_table` where type=\"sql\"; select \"${hello}\" as name as output; 如果我们没有真的运行第一个句子，那么set语法就无法执行。但是因为是编译期，所以我们肯定不会真实运行第一个句子，这就会导致执行失败。 根本原因是因为set依赖了运行时才会产生的表。 为了解决这个问题，我们引入了 compile/runtime 两个模式。如果你希望你的set语句可以编译时就evaluate值，那么添加该参数即可。 set hello=`select 1 as foo ` where type=\"sql\" and mode=\"compile\"; 如果你希望你的set值，只在运行时才需要，则设置为runtime: set hello=`select 1 as foo ` where type=\"sql\" and mode=\"runtime\"; 编译期我们不会运行这个set语句。 内置变量 MLSQL 提供了一些内置变量，看下面的代码： set jack=''' hello today is:${date.toString(\"yyyyMMdd\")} '''; date是内置的，你可以用他实现丰富的日期处理。 回顾 这个章节有点复杂，原因是因为set语法的灵活性已经功能很强大。但是用户只要掌握基本用法已经足够满足需求了。            该文件修订时间： 2021-01-12 10:27:46 "},"lang/load.html":{"url":"lang/load.html","title":"Load语法","keywords":"","body":"Load 语法 Load的作用主要是为了方便加载数据源并且将其映射为一张表。我们知道，数据的来源可能是千差万别的，比如可能是流式数据源Kafka,也可能是传统的MySQL. 所以我们需要一个一致的语法将这些数据源里的数据映射成MLSQL中的表。 基本使用 我们先来看一个最简单的load语句： set abc=''' { \"x\": 100, \"y\": 200, \"z\": 200 ,\"dataType\":\"A group\"} { \"x\": 120, \"y\": 100, \"z\": 260 ,\"dataType\":\"B group\"} '''; load jsonStr.`abc` as table1; 这里，我们通过前面学习的set语法，设置了一个变量，该变量是abc, 然后，我们使用load语句将这段文本注册成一张视图（表）。运行 结果如下： dataType x y z A group 100 200 200 B group 120 100 260 我们过来看下load语法。第一个关键词是load,紧接着接一个数据源或者格式名称，比如上面的例子是jsonStr。这表示我们要加载一个json的字符串， 在后面，我们会接.abc.通常`内是个路径。比如 csv./tmp/csvfile，到这一步，MLSQL已经知道如何加载数据了，我们最后使用as table1`,表示将数据 注册成视图（表），注册的东西后面可以引用。 比如在后续的select语句我们可以直接使用table1： set abc=''' { \"x\": 100, \"y\": 200, \"z\": 200 ,\"dataType\":\"A group\"} { \"x\": 120, \"y\": 100, \"z\": 260 ,\"dataType\":\"B group\"} '''; load jsonStr.`abc` as table1; select * from table1 as output; 我们可以获得和上面例子一样的输出。 load语句还可以接一些参数，比如 load jdbc.`db.table` where url=\"jdbc:postgresql://localhost/test?user=fred&password=secret\" as newtable; 这里，我们通过where条件里的url参数告诉jdbc格式，我们要连接的数据库地址，并且我们需要访问的路径是db数据库里的table表。最后，我们将对应的数据库表注册成 视图(表)newtable。 有的时候，我们需要多次书写load语句，比如我们需要load多张mysql表，那么每次都填写这些参数会变成一件让人厌烦的事情。 load jdbc.`db.table` options and driver=\"com.mysql.jdbc.Driver\" and url=\"jdbc:mysql://127.0.0.1:3306/....\" and user=\"...\" and password=\"....\" and prePtnArray = \"age 10\" and prePtnDelimiter = \"\\|\" as table1; 比如上面的配置就很多了。为了减少书写，我们引入一个特殊的配合load的语法，叫connect： connect jdbc where url=\"jdbc:mysql://127.0.0.1:3306/wow?characterEncoding=utf8&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false\" and driver=\"com.mysql.jdbc.Driver\" and user=\"${user}\" and password=\"${password}\" as db_1; connect 后面接数据格式，这里是jdbc, 然后后面接一些配置。最后将这个连接取一个名字，叫做db_1. 现在我们可以这样写load语句了： load jdbc.`db_1.test1` where directQuery=''' select * from test1 where a = \"b\" ''' as newtable; select * from newtable; 我们不再需要书写繁琐的配置。让我们再回顾下上面的语法： load/connect 都需要需要最后接as， as 表示为注册，前面是注册的内容，as后面是注册后的名字。load as 是注册视图（表），connect as 是注册连接。 注册的东西在后面的语句中都可以直接使用，这也是MLSQL能成为脚本的根基之一。 通用参数withoutColumns、withColumns，可以通过withoutColumns移除掉不需要的列，withColumns指定需要的列，两者只能选一个 set abc=''' { \"x\": 100, \"y\": 200, \"z\": 200 ,\"dataType\":\"A group\"} { \"x\": 120, \"y\": 100, \"z\": 260 ,\"dataType\":\"B group\"} '''; load jsonStr.`abc` options withoutColumns=\"x,y\" as table1; Load也支持 conditionExpr，你可以加载数据的时候指定条件： load hive.`test.test` options conditionExpr=\"dt=20190522\" as t; 如何高效的书写Load语句 从上面的例子，我们可以看出，使用load语句，前提是我们知道有哪些数据源或者格式可用。还有一个难题，就是这些数据源或者格式都有什么可选的配置，也就是我们 前面看到的where条件语句。 MLSQL Console提供了图形化的界面供我们选择和填写参数： 填写完成后，MLSQL会自动帮我们生成语句。 可能有的用户并不会使用Console,那么除了文档以外，我们还有办法去探索有哪些可用的数据源以及响应的配置么？ MLSQL 还提供了一些帮助指令。 !show datasources; 通过该命令，你可以看到大部分可用数据源。 接着，你找到你需要的数据源名称，然后通过下面的命令，获得这个数据源的参数以及文档(这里我们以CSV为例)： !show \"datasources/params/csv\"; 大家可以看到CSV可用参数比较详细的解释： 我们在后续章节中，会对常见的数据源有更详细的介绍。            该文件修订时间： 2021-01-12 10:27:46 "},"lang/select.html":{"url":"lang/select.html","title":"Select语法","keywords":"","body":"Select 语法 Select语法是我们处理数据最重要的方式之一。之所以说之一，是因为我们还有ET(应用于run/train/predict)。不过Select语法是一个通用的数据处理方案， 足够灵活并且功能强大。 基本语法 最简单的一个select语句： select 1 as col1 as table1; 从上面可以看到，MLSQL中的select语法和传统SQL中的select语法唯一的差别就是后面多了一个 as tableName。这也是为了方便后续继续对该SQL处理的结果进行 处理而引入的微小改良。 在后续讲解register语法的章节，我们会看到，如何通过scala/java/python来增加SQL中的可用的自定义函数。            该文件修订时间： 2021-01-12 10:27:46 "},"lang/et_statement.html":{"url":"lang/et_statement.html","title":"Train/Run/Predict语法","keywords":"","body":"Train/Run/Predict语法 将这三者放在一起，是因为他们有完全一致的语法。他们的存在是为了弥补Select对机器学习以及自定义数据处理支持的不够好的短板。 基础语法 首先是train。 train顾名思义，就是进行训练，主要是对算法进行训练时使用。下面是一个比较典型的示例： load json.`/tmp/train` as trainData; train trainData as RandomForest.`/tmp/rf` where keepVersion=\"true\" and fitParam.0.featuresCol=\"content\" and fitParam.0.labelCol=\"label\" and fitParam.0.maxDepth=\"4\" and fitParam.0.checkpointInterval=\"100\" and fitParam.0.numTrees=\"4\" ; 如果我们用白话文去读这段话，应该是这么念的： 加载位于/tmp/train目录下的，数据格式为json的数据，我们把这些数据叫做trainData, 接着， 我们对trainData进行训练，使用算法RandomForest，将模型保存在/tmp/rf下，训练的参数为 fitParam.0.* 指定的那些。 其中fitParam.0 表示第一组参数，用户可以递增设置N组，MLSQL会自动运行多组，最后返回结果列表。 run/predict 具有完全一致的用法，但是目的不同。 run的语义是对数据进行处理，而不是训练，他是符合大数据处理的语义的。我们下面来看一个例子： run testData as RepartitionExt.`` where partitionNum=\"5\" as newdata; 格式和train是一致的，那这句话怎么读呢？ 运行testData数据集，并且使用RepartitionExt进行处理，处理的参数是partitionNum=\"5\"，最后处理后的结果我们取名叫newdata RepartitionExt是用于从新分区的一个模块，也就是将数据集重新分成N分，N由partitionNum配置。 那么predict呢？ predict语句我们一看，应该就知道是和机器学习相关的，对的，他是为了批量预测用的。比如前面，我们将训练随机森林的结果模型放在了 /tmp/rf 目录下，现在我们可以通过predict语句加载他，并且预测新的数据。 predict testData as RandomForest.`/tmp/rf`; 这句话的意思是，对testData进行预测，预测的算法是RandomForest，对应的模型在/tmp/rf下。 通过上面的举例，我们知道，train/predict是应用机器学习领域的，分别对应建模和预测。run主要是数据处理用的。在前面提及的RandomForest，RepartitionExt， 我们叫做ET,用户都可以实现自己ET，添加新的功能特性。MLSQL提供了良好的扩展机制方便用户进行自己的封装，可以说，整个MLSQL大部分功能集合都是基于 ET做扩展而形成的。            该文件修订时间： 2021-01-12 10:27:46 "},"lang/register.html":{"url":"lang/register.html","title":"Register语法","keywords":"","body":"Register语法 在前面章节，我们已经介绍过train/predict语法。register语法其实和他们一样，也是为了配合机器学习而设置的。我们看下面的列表说明： train ,对数据进行训练从而得到模型。 predict, 使用已经得到的模型，对数据进行批量预测。 register, 将模型注册成UDF函数，方便在批，流,API中使用。 从上面我们看到，register 主要目的是将模型注册成UDF函数。随着MLSQL的发展， 他还具备将Python/Scala/Java代码动态转化为SQL函数的能力，以及在流式计算力也有自己特定的作用。 注册模型 如果我们需要将一个已经训练好的模型注册成一个函数，那时相当简单的： load json.`/tmp/train` as trainData; train trainData as RandomForest.`/tmp/rf` where .... ; register RandomForest.`/tmp/rf` as rf_predict; select rf_predict(features) as predict_label from trainData as output; 现在我们的单独把register拿出来： register RandomForest.`/tmp/rf` as rf_predict; 怎么念呢？ 将 RandomForest模型/tmp/rf 注册成一个函数，函数名叫rf_predict 之后，我们就可以在后续的部分使用这个函数了。 注册函数 在SQL中，最强大的莫过于函数了。在Hive中，其相比其他的传统数据而言，在于其可以很好的进行函数的扩展。 MLSQL将这个优势发展到极致。我们来看看如何新建 一个函数： register ScriptUDF.`` as plusFun where and lang=\"scala\" and udfType=\"udf\" code=''' def apply(a:Double,b:Double)={ a + b } '''; 使用ET ScriptUDF注册一个函数叫plusFun,这个函数使用scala语言，函数的类型是UDF,对应的实现代码在code参数里。 接着，我们就可以使用这个函数进行数据处理了： -- create a data table. set data=''' {\"a\":1} {\"a\":1} {\"a\":1} {\"a\":1} '''; load jsonStr.`data` as dataTable; -- using echoFun in SQL. select plusFun(1,2) as res from dataTable as output; 是不是很简单。除了Scala以外，我们还支持scala/python/java等语言。 流式计算中的使用 在流式计算中，有wartermark以及window的概念。MLSQL的register语法也可以用于流式计算里的watermark的注册。 -- register watermark for table1 register WaterMarkInPlace.`table1` as tmp1 options eventTimeCol=\"ts\" and delayThreshold=\"10 seconds\"; 这里大家只要有个感觉就行。本章节我们不会做过多解释，在后续专门的流式计算章节，我们会提供非常详细的说明。            该文件修订时间： 2021-01-12 10:27:46 "},"lang/save.html":{"url":"lang/save.html","title":"Save语法","keywords":"","body":"Save语法 save语法类似传统SQL中的insert语法。insert其实在MLSQL中也能使用，不过我们强烈建议使用save语法来取代。在前面章节里， 我们介绍了load语法，有数据的加载，必然就有数据的存储，存储也需要支持无数的数据源，还会涉及到Schema的问题。 基本语法 我们先来看一个基本的例子： set rawData=''' {\"jack\":1,\"jack2\":2} {\"jack\":2,\"jack2\":3} '''; set savePath=\"/tmp/jack\"; load jsonStr.`rawData` as table1; 最后一句单拎出来： save overwrite table1 as json.`${savePath}`; 怎么读呢？ 将table1覆盖保存，使用json格式，保存位置为 ${savePath}。 还是比较易于理解的吧。通常，save语句里的 数据源或者格式和load是保持一致的，配置参数也类似。但有部分数据源只支持save,有部分只支持load。典型的比如 jsonStr,他就 只支持load,不支持save。 如果我想讲数据分成十份保存呢？ save也支持where条件子句： save overwrite table1 as json.`${savePath}` where fileNum=\"10\"; 另外Save支持四种存储方式： overwrite append ignore errorIfExists 还记得我们前面介绍的Connect语句么，connect语句主要是简化连接一些复杂的数据源，需要配置特别多的参数的问题。这个在save语句中也是同样适用的。 比如下面的例子： set user=\"root\"; connect jdbc where url=\"jdbc:mysql://127.0.0.1:3306/wow?characterEncoding=utf8&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false\" and driver=\"com.mysql.jdbc.Driver\" and user=\"${user}\" and password=\"${password}\" as db_1; save append tmp_article_table as jdbc.`db_1.crawler_table`; 你也可以完全讲connect里面的参数放到save的where子句中。值得注意的是，MLSQL的connect语句的生命周期是application级别。意味着你只要运行一次， 就全生命周期有效。            该文件修订时间： 2021-01-12 10:27:46 "},"lang/macro.html":{"url":"lang/macro.html","title":"宏语法(命令行)","keywords":"","body":"! 宏语法 宏语法的设计初衷是为了方便脚本片段的复用。 基本语法 假设我封装了一个根据用户名查询用户的功能逻辑，然后我希望将他方便的提供给其他人使用， 我们可以按如下方式写： set findEmailByName = ''' -- notice that there are no semicolon in the end of this line. select email from table1 where name=\"{}\" '''; -- Execute it like a command. !findEmailByName \"jack\"; 我们先通过set语法包含了一些MLSQL脚本片段，这里是一条SQL语句。接着我只要通过将感叹号应用于变量名称之上，我们就能将其作为一个命令执行。 这个例子，我们还需要传参数，可以使用\"{}\" 或者\"{数字}\"来做占位符。 内置的宏变量 我们内置了非常多的宏变量，比如对HDFS进行管理的 !hdfs, 以及对python做支持的!python。我们在后续的章节会会对这些命令有更多的讲解。            该文件修订时间： 2021-01-12 10:27:46 "},"lang/include.html":{"url":"lang/include.html","title":"Include语法","keywords":"","body":"Include语法 和宏一样，include语法也是为了方便做代码的复用。 我们在脚本a.mlsql里定义了一个变量a： set a= \"a.mlsql\"; 接着在脚本b.mlsql里可以直接通过include进行引用： include project.`a.mlsql`; select \"${a}\" as col as output; 通常，include语法需要配合MLSQL Console使用。因为默认MLSQL Console提供了脚本管理功能， MLSQL Engine会回调MLSQL Console从而获得 需要的脚本，然后进行include. 值得注意的是，include语法只是简单的将被包含的脚本展开到当前脚本中。 include 另外也支持hdfs,比如： include hdfs.`/tmp/a.sql`; 或者http include http.`http://abc.com` options param.a=\"coo\" and param.b=\"wow\"; and method=\"GET\" 上面相当于如下方式请求获得脚本内容： curl -XGET 'http://abc.com?a=coo&b=wow'            该文件修订时间： 2021-01-12 10:27:46 "},"lang/analyze.html":{"url":"lang/analyze.html","title":"MLSQL语法解析接口","keywords":"","body":"MLSQL语法解析接口 MLSQL Engine提供了语法解析模式。这主要可以帮助用户更好的UI体验： 抽取set语法，从而实现表单功能 抽取各个语句部分，从而在Web上实现模块化交互。 语法解析需要在调用MLSQL Engine接口时提供一个新的参数，executeMode,默认是query, 也就是执行查询操作，如果你设置为analyze,则会执行语法解析操作： /run/script?executeMode=analyze 假设我们的MLSQL脚本如下： set a=\"b\"; load delta.`/tmp/delta/rate-3-table` where startingVersion=\"12\" and endingVersion=\"14\" as table1; select __delta_version__, collect_list(key), from table1 group by __delta_version__,key as table2; 解析结果如下： [ { \"raw\": \"!delta history /tmp\", \"command\": \"delta\", \"parameters\": [ \"history\", \"/tmp\" ] }, { \"raw\": \"set a=\\\"b\\\"\", \"key\": \"a\", \"command\": \"\\\"b\\\"\", \"original_command\": \"\\\"b\\\"\", \"option\": {}, \"mode\": \"\" }, { \"raw\": \"load delta.`/tmp/delta/rate-3-table` where \\nstartingVersion=\\\"12\\\"\\nand endingVersion=\\\"14\\\"\\nas table1\", \"format\": \"delta\", \"path\": \"`/tmp/delta/rate-3-table`\", \"option\": { \"startingVersion\": \"12\", \"endingVersion\": \"14\" }, \"tableName\": \"table1\" }, { \"raw\": \"select __delta_version__, collect_list(key), from table1 group by __delta_version__,key \\nas table2\", \"sql\": \"select __delta_version__, collect_list(key), from table1 group by __delta_version__,key \\n\", \"tableName\": \"table2\" } ]            该文件修订时间： 2021-01-12 10:27:46 "},"batch/":{"url":"batch/","title":"MLSQL批处理编程","keywords":"","body":"MLSQL批处理编程 这个章节主要介绍MLSQL批处理方面的编程。在这个章节，我们重点会分如下几部分进行描述： 数仓和数据湖的支持 各种类型数据源的读取和写入 各种内置SQL UDF以及ET插件的使用            该文件修订时间： 2021-01-12 10:27:46 "},"datahouse/":{"url":"datahouse/","title":"MLSQL数仓/数据湖使用","keywords":"","body":"MLSQL数仓/数据湖使用 做大数据，目前来说还离不开数仓或者数据湖。 MLSQL 支持传统的Hive操作,也支持最新的Delta. 同时，数仓比较棘手的几个事情，第一件是数据同步，第二件是流式支持，第三个是小文件问题。 我们在这个章节也会详细阐述MLSQL是如何解决他们的。 值得注意的是，MLSQL Engine需要能够访问Hive,最简单的办法是将hive相关的配置文件(如hive-site.xml)放到 SPARK_HOME/conf目录下。 另外，我们可以通过JDBC来访问Hive,但性能可能比较低下。            该文件修订时间： 2021-01-12 10:27:46 "},"datahouse/hive.html":{"url":"datahouse/hive.html","title":"Hive加载和存储","keywords":"","body":"Hive加载和存储 Hive在MLSQL中使用极为简单。 加载Hive表只需要一句话： load hive.`db1.table1` as table1; 保存则是： save overwrite table1 as hive.`db.table1`; 如果需要分区，则使用 save overwrite table1 as hive.`db.table1` partitionby col1; 我们也可以使用JDBC访问hive,具体做法如下： load jdbc.`db1.table1` where url=\"jdbc:hive2://127.0.0.1:10000\" and driver=\"org.apache.hadoop.hive.jdbc.HiveDriver\" and user=\"\" and password=\"\" and fetchsize=\"100\"; 我们也可以使用数据湖替换实际的hive存储： 启动时配置 -streaming.datalake.path 参数,启用数据湖。 配置 -spark.mlsql.datalake.overwrite.hive hive默认采用数据存储。 使用时如下： set rawText=''' {\"id\":9,\"content\":\"Spark好的语言1\",\"label\":0.0} {\"id\":10,\"content\":\"MLSQL是一个好的语言6\",\"label\":0.0} {\"id\":12,\"content\":\"MLSQL是一个好的语言7\",\"label\":0.0} '''; load jsonStr.`rawText` as orginal_text_corpus; select cast(id as String) as rowkey,content,label from orginal_text_corpus as orginal_text_corpus1; save overwrite orginal_text_corpus1 as hive.`public.orginal_text_corpus1`; load hive.`public.orginal_text_corpus1` as output ; 在你访问hive时，如果数据湖里没有，则会穿透数据湖，返回hive结果。如果你希望在写入的时候一定要写入到hive而不是数据湖里，可以这样： save overwrite orginal_text_corpus1 as hive.`public.orginal_text_corpus1` where storage=\"hive\"; 强制指定存储为hive.            该文件修订时间： 2021-01-12 10:27:46 "},"datahouse/delta.html":{"url":"datahouse/delta.html","title":"Delta加载和存储以及流式支持","keywords":"","body":"Delta加载和存储以及流式支持 Delta本质就是HDFS上一个目录。这就意味着你可以在自己的主目录里欢快的玩耍。我们会分如下几个部分介绍Delta的使用： 基本使用 按数据库表模式使用 Upsert语义的支持 流式更新支持 小文件合并 同时加载多版本数据为一个表 查看表的状态(如文件数等) 基本使用 set rawText=''' {\"id\":1,\"content\":\"MLSQL是一个好的语言\",\"label\":0.0}, {\"id\":2,\"content\":\"Spark是一个好的语言\",\"label\":1.0} {\"id\":3,\"content\":\"MLSQL语言\",\"label\":0.0} {\"id\":4,\"content\":\"MLSQL是一个好的语言\",\"label\":0.0} {\"id\":5,\"content\":\"MLSQL是一个好的语言\",\"label\":1.0} {\"id\":6,\"content\":\"MLSQL是一个好的语言\",\"label\":0.0} {\"id\":7,\"content\":\"MLSQL是一个好的语言\",\"label\":0.0} {\"id\":8,\"content\":\"MLSQL是一个好的语言\",\"label\":1.0} {\"id\":9,\"content\":\"Spark好的语言\",\"label\":0.0} {\"id\":10,\"content\":\"MLSQL是一个好的语言\",\"label\":0.0} '''; load jsonStr.`rawText` as orginal_text_corpus; save append orginal_text_corpus as delta.`/tmp/delta/table10`; 执行上面的语句，我们就能成功的将相关数据写入delta数据湖了。 加载的方式如下： load delta.`/tmp/delta/table10` as output; 按数据库表模式使用 很多用户并不希望使用路径，他们希望能够像使用hive那样使用数据湖。MLSQL对此也提供了支持。在Engine启动时，加上参数 -streaming.datalake.path /tmp/datahouse 系统便会按数据湖模式运行。此时用户不能自己写路径了，而是需要按db.table的模式使用。 加载数据湖表： load delta.`public.table1` as table1; 保存数据库湖表： save append table1 as delta.`public.table1` partitionBy col1; 如果你开启了权限验证 ，并且使用MLSQL Console时，那么需要按如下方式配置权限： 如果是流式，delta换成rate.从权限上看，我们依然是按路径进行授权的。但是在MLSQL中，用户看到的是库和表。 你可以使用下列命令查看所有的数据库和表： !delta show tables; Upsert语义的支持 Delta支持数据的Upsert操作，对应的语义为： 如果存在则更新，不存在则新增。 我们前面保存了十条数据，现在尝试如下代码： set rawText=''' {\"id\":1,\"content\":\"我更新了这条数据\",\"label\":0.0} '''; load jsonStr.`rawText` as orginal_text_corpus; save append orginal_text_corpus as delta.`/tmp/delta/table10` and idCols=\"id\"; 我们看到id为1的数据已经被更新为。 流式更新支持 这里，我们会简单涉及到流式程序的编写。大家可以先有个感觉，不用太关注细节。我们后续专门的流式章节会提供更详细的解释和说明。 为了完成这个例子，用户可能需要在本机启动一个Kafka,并且版本是0.10.0以上。 首先，我们通过MLSQL写入一些数据到Kafka: set abc=''' { \"x\": 100, \"y\": 201, \"z\": 204 ,\"dataType\":\"A group\"} '''; load jsonStr.`abc` as table1; select to_json(struct(*)) as value from table1 as table2; save append table2 as kafka.`wow` where kafka.bootstrap.servers=\"127.0.0.1:9092\"; 接着启动一个流式程序消费： -- the stream name, should be uniq. set streamName=\"kafkaStreamExample\"; !kafkaTool registerSchema 2 records from \"127.0.0.1:9092\" wow; -- convert table as stream source load kafka.`wow` options kafka.bootstrap.servers=\"127.0.0.1:9092\" and failOnDataLoss=\"false\" as newkafkatable1; -- aggregation select * from newkafkatable1 as table21; -- output the the result to console. save append table21 as rate.`/tmp/delta/wow-0` options mode=\"Append\" and idCols=\"x,y\" and duration=\"5\" and checkpointLocation=\"/tmp/s-cpl6\"; 这里，我们指定x,y为联合主键。 现在可以查看了： load delta.`/tmp/delta/wow-0` as show_table1; select * from show_table1 where x=100 and z=204 as output; 小文件合并 MLSQL对Delta的小文件合并的要求比较苛刻，要求必须是append模式，不能发生更新已有记录的的表才能进行小文件合并。 我们在示例中模拟一些Kafka的数据： set data=''' {\"key\":\"a\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":0,\"timestamp\":\"2008-01-24 18:01:01.001\",\"timestampType\":0} {\"key\":\"a\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":1,\"timestamp\":\"2008-01-24 18:01:01.002\",\"timestampType\":0} {\"key\":\"b\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":2,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"b\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":3,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"b\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":4,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"b\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":5,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} '''; 接着流式写入： -- the stream name, should be uniq. set streamName=\"streamExample\"; -- load data as table load jsonStr.`data` as datasource; -- convert table as stream source load mockStream.`datasource` options stepSizeRange=\"0-3\" as newkafkatable1; select * from newkafkatable1 as table21; -- output the the result to console. save append table21 as rate.`/tmp/delta/rate-1-table` options mode=\"Append\" and duration=\"10\" and checkpointLocation=\"/tmp/rate-1\" partitionBy key; 这里注意一下是流里面delta叫rate。 现在我们利用工具!delta查看已有的版本： !delta history /tmp/delta/rate-2-table; 内容如下： 现在我们可以对指定版本之前的数据做合并了： !delta compact /tmp/delta/rate-2-table 8 1; 这条命令表示对第八个版本之前的所有数据都进行合并，每个目录（分区）都合并成一个文件。 我们看下合并前每个分区下面文件情况： 合并后文件情况： 我们删除了16个文件，生成了两个新文件。另外在compaction的时候，并不影响读和写。所以是非常有用的。 同时加载多版本数据为一个表 Delta支持多版本，我们也可以一次性加载一个范围内的版本，比如下面的例子，我们说，将[12-14) 的版本的数据按 一个表的方式加载。接着用户可以比如可以按id做group by，在一行得到多个版本的数据。 set a=\"b\"; load delta.`/tmp/delta/rate-3-table` where startingVersion=\"12\" and endingVersion=\"14\" as table1; select __delta_version__, collect_list(key), from table1 group by __delta_version__,key as table2; 查看表的状态 !delta info scheduler.time_jobs;            该文件修订时间： 2021-01-12 10:27:46 "},"datahouse/binlog.html":{"url":"datahouse/binlog.html","title":"MySQL Binlog同步","keywords":"","body":"MySQL Binlog同步 MySQL得到了广泛的使用。数仓的一个核心点是需要将业务的数据库（离线或者实时）同步到数仓当中。离线模式比较简单，直接全量同步 覆盖，实时模式会略微复杂些。一般而言，走的流程会是： MySQL -> Cannel(或者一些其他工具) -> Kafka -> 流式引擎 -> Hudi(HBase)等能够提供更新存储的工具 -> 同步或者转储 -> 对外提供服务 我们看到，这是一个很繁琐的的流程。流程越长，总体出问题的概率就越高，我们调试也会越困难。 MLSQL提供了一个非常简单的解决方案： MySQL -> MLSQL Engine -> Delta(HDFS) 用户的唯一工作是编写一个MLSQL代码，就可以直接运行于 MLSQL Console上。 整个脚本只包含两段代码，一个Load， 一个Save,令人惊讶的简单。 下面是Load语句： set streamName=\"binlog\"; load binlog.`` where host=\"127.0.0.1\" and port=\"3306\" and userName=\"xxx\" and password=\"xxxx\" and bingLogNamePrefix=\"mysql-bin\" and binlogIndex=\"4\" and binlogFileOffset=\"4\" and databaseNamePattern=\"mlsql_console\" and tableNamePattern=\"script_file\" as table1; set streamName 表名这是一个流式的脚本，并且这个流程序的名字是binglog. load语句我们前面已经学习过，可以加载任意格式或者存储的数据为一张表。这里，我们将MySQL binglog的日志加载为一张表。 值得大家关注的参数主要有两组，第一组是binglog相关的： bingLogNamePrefix MySQL binglog配置的前缀。你可以咨询业务的DBA来获得。 binlogIndex 从第几个Binglog进行消费 binlogFileOffset 从单个binlog文件的第几个位置开始消费 binlogFileOffset并不能随便指定位置，因为他是二进制的，位置是有跳跃的。4表示一个binlogFileOffset的起始位置，是一个特殊的数字。 如果用户不想从起始位置开始，那么咨询DBA或者自己通过如下命令查看一个合理的位置： mysqlbinlog \\ --start-datetime=\"2019-06-19 01:00:00\" \\ --stop-datetime=\"2019-06-20 23:00:00\" \\ --base64-output=decode-rows \\ -vv master-bin.000004 如果随意指定了一个不合适的位置，展现出来的结果是数据无法得到消费，然后无法实现增量同步。 第二组参数是过滤哪些库的哪些表需要被同步： databaseNamePattern db的过滤正则表达式 tableNamePattern 表名的过滤正则表达式 现在我们得到了包含了binlog的table1, 我们现在要通过它将数据同步到Delta表中。这里一定需要了解，我们是同步数据， 而不是同步binglog本身。 我们将table1持续更新到delta中。具体代码如下： save append table1 as rate.`mysql_{db}.{table}` options mode=\"Append\" and idCols=\"id\" and duration=\"5\" and syncType=\"binlog\" and checkpointLocation=\"/tmp/cpl-binlog2\"; 这里，我们对每个参数都会做个解释。 mysql_{db}.{table} 中的 db,table是占位符。因为我们一次性会同步很多数据库的多张表，如果全部手动指定会显得 非常的麻烦和低效。MLSQL的 rate 数据源允许我们通过占位符进行替换。 第二个是idCols， 这个参数已经在前面Delta数据库的章节中和大家见过面。idCols需要用户指定一组联合主键，使得MLSQL能够完成 Upsert语义。 第三个syncType表示我们同步的是binlog,这样才会执行binlog特有的操作。 最后两个参数duration,checkpointLocation 则是流式计算特有的，分别表示运行的周期以及运行日志存放在哪。 现在，我们已经完成了我们的目标，将任意表同步到Delta数据湖中！ 目前Binlog 同步有一些限制： MySQL 需要配置 binlog_format=Row。 当然这个理论上是默认设置。 只支持binglog中 update/delete/insert 动作的同步。如果修改了数据库表结构，默认会同步失败，用户需要重新全量同步之后再进行增量同步。 如果希望能够继续运行，可以在Save语句中设置mergeSchema=\"true\"。 如果不同的表有不同的主键列(需要配置不同的idCols),那么可能需要些多个流式同步脚本。 常见错误 如果一直出现 Trying to restore lost connectioin to ..... Connected to .... 那么看看MySQL的my.cnf中的server_id参数是不是有配置。            该文件修订时间： 2021-01-12 10:27:46 "},"datasource/":{"url":"datasource/","title":"加载和存储多种数据源","keywords":"","body":"加载和存储多种数据源 MLSQL 具备加载和存储多种数据源的能力。本章节我们会给出常见数据源的使用示例。            该文件修订时间： 2021-01-12 10:27:46 "},"datasource/jdbc.html":{"url":"datasource/jdbc.html","title":"JDBC","keywords":"","body":"JDBC 加载数据 JDBC其实是一类数据源，比如MySQL, Oracle,Hive thrift server等等，只要数据源支持JDBC协议，那么就可以 通过JDBC进行加载。在这里，我们会以MySQL为主要例子进行介绍，其他也会提及。 首先我们需要建立连接，这是通过connect语法来完成的： set user=\"root\"; connect jdbc where url=\"jdbc:mysql://127.0.0.1:3306/wow?characterEncoding=utf8&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false\" and driver=\"com.mysql.jdbc.Driver\" and user=\"${user}\" and password=\"${password}\" as db_1; 这句话表示，我希望连接jdbc数据源，连接相关的参数在where语句里，驱动是MySQL的驱动，然后设置用户名和密码。 我们把我们这个链接叫做db_1,其实就是wow的别名，wow是MySQL里的一个DB. 接着我就可以这个数据库里加载任意表了： load jdbc.`db_1.table1` as table1; load jdbc.`db_1.table2` as table2; select * from table1 as output; 下面是一些参见参数： Property Name Meaning url The JDBC URL to connect to. The source-specific connection properties may be specified in the URL. e.g., jdbc:postgresql://localhost/test?user=fred&password=secret dbtable The JDBC table that should be read. Note that anything that is valid in a FROM clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses. driver The class name of the JDBC driver to use to connect to this URL. partitionColumn, lowerBound, upperBound These options must all be specified if any of them is specified. In addition, numPartitions must be specified. They describe how to partition the table when reading in parallel from multiple workers. partitionColumn must be a numeric column from the table in question. Notice that lowerBound and upperBound are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned. This option applies only to reading. numPartitions The maximum number of partitions that can be used for parallelism in table reading and writing. This also determines the maximum number of concurrent JDBC connections. If the number of partitions to write exceeds this limit, we decrease it to this limit by calling coalesce(numPartitions) before writing. fetchsize The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows). This option applies only to reading. batchsize The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing. It defaults to 1000. isolationLevel The transaction isolation level, which applies to current connection. It can be one of NONE, READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, or SERIALIZABLE, corresponding to standard transaction isolation levels defined by JDBC's Connection object, with default of READ_UNCOMMITTED. This option applies only to writing. Please refer the documentation in java.sql.Connection. sessionInitStatement After each database session is opened to the remote DB and before starting to read data, this option executes a custom SQL statement (or a PL/SQL block). Use this to implement session initialization code. Example: option(\"sessionInitStatement\", \"\"\"BEGIN execute immediate 'alter session set \"_serial_direct_read\"=true'; END;\"\"\") truncate This is a JDBC writer related option. When SaveMode.Overwrite is enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it. This can be more efficient, and prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. It defaults to false. This option applies only to writing. createTableOptions This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., CREATE TABLE t (name string) ENGINE=InnoDB.). This option applies only to writing. createTableColumnTypes The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g: \"name CHAR(64), comments VARCHAR(1024)\"). The specified types should be valid spark sql data types. This option applies only to writing. customSchema The custom schema to use for reading data from JDBC connectors. For example, \"id DECIMAL(38, 0), name STRING\". You can also specify partial fields, and the others use the default type mapping. For example, \"id DECIMAL(38, 0)\". The column names should be identical to the corresponding column names of JDBC table. Users can specify the corresponding data types of Spark SQL instead of using the defaults. This option applies only to reading. 其中，partitionColumn, lowerBound, upperBound,numPartitions 用来控制加载表的并行度。如果你 加载数据太慢，那么可以调整着几个参数。 MLSQL内置参数： Property Name Meaning prePtnArray Prepartitioned array, default comma delimited prePtnDelimiter Prepartition separator 预分区使用样例： load jdbc.`db.table` options and driver=\"com.mysql.jdbc.Driver\" and url=\"jdbc:mysql://127.0.0.1:3306/....\" and user=\"...\" and password=\"....\" and prePtnArray = \"age 10\" and prePtnDelimiter = \"\\|\" as table1; 当然，我们也可以不用connect语法，直接使用Load语法： load jdbc.`db.table` options and driver=\"com.mysql.jdbc.Driver\" and url=\"jdbc:mysql://127.0.0.1:3306/....\" and user=\"...\" and password=\"....\" as table1; 值得注意的是，JDBC还支持使用MySQL原生SQL的方式去加载MySQL数据。比如： load jdbc.`db_1.test1` where directQuery=''' select * from test1 where a = \"b\" ''' as newtable; select * from newtable; 这种情况要求加载的数据集不能太大。 如果你希望对这个语句也进行权限控制，如果是到表级别，那么只要系统开启授权即可。 如果是需要控制到列，那么启动时需要添加如下参数： --conf \"spark.mlsql.enable.runtime.directQuery.auth=true\" 保存更新数据 如同加载数据一样，你可以复用前面的数据连接： set user=\"root\"; connect jdbc where url=\"jdbc:mysql://127.0.0.1:3306/wow?characterEncoding=utf8&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false\" and driver=\"com.mysql.jdbc.Driver\" and user=\"${user}\" and password=\"${password}\" as db_1; 接着对得到的数据进行保存： save append tmp_article_table as jdbc.`db_1.crawler_table`; 这句话表示，我们需要保存，并且使用追加的方式，往 db_1中的 crawler_table里添加数据，这些数据来源于表 tmp_article_table。 如果你需要覆盖请使用 save overwrite .... 如果你希望先创建表，然后再写入表，那么你可以使用ET JDBC,该ET本质上是在Driver端执行各种操作指令的。 run command as JDBC.`db_1._` where `driver-statement-0`=\"drop table test1\" and `driver-statement-1`=\"create table test1.....\"; save append tmp_article_table as jdbc.`db_1.test1`; 这段语句，我们先删除test1,然后创建test1,最后使用save语句把进行数据结果的保存。 注意： JDBC.后面的路径要写成 db_1._,表示忽略表名。 如何执行Upsert语义(目前只支持MySQL) 要让MLSQL在保存数据时执行Upsert语义的话，你只需要提供提供idCol字段即可。下面是一个简单的例子： save append tmp_article_table as jdbc.`db_1.test1` where idCol=\"a,b,c\"; MLSQL内部使用了MySQL的duplicate key语法，所以用户需要对应的数据库表确实有重复联合主键的约束。那如果没有实现在数据库层面定义联合约束主键呢？ 结果会是数据不断增加，而没有执行update操作。 idCol的作用有两个，一个是标记，标记数据需要执行Upsert操作，第二个是确定需要的更新字段，因为主键自身的字段是不需要更新的。MLSQL会将表所有的字段减去 idCol定义的字段，得到需要更新的字段。 如何将流式数据写入MySQL 下面有个非常简单的例子： set streamName=\"mysql-test\"; ....... save append table21 as streamJDBC.`mysql1.test1` options mode=\"Complete\" and `driver-statement-0`=\"create table if not exists test1(k TEXT,c BIGINT)\" and `statement-0`=\"insert into wow.test1(k,c) values(?,?)\" and duration=\"3\" and checkpointLocation=\"/tmp/cpl3\"; 我们使用streamJDBC数据源可以完成将数据写入到MySQL中。driver-statement-0 在整个运行期间只会执行一次。statement-0 则会针对每条记录执行。 insert语句中的占位符顺序需要和table21中的列顺序保持一致。            该文件修订时间： 2021-01-15 10:43:42 "},"datasource/es.html":{"url":"datasource/es.html","title":"ElasticSearch","keywords":"","body":"ElasticSearch ElasticSearch 是一个应用很广泛的数据系统。MLSQL也支持将其中的某个索引加载为表。 注意，ES的包并没有包含在MLSQL默认发行包里，所以你需要通过 --jars 带上相关的依赖才能使用。 加载数据 示例： set data=''' {\"jack\":\"cool\"} '''; load jsonStr.`data` as data1; save overwrite data1 as es.`twitter/cool` where `es.index.auto.create`=\"true\" and es.nodes=\"127.0.0.1\"; load es.`twitter/cool` where and es.nodes=\"127.0.0.1\" as table1; select * from table1 as output1; connect es where `es.index.auto.create`=\"true\" and es.nodes=\"127.0.0.1\" as es_instance; load es.`es_instance/twitter/cool` as table1; select * from table1 as output2; 在ES里，数据连接引用和表之间的分隔符不是.,而是/。 这是因为ES索引名允许带\".\"。 所以es相关的参数可以参考驱动官方文档。            该文件修订时间： 2021-01-12 10:27:46 "},"datasource/solr.html":{"url":"datasource/solr.html","title":"Solr","keywords":"","body":"Solr Solr 是一个应用很广泛的存储。MLSQL也支持将其中的某个索引加载为表。 注意，Solr的包并没有包含在MLSQL默认发行包里，所以你需要通过 --jars 带上相关的依赖才能使用。 加载数据 示例： select 1 as id, \"this is mlsql_example\" as title_s as mlsql_example_data; connect solr where `zkhost`=\"127.0.0.1:9983\" and `collection`=\"mlsql_example\" and `flatten_multivalued`=\"false\" as solr1 ; load solr.`solr1/mlsql_example` as mlsql_example; save mlsql_example_data as solr.`solr1/mlsql_example` options soft_commit_secs = \"1\"; 在Solr里，数据连接引用和表之间的分隔符不是.,而是/。 这是因为Solr索引名允许带\".\"。 所有Solr相关的参数可以参考驱动官方文档。            该文件修订时间： 2021-01-12 10:27:46 "},"datasource/hbase.html":{"url":"datasource/hbase.html","title":"HBase","keywords":"","body":"HBase HBase 是一个应用很广泛的存储系统。MLSQL也支持将其中的某个索引加载为表。 注意，HBase的包并没有包含在MLSQL默认发行包里，所以你需要通过 --jars 带上相关的依赖才能使用。用户有三种种方式获得 HBase Jar包： 第一种，访问spark-hbase,然后自己进行编译。 第二种，通过官网下载已经打包的好的。地址 第三种(推荐)，直接使用Datasource插件：https://github.com/allwefantasy/mlsql-pluins/tree/master/ds-hbase-2x 你可以直接通过如下指令安装： !plugin ds add tech.mlsql.plugins.ds.MLSQLHBase2x ds-hbase-2x; 因为HBase依赖很多，大概80多M,下载会比较慢。 MLSQL Code: set rawText=''' {\"id\":9,\"content\":\"Spark好的语言1\",\"label\":0.0} {\"id\":10,\"content\":\"MLSQL是一个好的语言7\",\"label\":0.0} {\"id\":12,\"content\":\"MLSQL是一个好的语言7\",\"label\":0.0} '''; load jsonStr.`rawText` as orginal_text_corpus; select cast(id as String) as rowkey,content,label from orginal_text_corpus as orginal_text_corpus1; connect hbase2x where `zk`=\"127.0.0.1:2181\" and `family`=\"cf\" as hbase1; save overwrite orginal_text_corpus1 as hbase2x.`hbase1:mlsql_example`; load hbase2x.`hbase1:mlsql_example` where field.type.label=\"DoubleType\" as mlsql_example ; select * from mlsql_example as show_data; DataFrame Code: val data = (0 to 255).map { i => HBaseRecord(i, \"extra\") } val tableName = \"t1\" val familyName = \"c1\" import spark.implicits._ sc.parallelize(data).toDF.write .options(Map( \"outputTableName\" -> cat, \"family\" -> family ) ++ options) .format(\"org.apache.spark.sql.execution.datasources.hbase2x\") .save() val df = spark.read.format(\"org.apache.spark.sql.execution.datasources.hbase2x\").options( Map( \"inputTableName\" -> tableName, \"family\" -> familyName, \"field.type.col1\" -> \"BooleanType\", \"field.type.col2\" -> \"DoubleType\", \"field.type.col3\" -> \"FloatType\", \"field.type.col4\" -> \"IntegerType\", \"field.type.col5\" -> \"LongType\", \"field.type.col6\" -> \"ShortType\", \"field.type.col7\" -> \"StringType\", \"field.type.col8\" -> \"ByteType\" ) ).load() 一些有用的配置参数： Property Name Meaning tsSuffix to overwrite hbase value's timestamp namespace hbase namespace family hbase family，family=\"\" means load all existing families field.type.ck specify type for ck(field name),now supports:LongType、FloatType、DoubleType、IntegerType、BooleanType、BinaryType、TimestampType、DateType，default: StringType。            该文件修订时间： 2021-01-12 10:27:46 "},"datasource/mongodb.html":{"url":"datasource/mongodb.html","title":"MongoDB","keywords":"","body":"MongoDB MongoDB 是一个应用很广泛的存储系统。MLSQL也支持将其中的某个索引加载为表。 注意，MongoDB，所以你需要通过 --jars 带上相关的依赖才能使用。 加载数据 示例： set data=''' {\"jack\":\"cool\"} '''; load jsonStr.`data` as data1; save overwrite data1 as mongo.`twitter/cool` where partitioner=\"MongoPaginateBySizePartitioner\" and uri=\"mongodb://127.0.0.1:27017/twitter\"; load mongo.`twitter/cool` where partitioner=\"MongoPaginateBySizePartitioner\" and uri=\"mongodb://127.0.0.1:27017/twitter\" as table1; select * from table1 as output1; connect mongo where partitioner=\"MongoPaginateBySizePartitioner\" and uri=\"mongodb://127.0.0.1:27017/twitter\" as mongo_instance; load mongo.`mongo_instance/cool` as table1; select * from table1 as output2; load mongo.`cool` where partitioner=\"MongoPaginateBySizePartitioner\" and uri=\"mongodb://127.0.0.1:27017/twitter\" as table1; select * from table1 as output3; 在MongoDB里，数据连接引用和表之间的分隔符不是.,而是/。            该文件修订时间： 2021-01-12 10:27:46 "},"datasource/file.html":{"url":"datasource/file.html","title":"Parquet/Json/Text/Xml/Csv","keywords":"","body":"Parquet/Json/Text/Xml/Csv MLSQL支持大部分HDFS/本地文件数据读取。对于数据的保存或者加载，后面都可以接where语句。 where语句的配置选项随着数据源的不同而不同个，比如csv格式需要设置是否保留header。一些共性 需求如保存后的文件数，可以同构fileNum等参数设置。 值得注意的是，where条件所有的value都必须是字符串，也就是必须用\" 或者'''括起来。所有value可以使用set得到的 变量。下面是一些示例： Parquet set rawData=''' {\"jack\":1,\"jack2\":2} {\"jack\":2,\"jack2\":3} '''; set savePath=\"/tmp/jack\"; load jsonStr.`rawData` as table1; save overwrite table1 as parquet.`${savePath}`; load parquet.`${savePath}` as table2; Json set rawData=''' {\"jack\":1,\"jack2\":2} {\"jack\":2,\"jack2\":3} '''; set savePath=\"/tmp/jack\"; load jsonStr.`rawData` as table1; save overwrite table1 as json.`${savePath}`; load json.`${savePath}` as table2; csv set rawData=''' {\"jack\":1,\"jack2\":2} {\"jack\":2,\"jack2\":3} '''; set savePath=\"/tmp/jack\"; load jsonStr.`rawData` as table1; save overwrite table1 as csv.`${savePath}` where header=\"true\"; load csv.`${savePath}` as table2 where header=\"true\"; text set rawData=''' {\"jack\":1,\"jack2\":2} {\"jack\":2,\"jack2\":3} '''; set savePath=\"/tmp/jack\"; load jsonStr.`rawData` as table1; save overwrite table1 as json.`${savePath}`; load text.`${savePath}` as table2; Xml set rawData=''' {\"jack\":1,\"jack2\":2} {\"jack\":2,\"jack2\":3} '''; set savePath=\"/tmp/jack\"; load jsonStr.`rawData` as table1; save overwrite table1 as xml.`${savePath}`; load xml.`${savePath}` as table2;            该文件修订时间： 2021-01-12 10:27:46 "},"datasource/mlsql_source.html":{"url":"datasource/mlsql_source.html","title":"jsonStr/script/mlsqlAPI/mlsqlConf","keywords":"","body":"jsonStr/script/mlsqlAPI/mlsqlConf MLSQL 内置了一些特殊的数据源，比如jsonStr主要是为了加载通过set语法得到的json字符串，然后解析成表。 script则是为了加载一些脚本成为表，方便后续引用。 jsonStr/script jsonStr我们前面已经遇到多次： set rawData=''' {\"jack\":1,\"jack2\":2} {\"jack\":2,\"jack2\":3} '''; load jsonStr.`rawData` as table1; 这个对于调试异常方便。当然也可以作为模板来使用。 script使用方式和jsonStr类似，我们来看一个例子： -- 定义了一段python脚本，之后需要通过script load下进行使用 set python_script=''' import os import warnings import sys import mlsql if __name__ == \"__main__\": warnings.filterwarnings(\"ignore\") tempDataLocalPath = mlsql.internal_system_param[\"tempDataLocalPath\"] isp = mlsql.params()[\"internalSystemParam\"] tempModelLocalPath = isp[\"tempModelLocalPath\"] if not os.path.exists(tempModelLocalPath): os.makedirs(tempModelLocalPath) with open(tempModelLocalPath + \"/result.txt\", \"w\") as f: f.write(\"jack\") '''; load script.`python_script` as python_script; run testData as PythonParallelExt.`${modelPath}` where scripts=\"python_script\" and entryPoint=\"python_script\" and condaFile=\"....\"; 只有通过script 进行load之后才能方便的被后面的语句引用。后面的文档中我们会看到更多的 使用示例。 mlsqlAPI/mlsqlConf mlsqlAPI 可以查看所有API,mlsqlConf可以查看所有启动脚本参数。具体使用如下： load mslqlAPI.`` as api_table; load mlsqlConf.`` as confg_table; csvStr 同jsonStr，脚本数据源也支持csvStr。例如： set rawData=''' name,age zhangsan,1 lisi,2 '''; load csvStr.`rawData` options header=\"true\" as output;            该文件修订时间： 2021-01-12 10:27:46 "},"system_udf/":{"url":"system_udf/","title":"内置常见UDF","keywords":"","body":"系统UDF函数列表 MLSQL 自身提供了非常多的有用的UDF函数。            该文件修订时间： 2021-01-12 10:27:47 "},"system_udf/http.html":{"url":"system_udf/http.html","title":"http请求","keywords":"","body":"http 请求 http请求可以让MLSQL脚本变得更加强大，因为这可以集合所有内部或者外部API来完成某项工作。 MLSQL提供了一个支持较为全面http请求函数。 比如： select crawler_http(\"http://www.csdn.net\",\"GET\",map(\"k1\",\"v1\",\"k2\",\"v2\")) as c as output; 其中GET部分支持： GET POST MLSQL也提供了图片请求功能： select crawler_request_image(\"http://www.csdn.net\",\"GET\",map(\"k1\",\"v1\",\"k2\",\"v2\")) as c as output; 这个时候c字段是一个array[byte]类型。 我们提供了简单的标题和内容抽取函数： crawler_auto_extract_body crawler_auto_extract_title 给定一段文本，即可完成。 也支持xpath,对应的函数为 crawler_extract_xpath(html,xpath)。            该文件修订时间： 2021-01-12 10:27:47 "},"system_udf/vec.html":{"url":"system_udf/vec.html","title":"常见函数","keywords":"","body":"常见函数 sleep 休眠函数，方便调试。 -- 休眠1s select sleep(1000) uuid 产生一个唯一的字符串，去掉了\"-\" -- 休眠1s select uuid() 使用场景如生成一张随机表： set table = `select uuid()` options type=\"sql\"; matrix_array 矩阵转数组 select matrix_array(array_onehot(array(1,2),12)) vec_argmax 找到向量里面最大值所在的位置。 -- 休眠1s select vec_argmax(vec_dense(array(1.0,2.0,7.0))) as label vec_dense 生成一个紧凑向量 select vec_dense(array(1.0,2.0,7.0)) vec_sparse 生成一个稀疏向量 select vec_sparse(2,--此处传递一个map) vec_concat 拼接多个向量成为一个向量 select vec_concat( array( vec_dense(array(1.0)), vec_dense(array(1.0)) ) ) vec_cosine 计算consine 向量夹角 select vec_cosine(vec_dense(array(1.0,2.0)),vec_dense(array(1.0,1.0))) vec_slice 切割vector: select vec_slice(vec_dense(array(1.0,2.0,3.0)),array(0,1)) vec_array 把向量转化为数组 select vec_array(vec_dense(array(1.0,2.0))) vec_mk_string 把向量进行拼接 select vec_mk_string(vec_dense(array(1.0,2.0))) vec_wise_mul select vec_dense(cast(array(2.5, 2.0, 1.0) as array)) as f as data; select vec_wise_mul(f, f) as nf from data; [ { \"nf\": { \"type\": 1, \"values\": [ 6.25, 4.0, 1.0 ] } } ] vec_wise_add select vec_dense(cast(array(2.5, 2.0, 1.0) as array)) as f as data; select vec_wise_add(f, f) as nf from data; [ { \"nf\": { \"type\": 1, \"values\": [ 5.0, 4.0, 2.0 ] } } ] vec_wise_dif select vec_dense(cast(array(2.5, 2.0, 1.0) as array)) as f1, vec_dense(cast(array(2.5, 22.2, 1.6) as array)) as f2 as data; select vec_wise_dif(f1, f2) as nf from data; [ { \"nf\": { \"type\": 1, \"values\": [ 0.0, -20.2, -0.6000000000000001 ] } } ] vec_wise_mod select vec_dense(cast(array(2.5, 2.0, 1.0) as array)) as f1, vec_dense(cast(array(2.5, 4.0, 3.0) as array)) as f2 as data; select vec_wise_mod(f1, f2) as nf from data; [ { \"nf\": { \"type\": 1, \"values\": [ 0.0, 2.0, 1.0 ] } } ] vec_inplace_add select vec_dense(cast(array(2.5, 2.0, 1.0) as array)) as f as data; select vec_inplace_add(f, 4.4) as nf from data; [ { \"nf\": { \"type\": 1, \"values\": [ 6.9, 6.4, 5.4 ] } } ] vec_inplace_ew_mul select vec_dense(cast(array(2.5, 2.0, 1.0) as array)) as f as data; select vec_inplace_ew_mul(f, 4.4) as nf from data; [ { \"nf\": { \"type\": 1, \"values\": [ 11.0, 8.8, 4.4 ] } } ] vec_ceil select vec_dense(cast(array(2.5, 2.4, 1.6) as array)) as f as data; select vec_ceil(f) as nf from data; [ { \"nf\": { \"type\": 1, \"values\": [ 3.0, 3.0, 2.0 ] } } ] vec_floor select vec_dense(cast(array(2.5, 2.4, 1.6) as array)) as f as data; select vec_floor(f) as nf from data; [ { \"nf\": { \"type\": 1, \"values\": [ 2.0, 2.0, 1.0 ] } } ] vec_mean 向量平均值 select vec_mean(vec_dense(array(1.0,2.0,7.0, 2.0))) [ { \"UDF:vec_mean(UDF:vec_dense(cast(array(1.0, 2.0, 7.0, 2.0) as array)))\": 3.0 } ] vec_stddev 向量标准差 select vec_stddev(vec_dense(array(3.0, 4.0, 5.0))) [ { \"UDF:vec_stddev(UDF:vec_dense(cast(array(3.0, 4.0, 5.0) as array)))\": 1.0 } ] ngram select ngram(array(\"a\",\"b\",\"c\",\"d\",\"e\"),3) as 3ngr array_intersect select array_intersect(array(\"a\",\"b\",\"c\",\"d\",\"e\"),array(\"a\")) as k array_index select array_index(array(\"a\",\"b\",\"c\",\"d\",\"e\"),\"b\") as k array_slice select array_slice(array(\"a\",\"b\",\"c\",\"d\",\"e\"),3,-1) as k array_number_concat 多个数组拼接成一个数组，并且展开。比如 [[1,2],[2,3]] => [1,2,2,3] array_concat 同array_number_concat，支持元素类型为字符串 array_number_to_string 对数组内的元素做类型转换 array_string_to_double 对数组内的元素做类型转换 array_string_to_float 对数组内的元素做类型转换 array_string_to_int 对数组内的元素做类型转换 matrix_dense 生成一个紧凑矩阵 select matrix_dense(array(array(1.0, 2.0, 3.0), array(2.0, 3.0, 4.0))) [ { \"UDF:matrix_dense(cast(array(array(1.0, 2.0, 3.0), array(2.0, 3.0, 4.0)) as array>))\": { \"isTransposed\": false, \"numCols\": 3, \"numRows\": 2, \"type\": 1, \"values\": [ 1.0, 2.0, 2.0, 3.0, 3.0, 4.0 ] } } ] matrix_sum select matrix_sum(matrix_dense(array(array(1.0, 2.0, 3.0), array(2.0, 3.0, 4.0))), 0) [ { \"UDF:matrix_sum(UDF:matrix_dense(cast(array(array(1.0, 2.0, 3.0), array(2.0, 3.0, 4.0)) as array>)), 0)\": { \"type\": 1, \"values\": [ 3.0, 5.0, 7.0 ] } } ] keepChinese 对文本字段做处理，只保留中文字符 set query = \"你◣◢︼【】┅┇☽☾✚〓▂▃▄▅▆▇█▉▊▋▌▍▎▏↔↕☽☾の·▸◂▴▾┈┊好◣◢︼【】┅┇☽☾✚〓▂▃▄▅▆▇█▉▊▋▌▍▎▏↔↕☽☾の·▸◂▴▾┈┊啊，..。，！?katty\" select keepChinese(\"${query}\",false,array()) as jack as chitable -- 结果: 你好啊 第二个参数如果为true,则会保留一些常见的标点符号，第三个参数可以指定特定哪些字符需要保留字符。 度量函数 todo            该文件修订时间： 2021-01-12 10:27:47 "},"process/":{"url":"process/","title":"一些有用的内置ET","keywords":"","body":"一些有用的内置ET MLSQL 具备非常强的数据处理功能。其主要体现在： 完整的SQL功能 各种内置的Estimator/Transformer 支持Python/Scala自定义UDF/UDAF函数 部分Transformer内嵌Python项目支持 通常数据研发和分析师，产品经理等会使用前面三种。算法的同学除了前面三种以外，第四种也会用的比较多。            该文件修订时间： 2021-01-12 10:27:47 "},"process/estimator_transformer/TreeBuildExt.html":{"url":"process/estimator_transformer/TreeBuildExt.html","title":"计算复杂的父子关系","keywords":"","body":"计算复杂的父子关系 在SQL中计算父子关系无疑是复杂的，需要复杂的join关联，我看到分析师做了一个是个层级的关联，就已经是非常复杂的脚本了，但是 用户依然不买账。通常而言，用户计算父子关系，一般需要： 任意一个指定节点的所有子子孙孙节点。 任意一个指定节点树状层级 返回一个一个或者多个树状结构 假设我们要处理的数据样子是这样的： set jsonStr = ''' {\"id\":0,\"parentId\":null} {\"id\":1,\"parentId\":null} {\"id\":2,\"parentId\":1} {\"id\":3,\"parentId\":3} {\"id\":7,\"parentId\":0} {\"id\":199,\"parentId\":1} {\"id\":200,\"parentId\":199} {\"id\":201,\"parentId\":199} '''; 存在父子关系。 现在我想知道，给定节点7,那么7所有子子孙孙节点有多少，然后层级多深？ 具体的某个场景比如我们在做用户推广时，用户可以邀请，我想知道因为某个用户而邀请到了多少人， 邀请链有多深。 接着我们把这个数据映射成标，大家还记得load script指令么？ load jsonStr.`jsonStr` as data; 现在可以使用该模块了，我们用run指令： run data as TreeBuildExt.`` where idCol=\"id\" and parentIdCol=\"parentId\" and treeType=\"nodeTreePerRow\" as result; 大概结果是这个样子的： +---+-----+------------------+ |id |level|children | +---+-----+------------------+ |200|0 |[] | |0 |1 |[7] | |1 |2 |[200, 2, 201, 199]| |7 |0 |[] | |201|0 |[] | |199|1 |[200, 201] | |2 |0 |[] | +---+-----+------------------+ level是层级，children则是所有子元素（包括子元素的字元素）。 我们还可以直接生成N课书，然后你可以使用自定义udf函数继续处理这棵树： run data as TreeBuildExt.`` where idCol=\"id\" and parentIdCol=\"parentId\" and treeType=\"treePerRow\" as result; 结果如下： +----------------------------------------+---+--------+-----+ |children |id |parentID|level| +----------------------------------------+---+--------+-----+ |[[[], 7, 0]] |0 |null |1 | |[[[[[], 200, 199]], 199, 1], [[], 2, 1]]|1 |null |2 | +----------------------------------------+---+--------+-----+ children是一个嵌套row结构，可以在udf里很好的处理。            该文件修订时间： 2021-01-12 10:27:47 "},"process/estimator_transformer/RepartitionExt.html":{"url":"process/estimator_transformer/RepartitionExt.html","title":"改变表的分区数","keywords":"","body":"改变表的分区数 很多时候，我们需要改变分区数，比如保存文件之前，或者我们使用python,我们希望python worker尽可能的并行运行,这个时候就需要 RepartitionExt的帮助了。 具体使用案例： set jsonStr = ''' {\"id\":0,\"parentId\":null} {\"id\":1,\"parentId\":null} {\"id\":2,\"parentId\":1} {\"id\":3,\"parentId\":3} {\"id\":7,\"parentId\":0} {\"id\":199,\"parentId\":1} {\"id\":200,\"parentId\":199} {\"id\":201,\"parentId\":199} '''; load jsonStr.`jsonStr` as data; run data as RepartitionExt.`` where partitionNum=2 as newdata;            该文件修订时间： 2021-01-12 10:27:47 "},"process/estimator_transformer/SendMessage.html":{"url":"process/estimator_transformer/SendMessage.html","title":"如何发送邮件","keywords":"","body":"如何发送邮件 数据处理完成后，我们可以邮件通知业务方。 使用场景示例： 1、数据计算处理后，生成下载链接，发邮件给相关人员 2、数据量较少的情况，可以直接发送数据处理结果 发送邮件使用run语法，方式如下： set EMAIL_TITLE = \"这是邮件标题\"; set EMAIL_BODY = `select download_url from t1` options type = \"sql\"; set EMAIL_TO = \"\"; select \"${EMAIL_BODY}\" as content as data; run data as SendMessage.`` where method=\"mail\" and to = \"${EMAIL_TO}\" and subject = \"${EMAIL_TITLE}\" and smtpHost = \"xxxxxxxx\"; 下面给一个完整的例子： select download_csv_url(\"${DATA_FILE_PATH}\") as download_url as t1; set EMAIL_TITLE = \"这是邮件标题\"; set EMAIL_BODY = `select download_url from t1` options type = \"sql\"; set EMAIL_TO = \"\"; select \"${EMAIL_BODY}\" as content as data; run data as SendMessage.`` where method=\"mail\" and to = \"${EMAIL_TO}\" and subject = \"${EMAIL_TITLE}\" and smtpHost = \"xxxxxxxx\"; 其中 download_csv_url 是一个udf函数，他会使用一个接口服务根据路径获得一个下载地址，之后将这个地址发送给需要接收的人。            该文件修订时间： 2021-01-12 10:27:47 "},"process/estimator_transformer/mlsql-watcher.html":{"url":"process/estimator_transformer/mlsql-watcher.html","title":"数据采集组件mlsql-watcher使用","keywords":"","body":"MLSQL-Watcher插件使用 MLSQL-Watcher 可以收集一些关键数据到MySQL,然后可以通过MLSQL脚本计算对应的指标从而判别 一个MLSQL脚本是不是危险。具体的文章参考：如何实现Spark过载保护 开启方式 注册一个MySQL数据库即相当于开启收集功能: !watcher db add \"app_runtime_full\" ''' app_runtime_full: host: 127.0.0.1 port: 3306 database: app_runtime_full username: xxxx password: xxxxxx initialSize: 8 disable: false testWhileIdle: true maxWait: 100 filters: stat,log4j '''; 此时MLSQL会连接该数据库，并且每两秒收集一次数据写入MySQL. 设置日志保留时间 我们还需要设置MySQL的清理时间，避免MySQL数据集过大： !watcher cleaner 7d; 比如我这里设置保留七天的记录。支持秒(s),分钟(m),小时(h)等后缀。 加载数据表 你可以加载相关MySQL库表到你的MLSQL Console里： connect jdbc where url=\"jdbc:mysql://127.0.0.1:3306/app_runtime_full?characterEncoding=utf8&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false\" and driver=\"com.mysql.jdbc.Driver\" and user=\"xxxxx\" and password=\"xxxxxx\" as app_runtime_full; load jdbc.`app_runtime_full.w_executor` as w_executor; load jdbc.`app_runtime_full.w_executor_job` as w_executor_job; 之后就可以查询统计这些信息了。 下面一段脚本是针对每个executor计算一个特定MLSQL脚本的危险指数： ------------------------------------------------------------------ 参考文章：https://zhuanlan.zhihu.com/p/112608353 -------------------------------------------------------------------------------- -- 规整数据 select cluster_name,executor_name,group_id, (shuffle_local_bytes_read+shuffle_remote_bytes_read) as reads, shuffle_bytes_written as writes, shuffle_records_read as records_read, shuffle_records_read as records_writes, created_at from w_executor_job order by created_at as temp1; --找到最新的一次快照 select *, if((max(created_at) over())=created_at,0,1) as wow from temp1 where group_id=\"4b933c30-8f16-4cca-bac7-bc996d03b2fb\" as temp2; -- 计算平均值 select executor_name,reads+writes as rs,avg(reads) over () +avg(writes) over () as avg_rw from temp2 where wow=0 as temp3; -- 计算每个exeuctor 的不均衡值，此处每个节点只有一条记录 select executor_name,(rs-avg_rw)/(3*avg_rw) as rw_inbalance from temp3 as temp4; -- 计算 shuffle 速率，我们假设最大值为 1000M 每秒 对于一个节点 select *, (lag(reads,1) over (order by created_at)) as b_reads,(lag(writes,1) over (order by created_at)) as b_writes from temp1 where group_id=\"4b933c30-8f16-4cca-bac7-bc996d03b2fb\" as temp5; select executor_name,((reads-b_reads) + (writes-b_writes))/(1000*1024*1024) as speed from temp5 as temp5; -- 计算记录大小 单记录最大100m select executor_name,reads/records_read/(100*1024*1024) as record_size from temp2 where wow=0 as temp6; -- gc 时间 select name as executor_name,gc_time, if((max(created_at) over())=created_at,0,1) as wow, (lag(gc_time,1) over (order by created_at)) as b_gc_time from w_executor as temp7; select *,(gc_time-b_gc_time) as gc_s from temp7 where wow=0 as temp7; -- 最后根据executor join 起来，得到四个因子 select temp4.executor_name,temp4.rw_inbalance,temp5.speed,temp6.record_size,temp7.gc_s from temp4 left join temp5 on temp4.executor_name = temp5.executor_name left join temp6 on temp4.executor_name = temp6.executor_name left join temp7 on temp7.executor_name = temp4.executor_name as output;            该文件修订时间： 2021-01-12 10:27:47 "},"stream/":{"url":"stream/","title":"MLSQL流编程","keywords":"","body":"MLSQL流编程 MLSQL脚本也可以完成流式计算的功能，而且语法和批完全一致。MLSQL底层的流式引擎是Spark Structured Streaming。 读者有必要了解该引擎以便更好的使用MLSQL进行流编程.            该文件修订时间： 2021-01-12 10:27:47 "},"stream/datasource.html":{"url":"stream/datasource.html","title":"Kafka和MockStream","keywords":"","body":"Kafka和MockStream MLSQL 目前显式支持Kafka 以及MockStream. MockStream主要用于模拟数据源，测试场景中应用的比较多。 值得注意的是，MLSQL支持 Kafka 0.8,0.9以及1.x版本，而原生的struectured streaming只支持0.10版本的Kafka. 如果你使用的 kafka > 0.10： load kafka.`topic_name` options `kafka.bootstrap.servers`=\"---\" as kafka_post_parquet; 如果你使用的kafka 为 0.8/0.9 请使用： load kafka8.`topic_name` options `kafka.bootstrap.servers`=\"---\" as kafka_post_parquet; 获取了数据之后，MLSQL Stream支持的输出数据源有： 文件写入(比如 parquet,orc,json,csv等等) Kafka写入 以及MySQL写入。 具体使用如下： connect jdbc where driver=\"com.mysql.jdbc.Driver\" and url=\"jdbc:mysql://127.0.0.1:3306/wow\" and user=\"---\" and password=\"----\" as mysql1; save append table21 as streamJDBC.`mysql1.test1` options mode=\"Complete\" and `driver-statement-0`=\"create table if not exists test1(k TEXT,c BIGINT)\" and `statement-0`=\"insert into wow.test1(k,c) values(?,?)\" and duration=\"3\" and checkpointLocation=\"/tmp/cpl3\"; 只有save语法会触发整个流的提交和执行。这里面有几个核心的参数： duration，执行周期，单位为秒,如果是0,则执行完立马执行下一个周期。 checkpointLocation 流重启后恢复用 mode， 三种模式，Update,Append,Complete,请参考structured streaming里这三种模式的区别。 模拟输入数据源 为了方便测试，我们提供了一个Mock输入，来模拟Kafka输入。 -- mock some data. set data=''' {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":0,\"timestamp\":\"2008-01-24 18:01:01.001\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":1,\"timestamp\":\"2008-01-24 18:01:01.002\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":2,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":3,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":4,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":5,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} '''; -- load data as table load jsonStr.`data` as datasource; -- convert table as stream source load mockStream.`datasource` options stepSizeRange=\"0-3\" as newkafkatable1; 通过set 以及load语法我们制造了一批数据，然后呢，我们使用mockStream来加载这些数据，mockStream 会每个周期发送0到3条数据出来，这个通过stepSizeRange进行控制。 这样我们就可以脱离Kafka从而实现方便的代码测试。当然，你肯定很像知道newkafkatable1也就是我们加载完kafka后的数据该怎么处理， 和普通的批处理是类似的： select cast(key as string) as k,count(*) as c from newkafkatable1 group by key as table21; 下面是一个典型的流式程序： -- the stream name, should be uniq. set streamName=\"streamExample\"; -- connect mysql as the data sink. connect jdbc where driver=\"com.mysql.jdbc.Driver\" and url=\"jdbc:mysql://127.0.0.1:3306/wow\" and driver=\"com.mysql.jdbc.Driver\" and user=\"---\" and password=\"----\" as mysql1; -- mock some data. set data=''' {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":0,\"timestamp\":\"2008-01-24 18:01:01.001\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":1,\"timestamp\":\"2008-01-24 18:01:01.002\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":2,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":3,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":4,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":5,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} '''; -- load data as table load jsonStr.`data` as datasource; -- convert table as stream source load mockStream.`datasource` options stepSizeRange=\"0-3\" as newkafkatable1; -- aggregation select cast(key as string) as k,count(*) as c from newkafkatable1 group by key as table21; -- output the the result to console. -- save append table21 -- as console.`` -- options mode=\"Complete\" -- and duration=\"10\" -- and checkpointLocation=\"/tmp/cpl3\"; -- save the data to mysql. save append table21 as streamJDBC.`mysql1.test1` options mode=\"Complete\" and `driver-statement-0`=\"create table if not exists test1(k TEXT,c BIGINT)\" and `statement-0`=\"insert into wow.test1(k,c) values(?,?)\" and duration=\"3\" and checkpointLocation=\"/tmp/cpl3\"; 注意： 任何一个流程序都需要一个唯一的标识符，通过 set streamName=\"streamExample\"; 来设置。 如果你不想有任何依赖，就是想跑一个例子看看，可以使用如下的语句： -- the stream name, should be uniq. set streamName=\"streamExample\"; -- mock some data. set data=''' {\"key\":\"yes\",\"value\":\"a,b,c\",\"topic\":\"test\",\"partition\":0,\"offset\":0,\"timestamp\":\"2008-01-24 18:01:01.001\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"d,f,e\",\"topic\":\"test\",\"partition\":0,\"offset\":1,\"timestamp\":\"2008-01-24 18:01:01.002\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"k,d,j\",\"topic\":\"test\",\"partition\":0,\"offset\":2,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"m,d,z\",\"topic\":\"test\",\"partition\":0,\"offset\":3,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"o,d,d\",\"topic\":\"test\",\"partition\":0,\"offset\":4,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"m,m,m\",\"topic\":\"test\",\"partition\":0,\"offset\":5,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} '''; -- load data as table load jsonStr.`data` as datasource; -- convert table as stream source load mockStream.`datasource` options stepSizeRange=\"0-3\" and valueFormat=\"csv\" and valueSchema=\"st(field(column1,string),field(column2,string),field(column3,string))\" as newkafkatable1; -- aggregation select column1,column2,column3,kafkaValue from newkafkatable1 as table21; -- output the the result to console. save append table21 as console.`` options mode=\"Append\" and duration=\"10\" and checkpointLocation=\"/tmp/cpl3\"; 输出结果如下： ------------------------------------------- Batch: 6 ------------------------------------------- +-------+-------+-------+--------------------+ |column1|column2|column3| kafkaValue| +-------+-------+-------+--------------------+ | m| m| m|[yes, 0, 5, 2008-...| +-------+-------+-------+--------------------+ 注意： 使用console，每次重启你需要删除checkpointLocation            该文件修订时间： 2021-01-12 10:27:47 "},"stream/kakfa_tool.html":{"url":"stream/kakfa_tool.html","title":"MLSQL Kafka小工具集锦","keywords":"","body":"MLSQL Kafka小工具集锦 流式程序的一大特点就是调试没有批那么方便。为此，我们提供一些工具方便用户探索Kafka里的数据： 查看Kafka最新N条数据 !kafkaTool sampleData 10 records from \"127.0.0.1:9092\" wow; 这个命令表示我要采集10条数据，来源是Kafka集群\"127.0.0.1:9092\"，主题(topic)是wow。 自动推测Kafka的Schema !kafkaTool schemaInfer 10 records from \"127.0.0.1:9092\" wow; 句法格式和前面一致，唯一区别是换了个命令，把sampleData换成schemaInfer.目前只支持json格式。 查看流式程序的checkpoint目录的最新offset !kakfaTool offsetStream /tmp/ck;            该文件修订时间： 2021-01-12 10:27:47 "},"stream/data_convert.html":{"url":"stream/data_convert.html","title":"自动将Kafka中的JSON数据展开为表","keywords":"","body":"自动将Kafka中的JSON数据展开为表 在大多数系统里，Kafka的value都是json格式的，当然也有其他格式。这里我们会重点介绍如何使用MLSQL自动将json/csv展开成 表，方便后续操作。 自动推测（目前只支持json） -- the stream name, should be uniq. set streamName=\"kafkaStreamExample\"; -- sample 2 records and infer the schema from kafka server \"127.0.0.1:9092\" and the -- topic is wow. -- Make sure this statement is placed before the load statement. !kafkaTool registerSchema 2 records from \"127.0.0.1:9092\" wow; -- convert table as stream source load kafka.`wow` options kafka.bootstrap.servers=\"127.0.0.1:9092\" as newkafkatable1; -- aggregation select * from newkafkatable1 as table21; -- output the the result to console. save append table21 as rate.`/tmp/delta/wow-0` options mode=\"Append\" and duration=\"5\" and checkpointLocation=\"/tmp/s-cpl4\"; 核心是下面这一句： !kafkaTool registerSchema 2 records from \"127.0.0.1:9092\" wow; 系统会自动采集最新的两条Kafka数据，然后对数据进行schema推测，最后应用于流式程序里。 手动指定 我们先看如下一段代码： -- the stream name, should be uniq. set streamName=\"streamExample\"; -- mock some data. set data=''' {\"key\":\"yes\",\"value\":\"a,b,c\",\"topic\":\"test\",\"partition\":0,\"offset\":0,\"timestamp\":\"2008-01-24 18:01:01.001\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"d,f,e\",\"topic\":\"test\",\"partition\":0,\"offset\":1,\"timestamp\":\"2008-01-24 18:01:01.002\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"k,d,j\",\"topic\":\"test\",\"partition\":0,\"offset\":2,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"m,d,z\",\"topic\":\"test\",\"partition\":0,\"offset\":3,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"o,d,d\",\"topic\":\"test\",\"partition\":0,\"offset\":4,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"m,m,m\",\"topic\":\"test\",\"partition\":0,\"offset\":5,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} '''; -- load data as table load jsonStr.`data` as datasource; -- convert table as stream source load mockStream.`datasource` options stepSizeRange=\"0-3\" and valueFormat=\"csv\" and valueSchema=\"st(field(column1,string),field(column2,string),field(column3,string))\" as newkafkatable1; 在最后一个语句里，我们在加载数据流的时候，额外增加了 valueFormat和valueSchema. valueFormat告诉系统，Kafka value的存储格式是什么，在示例中是csv, valueSchema则告诉系统，里面的数据schema是什么。 为了方便的描述schema, MLSQL提供了一套简单的语法： st(field(column1,string),field(column2,string),field(column3,string)) st 表示StructType, field表示StructField。 分别表示表和字段，里面则是表名和数据类型。schema支持如下的数据结构： st field string float double integer short date binary map array 比如，你也可以这么描述一个json的数据：{\"column1\":{\"key\":\"value\"}} st(field(column1,map(string,string))) 当然也可以描述的更复杂一些，支持st的嵌套。 st(field(column1,map(string,array(st(field(columnx,string))))))            该文件修订时间： 2021-01-12 10:27:47 "},"stream/query_kafka.html":{"url":"stream/query_kafka.html","title":"如何高效的AdHoc查询Kafka","keywords":"","body":"如何高效AdHoc查询Kafka 虽然Kafka是一个流式数据源，但是将其作为普通的数据源进行AdHoc查询，性能也是比较可观的。 本章节我们会介绍如何用MLSQL AdHoc查询Kafka. 基本用法 使用上非常简单，你只要使用adHocKafka 数据源去加载Kafka即可： load adHocKafka.`topic` where kafka.bootstrap.servers=\"127.0.0.1:9200\" and multiplyFactor=\"2\" as table1; select count(*) from table1 where value like \"%yes%\" as output; 这里multiplyFactor表示我们需要提升两倍并行度。基数是Kafka的分区数。设置为2，表示我们会启动两个线程扫描同一个分区，从而加快速度。我们还可以 设置开始截止时间： load adHocKafka.`topic` where kafka.bootstrap.servers=\"127.0.0.1:9200\" and multiplyFactor=\"2\" and timeFormat=\"yyyyMMdd\" and startingTime=\"20170101\" and endingTime=\"20180101\" as table1; select cast(value as string) as textValue from table1 as table2; select count(*) from table2 where textValue like \"%yes%\" as output; 当然也可以直接指定offset区间： load adHocKafka.`topic` where kafka.bootstrap.servers=\"127.0.0.1:9200\" and multiplyFactor=\"2\" and staringOffset=\"oldest\" and endingOffset=\"latest\" as table1; 因为指定offset比较麻烦，我们一般都不怎么使用。因为你需要指定每个分区的起始结束offset。            该文件修订时间： 2021-01-12 10:27:47 "},"stream/callback.html":{"url":"stream/callback.html","title":"如何设置流式计算回调","keywords":"","body":"如何设置流式计算回调 用户可以通过特定的命令查看一个流式程序的进度： !show progress/streamExample; 如果你忘记了自己流程序的名字，那么可以使用 !show jobs; 获得列表。如果我想收集一个流程序什么时候开始，运行的状态，以及如果异常或者被正常杀死的事件，用户可以使用回调，具体使用方式如下： -- the stream name, should be uniq. set streamName=\"streamExample\"; -- mock some data. set data=''' {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":0,\"timestamp\":\"2008-01-24 18:01:01.001\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":1,\"timestamp\":\"2008-01-24 18:01:01.002\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":2,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":3,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":4,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":5,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} '''; -- load data as table load jsonStr.`data` as datasource; -- convert table as stream source load mockStream.`datasource` options stepSizeRange=\"0-3\" as newkafkatable1; -- aggregation select cast(key as string) as k,count(*) as c from newkafkatable1 group by key as table21; -- run command as MLSQLEventCommand.`` where -- eventName=\"started,progress,terminated\" -- and handleHttpUrl=\"http://127.0.0.1:9002/jack\" -- and method=\"POST\" -- and params.a=\"\" -- and params.b=\"\"; !callback post \"http://127.0.0.1:9002/api_v1/test\" when \"started,progress,terminated\"; -- output the the result to console. save append table21 as console.`` options mode=\"Complete\" and duration=\"15\" and checkpointLocation=\"/tmp/cpl14\"; 核心关键点是： !callback post \"http://127.0.0.1:9002/api_v1/test\" when \"started,progress,terminated\"; 这个表示如果发生started,progress,terminated三个事件中的任何一个，都以HTTP POST协议上报给http://127.0.0.1:9002/api_v1/test接口。            该文件修订时间： 2021-01-12 10:27:47 "},"stream/batch.html":{"url":"stream/batch.html","title":"如何对流的结果以批的形式保存","keywords":"","body":"如何对流的结果以批的形式保存 MLSQL对流的数据源支持有限。如果我想流的数据保存到ES中，但是没有相应的流式ES数据源的实现该怎么办？为了解决这个问题， MLSQL提供了一个'custom' 流式数据源，可以方便的让你用批的方式操作流的结果数据。 我们来看看具体的示例代码： -- the stream name, should be uniq. set streamName=\"streamExample\"; -- mock some data. set data=''' {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":0,\"timestamp\":\"2008-01-24 18:01:01.001\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":1,\"timestamp\":\"2008-01-24 18:01:01.002\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":2,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":3,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":4,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} {\"key\":\"yes\",\"value\":\"no\",\"topic\":\"test\",\"partition\":0,\"offset\":5,\"timestamp\":\"2008-01-24 18:01:01.003\",\"timestampType\":0} '''; -- load data as table load jsonStr.`data` as datasource; -- convert table as stream source load mockStream.`datasource` options stepSizeRange=\"0-3\" as newkafkatable1; -- aggregation select cast(value as string) as k from newkafkatable1 as table21; -- run command as MLSQLEventCommand.`` where -- eventName=\"started,progress,terminated\" -- and handleHttpUrl=\"http://127.0.0.1:9002/jack\" -- and method=\"POST\" -- and params.a=\"\" -- and params.b=\"\"; !callback post \"http://127.0.0.1:9002/api_v1/test\" when \"started,progress,terminated\"; -- output the the result to console. save append table21 as custom.`` options mode=\"append\" and duration=\"15\" and sourceTable=\"jack\" and code=''' select count(*) as c from jack as newjack; save append newjack as parquet.`/tmp/jack`; ''' and checkpointLocation=\"/tmp/cpl15\"; 我们关注点放在最后一句： save append table21 as custom.`` options mode=\"append\" and duration=\"15\" and sourceTable=\"jack\" and code=''' select count(*) as c from jack as newjack; save append newjack as parquet.`/tmp/jack`; ''' and checkpointLocation=\"/tmp/cpl15\"; 有几个点需要注意： 数据源名称是 custom 我们需要将结果表通过sourceTable重新取名，这里我们把table21取名为jack,然后在子代码中使用。 code里允许你用批的形态操作jack表。 这样，我们就能很方便的将大部分数据写入到支持批的数据源中了。 Hive分区表写入 如果我们希望把数据写入hive分区表怎么办？依然只要修改最后一句。如果是动态分区， 可以按如下方式写： save append table21 as custom.`` options mode=\"append\" and duration=\"15\" and sourceTable=\"jack\" and code=''' save append jack as hive.`/tmp/jack` partitionBy 【partitionCol】; ''' and checkpointLocation=\"/tmp/cpl15\"; 如果是静态分区，则直接指定目录即可。另外也可以使用hive原生insert语句,例如下面的例子 --开启hive相关配置，放在脚本前面 set hive.exec.dynamic.partition=true where type=\"conf\"; set hive.exec.dynamic.partition.mode=nostrict where type=\"conf\"; save append table21 as custom.`` options mode=\"append\" and duration=\"15\" and sourceTable=\"jack\" and code=''' insert into table db.tb partition(【partitionCol】) select * from jack; ''' and checkpointLocation=\"/tmp/cpl15\";            该文件修订时间： 2021-01-15 11:00:50 "},"stream/window_wartermark.html":{"url":"stream/window_wartermark.html","title":"window/watermark的使用","keywords":"","body":"window/watermark的使用 window/watermark是流式计算里特有的概念，下面是一个具体的使用模板： select ts,f1 as table1; -- register watermark for table1 register WaterMarkInPlace.`table1` as tmp1 options eventTimeCol=\"ts\" and delayThreshold=\"10 seconds\"; -- process table1 select f1,count(*) as num from table1 -- window size is 30 minutes,slide is 10 seconds group by f1, window(ts,\"30 minutes\",\"10 seconds\") as table2; save append table2 as .... 得到一张表之后，你可以通过WaterMarkInPlace对该表进行watermark的设置，核心参数是 eventTimeCol 以及 delayThreshold， 分别设置evetTime字段和延时。 对于window没有特殊用法，只是一个函数。            该文件修订时间： 2021-01-12 10:27:47 "},"stream/stream_mysql_update.html":{"url":"stream/stream_mysql_update.html","title":"如何使用MLSQL流式更新MySQL数据","keywords":"","body":"如何使用MLSQL流式更新MySQL数据 很多场景会要求使用MLSQL把流式数据写入MySQL,而且通常都是upsert语义，也就是根据特定字段组合， 如果存在了则更新，如果不存在则插入。MLSQL在批处理层面，你只要配置一个idCol字段就可以实现upsert语义。 那么在流式计算里呢？ 同样也是很简单的。 下面的例子来源于用户讨论. 场景 有一个patient的主题，里面包含了 name,string age,integer addr,string (means the province the patient from) arriveTime,long (timestamp in miliseconds) 最终我们要求根据addr,arriveTime作为uniq key，如果存在更新，否则insert,并且把数据存储在MySQL里。 流程 我们可以通过load语法把Kafka的topic加载成一张表： set streamName=\"test_patient_count_update\"; load kafka.`patient` options `kafka.bootstrap.servers`=\"dn3:9092\" and `valueFormat`=\"json\" and `valueSchema`=\"st(field(name,string),field(age,integer),field(addr,string),field(arriveTime,string))\" as patientKafkaData; 因为kafka里的数据是json格式的，我希望直接展开成json表，所以配置了valueFormat和valueSchema，更多细节 参考前面的章节。 接着我要链接一个数据库： connect jdbc where url=\"jdbc:mysql://dn1:3306/streamingpro-cluster?characterEncoding=utf8&zeroDateTimeBehavior=convertToNull\" and driver=\"com.mysql.jdbc.Driver\" and user=\"XXX\" and password=\"XXXXX\" as mydb; 第一步我们考虑使用存储过程（以及相关的表） set procStr=''' CREATE DEFINER=`app`@`%` PROCEDURE `test_proc`(in `dt` date,in `addr` VARCHAR(100),in `num` BIGINT) begin DECLARE cnt bigint; select p.num into cnt from patientUpdate p where p.dt=`dt` and p.addr=`addr`; if ISNULL(CNT) then INSERT into patientUpdate(dt,addr,num) values(`dt`,`addr`,`num`); else update patientUpdate p set p.num=`num` where p.dt=`dt` and p.addr=`addr`; end if; end ''' ; -- create table and procedure 创建table和存储过程 select 1 as a as FAKE_TABLE; run FAKE_TABLE as JDBC.`mydb` where `driver-statement-0`=\"create table if not exists patientUpdate(dt date,addr varchar(100),num BIGINT)\" and `driver-statement-1`=\"DROP PROCEDURE IF EXISTS test_proc;\" and `driver-statement-2`=\"${procStr}\" 接着对数据进行操作，获得需要写入到数据库的表结构： select name,addr,cast(from_unixtime(arriveTime/1000) as date) as dt from patientKafkaData as patient; select dt,addr,count(*) as num from patient group by dt,addr as groupTable; 最后，调用save语法写入： save append groupTable as streamJDBC.`mydb.patient` options mode=\"update\" -- call procedure 调用存储过程 and `statement-0`=\"call test_proc(?,?,?)\" and duration=\"5\" and checkpointLocation=\"/streamingpro-test/kafka/patient/mysql/update\"; 存储过程是个技巧。其实还有更好的办法： select dt,addr,num, dt as dt1, addr as addr2 from groupTable as outputTable; save append outputTable as streamJDBC.`mydb.patient` options mode=\"update\" and `statement-0`=\"insert into patientUpdate(dt,addr,num) value(?,?,?) ON DUPLICATE KEY UPDATE dt=?,addr=?,num=?;\" and duration=\"5\" and checkpointLocation=\"/streamingpro-test/kafka/patient/mysql/update\"; 通过ON DUPLICATE KEY UPDATE实现相关功能。 指的注意的是，statement-0 里是原生的sql语句，通过?占位符来设置参数， 我们会把待写入的表字段按顺序配置给对应的SQL语句。            该文件修订时间： 2021-01-12 10:27:47 "},"stream/web_console.html":{"url":"stream/web_console.html","title":"流结果输出到WebConsole","keywords":"","body":"流结果输出到WebConsole 流的一个很大缺点是输出不直观，不像交互式，跑完就能在Console看到结果. MLSQL提供了webConsole,方便大家 调试。 我们新建一个批脚本，方便往Kafka写数据： set abc=''' { \"x\": 100, \"y\": 200, \"z\": 200 ,\"dataType\":\"A group\"} { \"x\": 120, \"y\": 100, \"z\": 260 ,\"dataType\":\"B group\"} { \"x\": 100, \"y\": 100, \"z\": 260 ,\"dataType\":\"B group\"} { \"x\": 120, \"y\": 100, \"z\": 260 ,\"dataType\":\"B group\"} { \"x\": 100, \"y\": 100, \"z\": 260 ,\"dataType\":\"B group\"} { \"x\": 120, \"y\": 100, \"z\": 260 ,\"dataType\":\"B group\"} { \"x\": 120, \"y\": 100, \"z\": 260 ,\"dataType\":\"B group\"} { \"x\": 100, \"y\": 100, \"z\": 260 ,\"dataType\":\"B group\"} { \"x\": 120, \"y\": 100, \"z\": 260 ,\"dataType\":\"B group\"} { \"x\": 120, \"y\": 100, \"z\": 260 ,\"dataType\":\"B group\"} { \"x\": 120, \"y\": 100, \"z\": 260 ,\"dataType\":\"B group\"} '''; load jsonStr.`abc` as table1; select to_json(struct(*)) as value from table1 as table2; save append table2 as kafka.`wow` where kafka.bootstrap.servers=\"127.0.0.1:9092\"; 接着写一个流脚本： -- the stream name, should be uniq. set streamName=\"kafkaStreamExample\"; !kafkaTool registerSchema 2 records from \"127.0.0.1:9092\" wow; -- convert table as stream source load kafka.`wow` options kafka.bootstrap.servers=\"127.0.0.1:9092\" and failOnDataLoss=\"false\" as newkafkatable1; -- aggregation select * from newkafkatable1 as table21; -- output the the result to console. save append table21 as webConsole.`` options mode=\"Append\" and idCols=\"x,y\" and dropDuplicate=\"true\" and duration=\"5\" and checkpointLocation=\"/tmp/s-cpl7\"; 打开两个标签页，启动流，时不时点下批，在console下端就能看到如下结果： 19/09/20 11:07:05 INFO MLSQLConsoleWriter: [owner] [allwefantasy@gmail.com] [groupId] [41613ea5-8db9-4bf9-a91e-c739387af2d3] kafkaValue | [, 1, 12, 2019-09... 19/09/20 11:07:05 INFO MLSQLConsoleWriter: [owner] [allwefantasy@gmail.com] [groupId] [41613ea5-8db9-4bf9-a91e-c739387af2d3] -RECORD 6-------------------------- 19/09/20 11:07:05 INFO MLSQLConsoleWriter: [owner] [allwefantasy@gmail.com] [groupId] [41613ea5-8db9-4bf9-a91e-c739387af2d3] dataType | B group 19/09/20 11:07:05 INFO MLSQLConsoleWriter: [owner] [allwefantasy@gmail.com] [groupId] [41613ea5-8db9-4bf9-a91e-c739387af2d3] x | 100 19/09/20 11:07:05 INFO MLSQLConsoleWriter: [owner] [allwefantasy@gmail.com] [groupId] [41613ea5-8db9-4bf9-a91e-c739387af2d3] y | 100 19/09/20 11:07:05 INFO MLSQLConsoleWriter: [owner] [allwefantasy@gmail.com] [groupId] [41613ea5-8db9-4bf9-a91e-c739387af2d3] z | 260 19/09/20 11:07:05 INFO MLSQLConsoleWriter: [owner] [allwefantasy@gmail.com] [groupId] [41613ea5-8db9-4bf9-a91e-c739387af2d3] kafkaValue | [, 1, 13, 2019-09... 19/09/20 11:07:05 INFO MLSQLConsoleWriter: [owner] [allwefantasy@gmail.com] [groupId] [41613ea5-8db9-4bf9-a91e-c739387af2d3] -RECORD 7-------------------------- 19/09/20 11:07:05 INFO MLSQLConsoleWriter: [owner] [allwefantasy@gmail.com] [groupId] [41613ea5-8db9-4bf9-a91e-c739387af2d3] dataType | B group 19/09/20 11:07:05 INFO MLSQLConsoleWriter: [owner] [allwefantasy@gmail.com] [groupId] [41613ea5-8db9-4bf9-a91e-c739387af2d3] x | 120 19/09/20 11:07:05 INFO MLSQLConsoleWriter: [owner] [allwefantasy@gmail.com] [groupId] [41613ea5-8db9-4bf9-a91e-c739387af2d3] y | 100 19/09/20 11:07:05 INFO MLSQLConsoleWriter: [owner] [allwefantasy@gmail.com] [groupId] [41613ea5-8db9-4bf9-a91e-c739387af2d3] z | 260 19/09/20 11:07:05 INFO MLSQLConsoleWriter: [owner] [allwefantasy@gmail.com] [groupId] [41613ea5-8db9-4bf9-a91e-c739387af2d3] kafkaValue | [, 1, 14, 2019-09...            该文件修订时间： 2021-01-12 10:27:47 "},"ml/":{"url":"ml/","title":"MLSQL机器学习编程","keywords":"","body":"MLSQL机器学习编程 用户可以使用MLSQL进行算法的特征处理，训练，预测。MLSQL内建了非常多的特征工具以及模型。与此同时，MLSQL因为内置了Python语言的支持，并且 完美的解决了大数据和AI数据的互通问题，用户可以非常愉快的使用MLSQL语法来完成自己的工作。            该文件修订时间： 2021-01-12 10:27:46 "},"feature/":{"url":"feature/","title":"特征工程","keywords":"","body":"特征工程组件 MLSQL提供了非常多的特征工程Estimator/Transformer（本章节我们都会简称ET）。 做机器学习一大痛点就是训练阶段的特征工程代码无法复用在API 预测阶段。 MLSQL的 ET很好的解决了这个问题。 MLSQL内置这些ET有如下特点： 训练阶段可用，保证吞吐量 预测阶段使用，保证性能，一般毫秒级 在下面章节中，我们会看到具体如何使用ET来解决这个痛点。 系统需求 启动MLSQL时，请使用--jars带上 ansj_seg-5.1.6.jar,nlp-lang-1.7.8.jar. 因为在很多示例中，我们需要用到分词相关的功能。            该文件修订时间： 2021-01-12 10:27:46 "},"feature/nlp.html":{"url":"feature/nlp.html","title":"文本向量化","keywords":"","body":"文本向量化 Estimator/Transformer本章节我们都会简称ET NLP领域最核心的就是把文本转化为向量，这也是整个特征工程里的重头戏。在MLSQL中，我们提供了两种ET常见的把Raw文本转化 为向量。他们分别是： TF/IDF Word2Vec            该文件修订时间： 2021-01-12 10:27:46 "},"feature/tfidf.html":{"url":"feature/tfidf.html","title":"TFIDF","keywords":"","body":"TFIDF 假设我们有如下的数据 set rawText=''' {\"content\":\"MLSQL是一个好的语言\",\"label\":0.0}, {\"content\":\"Spark是一个好的语言\",\"label\":1.0} {\"content\":\"MLSQL语言\",\"label\":0.0} {\"content\":\"MLSQL是一个好的语言\",\"label\":0.0} {\"content\":\"MLSQL是一个好的语言\",\"label\":1.0} {\"content\":\"MLSQL是一个好的语言\",\"label\":0.0} {\"content\":\"MLSQL是一个好的语言\",\"label\":0.0} {\"content\":\"MLSQL是一个好的语言\",\"label\":1.0} {\"content\":\"Spark好的语言\",\"label\":0.0} {\"content\":\"MLSQL是一个好的语言\",\"label\":0.0} '''; 我们看到，我们把文本分成两类，一个是1,一类是0。现在我们需要判断一个新的文本进来，他的分类是什么。 我们打算通过TFIDF的方式把文本content转化为一个向量。我们使用ET TfIdfInPlace来完成这个工作。 load jsonStr.`rawText` as orginal_text_corpus; train orginal_text_corpus as TfIdfInPlace.`/tmp/tfidfinplace` where inputCol=\"content\" and ignoreNature=\"true\" and nGrams=\"2,3\" as tfTable; select * from tfTable as output; 经过上面的处理，我们看看结果: 我们会发现content已经被数字化了，里面有type,size,indices,values等四个字段。这是向量稀疏化表示的一种方式。 在上面的示例中，我们只使用了三个参数，完整的参数列表在这里： Parameter Default Comments inputCol 需要处理的字段名称 dicPaths None 用户自定义词典，注意在Yarn上是HDFS路径 stopWordPath 用户自定义停用词词典，注意在Yarn上是HDFS路径 priorityDicPath 优先级高的词典，注意在Yarn上是HDFS路径 priority 对于优先级高的词典里，我们应该提权多少倍 nGrams None 词组合，比如设置为2,3则表示分别以两个词，三个词进行组合得到新词 split 可选，如果你不想用分词，可以自己定义切分字符 ignoreNature 分词后是否在每个词上都带上词性。请设置为true - [danger] 此处有坑警告 请将ignoreNature设置为true 如何在预测时使用 任何ET都具备在\"训练时学习到经验\"转化为一个函数，从而可以使得你把这个功能部署到流式计算，API服务里去。同时，部分ET还有batch predict功能， 可以让在批处理做更高效的预测。 对于ET TfIdfInPlace 而言，我们要把它转化为一个函数非常容易： register TfIdfInPlace.`/tmp/tfidfinplace` as tfidf_convert; 通过上面的命令，TfIdfInPlace就会把训练阶段学习到的东西应用起来，现在，任意给定一段文本，都可以使用tfidf_convert函数将 内容转化为向量了。 select tfidf_convert(\"MLSQL是一个好的语言\") as features as output;            该文件修订时间： 2021-01-12 10:27:46 "},"feature/word2vec.html":{"url":"feature/word2vec.html","title":"Word2Vec","keywords":"","body":"Word2Vec 假设我们有如下的数据 set rawText=''' {\"content\":\"MLSQL是一个好的语言\",\"label\":0.0}, {\"content\":\"Spark是一个好的语言\",\"label\":1.0} {\"content\":\"MLSQL语言\",\"label\":0.0} {\"content\":\"MLSQL是一个好的语言\",\"label\":0.0} {\"content\":\"MLSQL是一个好的语言\",\"label\":1.0} {\"content\":\"MLSQL是一个好的语言\",\"label\":0.0} {\"content\":\"MLSQL是一个好的语言\",\"label\":0.0} {\"content\":\"MLSQL是一个好的语言\",\"label\":1.0} {\"content\":\"Spark好的语言\",\"label\":0.0} {\"content\":\"MLSQL是一个好的语言\",\"label\":0.0} '''; 我们看到，我们把文本分成两类，一个是1,一类是0。现在我们需要判断一个新的文本进来，他的分类是什么。 我们打算通过Word2Vec的方式把文本content转化为一个向量。我们使用ET Word2VecInPlace来完成这个工作。 load jsonStr.`rawText` as orginal_text_corpus; train orginal_text_corpus as Word2VecInPlace.`/tmp/word2vec` where inputCol=\"content\" and ignoreNature=\"true\" and resultFeature=\"merge\"; load parquet.`/tmp/word2vec/data` as lwys_corpus_with_featurize; 我们会发现content已经被数字化了，里面有type,size,indices,values等四个字段。这是向量稀疏化表示的一种方式。 在上面的示例中，我们只使用了三个参数，完整的参数列表在这里： Parameter Default Comments inputCol None resultFeature None flat:把多个向量拼接成一个向量;merge: 把多个向量按位相加得到一个向量；index: 只输出词序列，每次词用一个数字表示 dicPaths None 用户定义的词典路径，按逗号分给 wordvecPaths None 如果你已经训练了一个word2vec模型，你可以通过该参数指定其路劲。该文本格式为： word + 空格 + 按逗号分隔的数字，类似SVM格式 vectorSize None 词向量长度 length None 文本最大长度 stopWordPath 停用词词典 split 如果不分词，使用什么进行分隔 minCount 词最少出现的次数，低于该次数不会为词生成一个向量 - [danger] 此处有坑警告 请将ignoreNature设置为true 如何在预测时使用 任何ET都具备在\"训练时学习到经验\"转化为一个函数，从而可以使得你把这个功能部署到流式计算，API服务里去。同时，部分ET还有batch predict功能， 可以让在批处理做更高效的预测。 对于ET TfIdfInPlace 而言，我们要把它转化为一个函数非常容易： register TfIdfInPlace.`/tmp/tfidfinplace` as word2vec_convert; 通过上面的命令，TfIdfInPlace就会把训练阶段学习到的东西应用起来，现在，任意给定一段文本，都可以使用tfidf_convert函数将 内容转化为向量了。 select word2vec_convert(\"MLSQL是一个好的语言\") as features as output;            该文件修订时间： 2021-01-12 10:27:46 "},"feature/scale.html":{"url":"feature/scale.html","title":"特征平滑","keywords":"","body":"特征平滑 ScalerInPlace 支持min-max,log2,logn 去平滑数据。不同于后续章节会介绍的NormalizeInPlace，该ET针对的是列。 我们先构造一些测试数据： -- create test data set jsonStr=''' {\"a\":1, \"b\":100, \"label\":0.0}, {\"a\":100, \"b\":100, \"label\":1.0} {\"a\":1000, \"b\":100, \"label\":0.0} {\"a\":10, \"b\":100, \"label\":0.0} {\"a\":1, \"b\":100, \"label\":1.0} '''; load jsonStr.`jsonStr` as data; 接着我们对第一列数据a,b两列数据都进行平滑。 train data as ScalerInPlace.`/tmp/scaler` where inputCols=\"a,b\" and scaleMethod=\"min-max\" and removeOutlierValue=\"false\" ; load parquet.`/tmp/scaler/data` as featurize_table; 结果如下： a b label 0 0.5 0 0.0990990990990991 0.5 1 1 0.5 0 0.009009009009009009 0.5 0 0 0.5 1 removeOutlierValue设置为true，会自动用中位数填充异常值。 如何在预测时使用 任何ET都具备在\"训练时学习到经验\"转化为一个函数，从而可以使得你把这个功能部署到流式计算，API服务里去。同时，部分ET还有batch predict功能， 可以让在批处理做更高效的预测。 对于ET ScalerInPlace 而言，我们要把它转化为一个函数非常容易： register ScalerInPlace.`/tmp/scaler` as scale_convert; 通过上面的命令，ScalerInPlace就会把训练阶段学习到的东西应用起来，现在，任意给定两个数字，都可以使用scale_convert函数将 内容转化为向量了。 select scale_convert(array(7,8)) as features as output; 输出结果为： features [0.006006006006006006,0.5]            该文件修订时间： 2021-01-12 10:27:46 "},"feature/normalize.html":{"url":"feature/normalize.html","title":"归一化","keywords":"","body":"特征归一化 特征归一化，在MLSQL对应的ET为NormalizeInPlace。本质上，该方式是为了统一量纲，让一个向量里的元素变得可以比较。 这对于比如KMeans,nearest neighbors methods, RBF kernels, 以及任何依赖于距离的算法，都是必要的。 我们先构造一些测试数据： -- create test data set jsonStr=''' {\"a\":1, \"b\":100, \"label\":0.0}, {\"a\":100, \"b\":100, \"label\":1.0} {\"a\":1000, \"b\":100, \"label\":0.0} {\"a\":10, \"b\":100, \"label\":0.0} {\"a\":1, \"b\":100, \"label\":1.0} '''; load jsonStr.`jsonStr` as data; 接着我们对第一列数据a,b两列数据按照行的方式进行归一化。 train data as NormalizeInPlace.`/tmp/model` where inputCols=\"a,b\" and scaleMethod=\"standard\" and removeOutlierValue=\"false\" ; load parquet.`/tmp/model/data` as output; 结果如下： a b label -0.5069956180959223 0 0 -0.2802902604107538 0 1 1.7806675367271416 0 0 -0.48638604012454334 0 0 -0.5069956180959223 0 1 removeOutlierValue设置为true，会自动用中位数填充异常值。 如果inputCols只有一列，那么该列可以为double数组 如何在预测时使用 任何ET都具备在\"训练时学习到经验\"转化为一个函数，从而可以使得你把这个功能部署到流式计算，API服务里去。同时，部分ET还有batch predict功能， 可以让在批处理做更高效的预测。 对于ET NormalizeInPlace 而言，我们要把它转化为一个函数非常容易： register NormalizeInPlace.`/tmp/model` as convert; 通过上面的命令，NormalizeInPlace就会把训练阶段学习到的东西应用起来，现在，任意给定两个数字，都可以使用convert函数将 内容转化为向量了。 select convert(array(7,8)) as features as output; 输出结果为： features [-0.4932558994483363,0]            该文件修订时间： 2021-01-12 10:27:46 "},"feature/confusion_matrix.html":{"url":"feature/confusion_matrix.html","title":"混淆矩阵","keywords":"","body":"混淆矩阵 混淆矩阵在分类算法里用处很多，可以让你直观看到数据的错误分布情况。他主要是将每个分类的实际值和预测值形成一个矩阵，这样 我们就可以很清楚的知道错误情况了。 假设我们有动物分类，有两列，一列是实际值，一列是预测值，内容如下： set rawData=''' {\"label\":\"cat\",\"predict\":\"rabbit\"} {\"label\":\"cat\",\"predict\":\"dog\"} {\"label\":\"cat\",\"predict\":\"cat\"} {\"label\":\"dog\",\"predict\":\"dog\"} {\"label\":\"cat\",\"predict\":\"dog\"} '''; load jsonStr.`rawData` as data; 接着我们使用混淆矩阵来查看最后的预测结果分布： train data as ConfusionMatrix.`/tmp/model` where actualCol=\"label\" and predictCol=\"predict\"; load parquet.`/tmp/model/data` as output; 结果如下： act\\prt cat dog rabbit cat 1 2 1 dog 0 1 0 rabbit 0 0 0 另外我们也可以看到一些统计值： load parquet.`/tmp/model/detail` as output; 结果如下： lable name value desc cat TP 1 True positive [eqv with hit] cat TN 1 True negative [eqv with correct rejection] cat FP 0 False positive [eqv with false alarm, Type I error] cat FN 3 False negative [eqv with miss, Type II error] ......            该文件修订时间： 2021-01-12 10:27:46 "},"feature/some_extract.html":{"url":"feature/some_extract.html","title":"QQ/电话/邮件抽取","keywords":"","body":"QQ/电话/邮件抽取 FeatureExtractInPlace 是个比较有趣的ET,他可以帮你对文本里的类似QQ,电话抽取出来，在反垃圾方面比较有用。 set rawData=''' {\"content\":\"请联系 13634282910\",\"predict\":\"rabbit\"} {\"content\":\"扣扣 527153688@qq.com\",\"predict\":\"dog\"} {\"content\":\" dddd img.dxycdn.com ffff 527153688@qq.com\",\"predict\":\"cat\"} '''; load jsonStr.`rawData` as data; 接下来我们算算看： train data as FeatureExtractInPlace.`/tmp/model` where inputCol=\"content\"; load parquet.`/tmp/model/data` as output; 下边是结果： content predict phone noEmoAndSpec email qqwechat url pic cleanedDoc blank chinese english number punctuation uninvisible mostchar length 请联系 13634282910 rabbit 请联系 13634282910 0 0 请联系 13634282910 6 20 0 73 0 0 2 15 扣扣 527153688@qq.com dog 扣扣 527153688@qq.com 0 0 扣扣 33 66 0 0 0 0 2 3 dddd img.dxycdn.com ffff 527153688@qq.com cat dddd img.dxycdn.com ffff 527153688@qq.com 0 1 dddd ffff 27 0 72 0 0 内容很丰富。 如何在预测时使用 任何ET都具备在\"训练时学习到经验\"转化为一个函数，从而可以使得你把这个功能部署到流式计算，API服务里去。同时，部分ET还有batch predict功能， 可以让在批处理做更高效的预测。 对于ET FeatureExtractInPlace 而言，我们要把它转化为一个函数非常容易： register FeatureExtractInPlace.`/tmp/model` as convert; 该ET比较特殊，会隐式的给你生成非常多的函数： convert_phone convert_email convert_qqwechat convert_url the number of url convert_pic the number of pic convert_blank blank percentage convert_chinese chinese percentage, convert_english english percentage, convert_number number percentage, convert_punctuation punctuation percentage, convert_mostchar mostchar percentage, convert_length text length 你可以根据需求使用。比如： select convert_qqwechat(\"扣扣 527153688@qq.com \") as features as output; 输出结果为： features true            该文件修订时间： 2021-01-12 10:27:46 "},"feature/discretizer/":{"url":"feature/discretizer/","title":"离散化","keywords":"","body":"离散化 MLSQL提供了两种离散化策略： Bucketizer Quantile            该文件修订时间： 2021-01-12 10:27:46 "},"feature/discretizer/bucketizer.html":{"url":"feature/discretizer/bucketizer.html","title":"Bucketizer","keywords":"","body":"Bucketizer Bucketizer可以让你手动指定如何对连续数据进行切分，从而形成一个类似分类的数据。 假设我们有如下数据： -- create test data set jsonStr=''' {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0}, {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.4,2.9,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.7,3.2,1.3,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} '''; load jsonStr.`jsonStr` as data; select features[0] as a ,features[1] as b from data as data1; 现在我们得到了a,b两个字段，我们对他们分别进行切分，转化为离散值： train data1 as Discretizer.`/tmp/model` where method=\"bucketizer\" and `fitParam.0.inputCol`=\"a\" and `fitParam.0.splitArray`=\"-inf,0.0,1.0,inf\" and `fitParam.1.inputCol`=\"b\" and `fitParam.1.splitArray`=\"-inf,0.0,1.0,inf\"; 这里，我们使用fitParam.0 表是第一组切分规则，fitParam.1表示第二组切分规则。fitParam.0 负责对a切分，fitParam.1负责对b切分。 该ET目前比较特殊查看切分结果需要使用register语法注册函数。 参数描述： parameter default comments method bucketizer support: bucketizer, quantile fitParam.${index}.inputCols None double类型字段 fitParam.${index}.splitArray None bucket array，-inf ~ inf ，size should > 3，[x, y) 如何在预测时使用 任何ET都具备在\"训练时学习到经验\"转化为一个函数，从而可以使得你把这个功能部署到流式计算，API服务里去。同时，部分ET还有batch predict功能， 可以让在批处理做更高效的预测。 对于ET NormalizeInPlace 而言，我们要把它转化为一个函数非常容易： register Discretizer.`/tmp/model` as convert; 通过上面的命令，Discretizer 就会把训练阶段学习到的东西应用起来，现在，可以使用convert函数了。 select convert(array(7,8)) as features as output; 输出结果为： features [2,2] 都被分在2分区里。            该文件修订时间： 2021-01-12 10:27:46 "},"feature/discretizer/quantile.html":{"url":"feature/discretizer/quantile.html","title":"Quantile","keywords":"","body":"Quantile Quantile使用更加简单，你只要指指定bucket数目即可。 假设我们有如下数据： -- create test data set jsonStr=''' {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0}, {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.4,2.9,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.7,3.2,1.3,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} '''; load jsonStr.`jsonStr` as data; select features[0] as a ,features[1] as b from data as data1; 现在我们得到了a,b两个字段，我们对他们分别进行切分，转化为离散值： train data1 as Discretizer.`/tmp/model` where method=\"quantile\" and `fitParam.0.inputCol`=\"a\" and `fitParam.0.outputCol`=\"a_v\" and `fitParam.0.numBuckets`=\"3\" and `fitParam.1.inputCol`=\"b\" and `fitParam.1.outputCol`=\"b_v\" and `fitParam.1.numBuckets`=\"3\"; 这里，我们使用fitParam.0 表是第一组切分规则，fitParam.1表示第二组切分规则。fitParam.0 负责对a切分，fitParam.1负责对b切分。 该ET目前比较特殊查看切分结果需要使用register语法注册函数。 需要注意的是，spark 2.4.x 要求outputCol必须设置。所以我们设置下。 参数描述： parameter default comments method bucketizer support: bucketizer, quantile fitParam.${index}.inputCols None double类型字段 fitParam.${index}.splitArray None bucket array，-inf ~ inf ，size should > 3，[x, y) 如何在预测时使用 任何ET都具备在\"训练时学习到经验\"转化为一个函数，从而可以使得你把这个功能部署到流式计算，API服务里去。同时，部分ET还有batch predict功能， 可以让在批处理做更高效的预测。 对于ET NormalizeInPlace 而言，我们要把它转化为一个函数非常容易： register Discretizer.`/tmp/model` as convert; 通过上面的命令，Discretizer 就会把训练阶段学习到的东西应用起来，现在，可以使用convert函数了。 select convert(array(7,8)) as features as output; 输出结果为： features [1,1] 都被分在2分区里。            该文件修订时间： 2021-01-12 10:27:46 "},"feature/vecmap.html":{"url":"feature/vecmap.html","title":"Map转化为向量","keywords":"","body":"Map转化为向量 VecMapInPlace 可以把一个 Map[String,Double] 转化为一个向量。具体使用案例： 假设我们有如下数据： set jsonStr=''' {\"features\":{\"a\":1.6,\"b\":1.2},\"label\":0.0} {\"features\":{\"a\":1.5,\"b\":0.2},\"label\":0.0} {\"features\":{\"a\":1.6,\"b\":1.2},\"label\":0.0} {\"features\":{\"a\":1.6,\"b\":7.2},\"label\":0.0} '''; load jsonStr.`jsonStr` as data; register ScriptUDF.`` as convert_st_to_map where code=''' def apply(row:org.apache.spark.sql.Row) = { Map(\"a\"->row.getAs[Double](\"a\"),\"b\"->row.getAs[Double](\"b\")) } '''; select convert_st_to_map(features) as f from data as newdata; 这里我们使用了自定义UDF去将Row转化为Map，用户方便后续的示例。大家可以参看 创建UDF/UDAF 相关章节的内容。 接着我们就可以进行学习训练了： train newdata as VecMapInPlace.`/tmp/model` where inputCol=\"f\"; load VecMapInPlace.`/tmp/model/data` as output; 显示结果如下： f {\"type\":0,\"size\":2,\"indices\":[0,1],\"values\":[1.6,1.2]} {\"type\":0,\"size\":2,\"indices\":[0,1],\"values\":[1.5,0.2]} {\"type\":0,\"size\":2,\"indices\":[0,1],\"values\":[1.6,1.2]} {\"type\":0,\"size\":2,\"indices\":[0,1],\"values\":[1.6,7.2]} 可以看到已经转化为一个二维向量了。 如何在预测时使用 任何ET都具备在\"训练时学习到经验\"转化为一个函数，从而可以使得你把这个功能部署到流式计算，API服务里去。同时，部分ET还有batch predict功能， 可以让在批处理做更高效的预测。 对于ET NormalizeInPlace 而言，我们要把它转化为一个函数非常容易： register VecMapInPlace.`/tmp/model` as convert; 通过上面的命令，VecMapInPlace 就会把训练阶段学习到的东西应用起来，现在，可以使用convert函数了。 select convert(map(\"a\",1,\"b\",0)) as features as output; 输出结果为： features {\"type\":0,\"size\":2,\"indices\":[0,1],\"values\":[1,0]} 通常，我们需要对向量做平滑或者归一化，请参考相应章节。            该文件修订时间： 2021-01-12 10:27:46 "},"feature/rate_sample.html":{"url":"feature/rate_sample.html","title":"数据集切分","keywords":"","body":"数据集切分 在做算法时，我们需要经常对数据切分成train/test。但是如果有些分类数据特别少，可能就会都跑到一个集合里去了。 RateSample 支持对每个分类的数据按比例切分。 -- create test data set jsonStr=''' {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0}, {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.4,2.9,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.7,3.2,1.3,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} '''; load jsonStr.`jsonStr` as data; 现在我们使用RateSample进行切分： train data as RateSampler.`/tmp/model` where labelCol=\"label\" and sampleRate=\"0.7,0.3\"; load parquet.`/tmp/model` as output ; 其中labelCol指定按什么字段进行切分，sampleRate指定切分比例。 结果如下： features label __split__ [5.1,3.5,1.4,0.2] 1 0 [5.1,3.5,1.4,0.2] 1 0 [4.7,3.2,1.3,0.2] 1 1 数据集多出了一个字段 split, 0 表示0.7那个集合（训练集）， 1表示0.3那个集合（测试集）。 可以这么使用 select * from output where __split__=1 as validateTable; select * from output where __split__=0 as trainingTable; 默认RateSampler采用估算算法，如果数据集较小，可以通过参数isSplitWithSubLabel=\"true\" 获得非常精确的划分。 如何在预测时使用 该模块无法在预测中使用。            该文件修订时间： 2021-01-12 10:27:46 "},"python/":{"url":"python/","title":"MLSQL Python支持","keywords":"","body":"MLSQL Python支持 MLSQL 以一种非常创新的方式支持Python,从而完成ETL以及算法训练，预测。 Python代码既可以以字符串形式内嵌到MLSQL中，也可以单独成脚本Include进MLSQL中。 Python对数据处理的方式也是表进表出。这意味着你可以使用Python对数据进行ETL也可以做算法训练，预测等等，没有任何约束。 无缝整合Ray,用户可以使用Ray API 进行分布式计算，亦或是机器学习训练预测。 典型示例 下面代码分别对应控制台的两个脚本: 大家可以看看MLSQL是对他们如何进行有机整合的。 foreach.mlsql -- 加载python脚本 !pyInclude ray_data.foreach.py named wow1; -- 加载并且处理数据 load delta.`public.consumer_complains` as cc; select * from cc union all select * from cc union all select * from cc union all select * from cc as newcc; -- ray 配置 !python env \"PYTHON_ENV=source activate dev\"; !python conf \"schema=st(field(ProductName,string),field(SubProduct,string))\"; !python conf \"runIn=driver\"; !python conf \"dataMode=data\"; -- ray处理数据代码 !ray on newcc py.wow1 named mlsql_temp_table2; --结果 save overwrite mlsql_temp_table2 as parquet.`/tmp/mlsql_temp_table2`; load parquet.`/tmp/mlsql_temp_table2` as mlsql_temp_table3; foreach.py import ray from pyjava.api.mlsql import RayContext import numpy as np; ray_context = RayContext.connect(globals(),\"192.168.209.29:49417\") def echo(row): row1 = {} row1[\"ProductName\"]=\"jackm\" row1[\"SubProduct\"] = row[\"SubProduct\"] return row1 ray_context.foreach(echo)            该文件修订时间： 2021-01-12 10:27:47 "},"python/env.html":{"url":"python/env.html","title":"环境依赖","keywords":"","body":"环境依赖 在运行MLSQL Driver(Executor节点可选)的节点上，需要有Python环境。如果你使用yarn,推荐使用Conda管理Python环境。如果你使用K8s,可以直接使用镜像管理。 Python环境最好是3.6版本，并且请安装如下依赖： pip install Cython pip install pyarrow==0.10.0 pip install ray==0.8.0 pip install aiohttp psutil setproctitle grpcio pandas xlsxwriter pip install watchdog requests click uuid sfcli pyjava 如果你要使用Ray做计算，请确保Driver节点（Executor节点可选）按如下方式启动ray worker: ray start --address= --num-cpus=0 --num-cpus=0 其中address地址为Ray集群地址，类似123.45.67.89:6379这样。同时我们将cpu,gpus等资源设置为0. 启动一个没有资源的ray worker ,是因为我们需要通过ray worker提交任务。 如何显示实时日志 MLSQL Console支持实时日志显示， 为了支持上面这个功能，用户需要修改SPARK_HOME/conf下的log4j.properties文件。 复制黏贴如下范例： MLSQL log4j.properties            该文件修订时间： 2021-01-12 10:27:47 "},"python/etl.html":{"url":"python/etl.html","title":"数据处理","keywords":"","body":"数据处理 在MLSQL中，你可以先用SQL处理数据，然后接着再对数据使用Python处理。使用者可以非常的灵活。 在Console中，MLSQL文件和Python文件可以分开写： 当然，也可以直接把Python代码内嵌到MLSQL脚本里。 SQL加载和预处理数据 下面的代码我们加载了数据湖里的测试数据。 load delta.`public.example_data` as cc; select * from cc -- union all select * from cc -- union all select * from cc -- union all select * from cc as newcc; 得到表newcc。 配置Python以及使用Python代码处理数据 如果你使用conda,那么可以用下面的指令指定需要使用的环境（如果没有使用，则不需要这句命令）： -- ray 配置 !python env \"PYTHON_ENV=source /usr/local/miniconda/bin/activate dev\"; 接着指定Python代码是在Driver还是在Executor端跑，推荐Driver跑。 !python conf \"runIn=driver\"; 现在指定python的数据返回格式： !python conf \"schema=st(field(ProductName,string),field(SubProduct,string))\"; 如果用户使用Ray做分布式数据处理，请将dataMode设置为data.否则设置为model. 在现在这个例子里，我们设置为model，因为我们只是简单的在python中获取所有数据，然后处理返回。 !python conf \"dataMode=model\"; 在MLSQL中加载python脚本： !pyInclude python_example.foreach.py named wow1; 对应的目录结构如下： 加载后就可以使用Python对表进行处理了： !ray on newcc py.wow1 named mlsql_temp_table2; 最后保存结果并且加载显示： --结果 save overwrite mlsql_temp_table2 as parquet.`/tmp/mlsql_temp_table2`; load parquet.`/tmp/mlsql_temp_table2` as mlsql_temp_table3; Python脚本说明 前面我们用到的 python_example/foreach.py 的内容如下： ## 引入必要的包 import ray from pyjava.api.mlsql import RayContext import numpy as np; ## 获取ray_context,如果需要使用Ray，那么第二个参数填写Ray地址 ## 否则设置为None就好。 ray_context = RayContext.connect(globals(),None) # 通过ray_context.data_servers() 获取所有数据源，如果开启了Ray， 那么就可以在 # 分布式获取这些数据进行处理。 datas = RayContext.collect_from(ray_context.data_servers()) ## 对数据进行处理 def echo(row): row1 = {} row1[\"ProductName\"]=\"jackm\" row1[\"SubProduct\"] = row[\"SubProduct\"] return row1 buffer = (echo(row) for row in datas) ## 输出结果. 其中context 是内置变量，无需申明就可以使用。 context.build_result(buffer) 如何使用Ray分布式处理 前面的例子我们其实没有使用到Ray，这里我们简单介绍下用户如何使用pyjava高阶API自动使用Ray完成分布式处理： import ray from pyjava.api.mlsql import RayContext import numpy as np; ## 做分布式处理，第二个Ray地址参数是必须的 ray_context = RayContext.connect(globals(),\"192.168.209.29:49417\") def echo(row): row1 = {} row1[\"ProductName\"]=\"jackm\" row1[\"SubProduct\"] = row[\"SubProduct\"] return row1 ray_context.foreach(echo) 除了foreach,还有map_iter， map_iter 里的函数接受一个generator,返回也需要是一个generator. 在PyJava API简介,我们会更详细的介绍各种API，方便用户获取和处理数据。            该文件修订时间： 2021-01-12 10:27:47 "},"python/train.html":{"url":"python/train.html","title":"模型训练","keywords":"","body":"模型训练 下面是一个模型训练的python代码： import ray from pyjava import rayfix from pyjava.api.mlsql import RayContext ## 如果需要找Ray节点做训练，则第二个参数必须填写 ray_context = RayContext.connect(globals(),\"192.168.31.80:17019\") data_servers = ray_context.data_servers() # 在某个Ray节点上获取所有数据并且进行训练,上传模型 @ray.remote @rayfix.last def train(servers): import time time.sleep(1) data = RayContext.collect_from(servers) ## 这里对数据进行模型训练 model_train(data) ## 上传模型到模型仓库之类的远程地址 path = upload_model() ## 返回地址 return path path = ray.get(train.remote(data_servers)) context.build_result([{\"path\":path}]) 理论上用户可以通过Ray API 实现分布式训练。之后可以在MLSQL脚本里引用该脚本进行数据处理。 load delta.`public.example_data` as cc; !ray on cc py.wow1 named mlsql_temp_table2;            该文件修订时间： 2021-01-12 10:27:47 "},"python/pyjava.html":{"url":"python/pyjava.html","title":"PyJava API简介","keywords":"","body":"PyJava API简介 前面所有的Python文件里都引入了pyjava包， from pyjava import rayfix from pyjava.api.mlsql import RayContext 该包实现了Python和系统的交互。 系统内置了一个context,类型为PythonContext. 大部分情况下，用户需要使用RayContext 初始化RayContext 通过如下代码初始化RayContext： ray_context = RayContext.connect(globals(),\"192.168.31.80:17019\") 其中第二个参数是可选的，如果不连接Ray集群，则设置为None即可。 获取数据分片 通过下面代码可以获取多个数据地址： data_servers = ray_context.data_servers() data_servers是字符串数组，每个元素是一个 ip:port的形态. 从这些数据地址，用户可以获取下列代码中 load delta.`public.example_data` as cc; !ray on cc py.wow1 named mlsql_temp_table2; cc 表的数据。 有了地址之后，MLSQL提供了多种方式去拉取真实的数据。 最简单的模式： RayContext.collect_from(ray_context.data_servers()) 该代码会遍历每个地址，然后返回一个generator. 如果已经连接了Ray,那么可以直接使用高阶API ray_context.foreach def echo(row): row1 = {} row1[\"ProductName\"]=\"jackm\" row1[\"SubProduct\"] = row[\"SubProduct\"] return row1 buffer = ray_context.foreach(echo) foreach接受一个回调函数，函数的入参是一条记录。用户无需显示的申明如何获取数据，只要实现回调函数即可。我们也可以获得一批数据，可以使用ray_context.map_iter。 系统会自动调度多个任务到Ray上并行运行。 map_iter会根据表的分片大小启动相应个数的task,如果你希望通过map_iter拿到所有的数据，而非部分数据，可以先对表做重新分区： -- simpleDataTemp转化成只有一个分片数据的新表simpleData; !tableRepartition _ -i simpleDataTemp -num 1 -o simpleData; 一个使用map_iter的例子： !pyInclude ray_data.foreach.py named wow1; -- load delta.`public.consumer_complains` as cc; -- select * from cc -- union all select * from cc -- union all select * from cc -- union all select * from cc -- as newcc; -- save overwrite newcc as parquet.`/tmp/newcc` where fileNum=\"1\"; load parquet.`/tmp/newcc` as newcc_temp; !tableRepartition _ -i newcc_temp -num 1 -o newcc; -- select count(*) from newcc as output; -- ray 配置 !python env \"PYTHON_ENV=source activate dev\"; !python conf \"schema=st(field(ProductName,string),field(SubProduct,string))\"; !python conf \"runIn=driver\"; !python conf \"dataMode=data\"; -- ray处理数据代码 !ray on newcc ''' import ray from pyjava.api.mlsql import RayContext import numpy as np; import time ray_context = RayContext.connect(globals(),\"xxxx:xxxx\") def echo(rows): count = 0 for row in rows: row1 = {} row1[\"ProductName\"]=\"jackm\" row1[\"SubProduct\"] = row[\"SubProduct\"] count = count + 1 if count%1000 == 0: print(\"=====> \" + str(time.time()) + \" ====>\" + str(count)) yield row1 ray_context.map_iter(echo) ''' named mlsql_temp_table2; --结果 save overwrite mlsql_temp_table2 as parquet.`/tmp/mlsql_temp_table2`; load parquet.`/tmp/mlsql_temp_table2` as mlsql_temp_table3; 如果用户希望自己掌控任务的调度，那么可以使用Ray 的remote方法 data_servers = ray_context.data_servers() # 对每个分区的数据进行处理 @ray.remote @rayfix.last def process(server): import time time.sleep(1) data = RayContext.fetch_once_as_rows(server) items = [{\"content\":item['content']} for item in data] return items # 启动N个任务进行处理 items = [ray.get(process(server)) for server in servers] 在这里，用户可以使用RayContext.fetch_once_as_rows(server) 获取一个特定分区的数据。 获取数据并且存储到python运行的主机 通过 ray_context.collect_as_file 你就可以获取一个文件引用，然后可以反复迭代数据。 data = ray_context.collect_as_file(2) for item in data: print(item) 其中里面的参数表示，你需要迭代几次的数据量。算法需要对数据进行多次训练，该参数就是为了满足这个需求。            该文件修订时间： 2021-01-12 10:27:47 "},"python/read_excel.html":{"url":"python/read_excel.html","title":"结合Python读取Excel","keywords":"","body":"结合Python读取Excel MLSQL 2.0.1-SNAPSHOT/2.0.1 以及以上版本可用 MLSQL 提供了 excel插件，可以让用户读写Excel。更多详情参考插件 Python对Excel的支持也很不错，而MLSQL支持Python,所以自然能够在MLSQL操作和使用Excel数据。这样我们可以使用MLSQL将大规模的数据变小，然后保存成Excel后用户下载下来做进一步处理。 读取excel文件 load binaryFile.`/tmp/wow.xlsx` as excel_table; 我们也可以读取一个目录下所有的excel文件，只要把路径写到目录即可。 Ray配置 -- python处理完后的数据的schema !python conf \"schema=st(field(file,binary))\"; -- Python跑在driver端， !python conf \"runIn=driver\"; -- 不需要从Ray集群获取数据，需要将dataMode设置为model. !python conf \"dataMode=model\"; 使用Python解析Excel !ray on excel_table ''' import io import ray from pyjava.api.mlsql import RayContext import pandas as pd ray_context = RayContext.connect(globals(),None) excel_file_binary_list = [item for item in RayContext.collect_from(ray_context.data_servers())] df = pd.read_excel(io.BytesIO(excel_file_binary_list[0][\"content\"])) context.build_result([row for row in df.to_dict('records')]) ''' named excel_data; 简单解释下。 ray_context = RayContext.connect(globals(),None) 获取ray_context,第二个参数为None表示不适用Ray集群。 excel_file_binary_list = [item for item in RayContext.collect_from(ray_context.data_servers())] 获取所有文件内容。 df = pd.read_excel(io.BytesIO(excel_file_binary_list[0][\"content\"])) 我们这里只有一条记录，对应的二进制内容字段是content. context.build_result([row for row in df.to_dict('records')]) 将结果展开成行构建返回。 附录 下面是完整代码： load binaryFile.`/tmp/wow.xlsx` as excel_table; !pyInclude excel.read.py named read_excel; -- ray 配置 !python env \"PYTHON_ENV=source activate dev\"; !python conf \"schema=st(field(a,string),field(b,string))\"; !python conf \"runIn=driver\"; !python conf \"dataMode=model\"; !ray on excel_table ''' import io import ray from pyjava.api.mlsql import RayContext import pandas as pd ray_context = RayContext.connect(globals(),None) excel_file_binary_list = [item for item in RayContext.collect_from(ray_context.data_servers())] df = pd.read_excel(io.BytesIO(excel_file_binary_list[0][\"content\"])) context.build_result([row for row in df.to_dict('records')]) ''' named excel_data;            该文件修订时间： 2021-01-12 10:27:47 "},"python/write_excel.html":{"url":"python/write_excel.html","title":"结合Python保存Excel","keywords":"","body":"结合Python保存Excel MLSQL 2.0.1-SNAPSHOT/2.0.1 以及以上版本可用 MLSQL 提供了 excel插件，可以让用户读写Excel。更多详情参考插件 加载数据 首先加载数据 load delta.`public.simpleData` as simpleDataTemp; -- simpleDataTemp转化成只有一个分片数据的新表simpleData; !tableRepartition _ -i simpleDataTemp -num 1 -o simpleData; 如下： Ray配置 -- python处理完后的数据的schema !python conf \"schema=st(field(file,binary))\"; -- Python跑在driver端， !python conf \"runIn=driver\"; -- 不需要从Ray集群获取数据，需要将dataMode设置为model. !python conf \"dataMode=model\"; 使用Python代码处理数据，生成Excel !ray on simpleData ''' import io import ray from pyjava.api.mlsql import RayContext import pandas as pd ray_context = RayContext.connect(globals(),None) rows = [item for item in RayContext.collect_from(ray_context.data_servers())] df = pd.DataFrame(data=rows) output = io.BytesIO() writer = pd.ExcelWriter(output, engine='xlsxwriter') df.to_excel(writer) writer.save() xlsx_data = output.getvalue() context.build_result([{\"file\":xlsx_data}]) ''' named excel_data; 简单解释下。 ray_context = RayContext.connect(globals(),None) 获取ray_context,第二个参数为None表示不适用Ray集群。 rows = [item for item in RayContext.collect_from(ray_context.data_servers())] df = pd.DataFrame(data=rows) 获取表数据，并且将其转化为Pandas DataFrame. 剩下的代码是具体将数据转化为excel的代码，得到一个二进制数组。使用 context.build_result([{\"file\":xlsx_data}]) 输出到 excel_data 表中。 Pandas对Excel更多操作参考这里working_with_pandas 保存文件 最后，我们使用!saveFile 将excel保存起来. !saveFile _ -i excel_data -o /tmp/wow.xlsx; 第一个参数 _ 表示这里会使用命令行模式解析。 -i 表示输入的表， -o 表示输出路径。 附录 完整代码如下： load delta.`public.simpleData` as simpleData; -- !pyInclude \"excel.save.py\" named save_excel; -- ray 配置 !python env \"PYTHON_ENV=source activate dev\"; !python conf \"schema=st(field(file,binary))\"; !python conf \"runIn=driver\"; !python conf \"dataMode=model\"; !ray on simpleData ''' import io import ray from pyjava.api.mlsql import RayContext import pandas as pd ray_context = RayContext.connect(globals(),None) rows = [item for item in RayContext.collect_from(ray_context.data_servers())] df = pd.DataFrame(data=rows) output = io.BytesIO() writer = pd.ExcelWriter(output, engine='xlsxwriter') df.to_excel(writer) writer.save() xlsx_data = output.getvalue() context.build_result([{\"file\":xlsx_data}]) ''' named excel_data; !saveFile _ -i excel_data -o /tmp/wow.xlsx;            该文件修订时间： 2021-01-12 10:27:47 "},"python/resource.html":{"url":"python/resource.html","title":"K8s下的Python资源限制","keywords":"","body":"Python资源限制 在MLSQL中的Python代码会单独在Driver或者Executor节点上启动一个Python进程运行。默认总数量不超过节点的核数。 不过遗憾的是，如果不注意控制Python进程的资源占用，而MLSQL Engine又跑在K8s(Yarn上也是类似情况)上，很可能导致容器被杀，如果是driver节点被杀，那么会导致整个Engine失败。为了避免这种情况： 连接Ray集群并且将处理逻辑都放到Ray里去完成（官方推荐） 对Python代码所处的进程做资源限制 对于1，我们可以使用RayContext.foreach/RayContext.map_iter 做处理。这样可以保证数据的交互无需经过client。 对于2，用户可以在容器里设置环境变量 export PY_EXECUTOR_MEMORY=300 亦或是 !python conf \"py_executor_memory=300\"; 这里表示Python 内存不应该超过300M. 尽管如此，第二种方案还是有缺陷，如果你有8核，当多个用户并行使用时，最多会占用2.4G内存，很可能导致容器被杀死。 对于方式一，下面是一个典型的例子： load delta.`public.consumer_complains` as cc; select * from cc limit 100 -- union all select * from cc -- union all select * from cc -- union all select * from cc as newcc; -- ray 配置 !python env \"PYTHON_ENV=source activate dev\"; !python conf \"schema=st(field(ProductName,string),field(SubProduct,string))\"; !python conf \"runIn=driver\"; !python conf \"dataMode=data\"; -- ray处理数据代码 !ray on newcc ''' import ray from pyjava.api.mlsql import RayContext import numpy as np; ray_context = RayContext.connect(globals(),\"192.168.209.29:12879\") def echo(rows): for row in rows: row1 = {} row1[\"ProductName\"]=\"jackm\" row1[\"SubProduct\"] = row[\"SubProduct\"] yield row1 ray_context.map_iter(echo) ''' named mlsql_temp_table2; --结果 save overwrite mlsql_temp_table2 as parquet.`/tmp/mlsql_temp_table2`; load parquet.`/tmp/mlsql_temp_table2` as mlsql_temp_table3; ray_context.map_iter 会保证你的数据处理逻辑都运行在Ray集群上。对于上面的模式，仍然有个细节需要了解，就是map_iter里函数运行的次数，取决于数据表newcc的分片数。如果你希望在函数里拿到所有数据，那么可以将newcc 的分片数设置为1。 将newcc分片数设置为1的方式有两种：            该文件修订时间： 2021-01-12 10:27:47 "},"python/datamode.html":{"url":"python/datamode.html","title":"dataMode 详解","keywords":"","body":"dataMode 详解 在使用!ray命令时，dataMode是必须设置的。dataMode可选值为 data/model. 那么他们到底 是什么含义呢？ data 简单场景是，如果你使用了foreach/map_iter 等高阶函数，并且设置了Ray地址，则使用data模式。 如果从更深入的角度来看，就是你的数据会经过ray分布式处理并且不通过ray client端回流到MLSQL,则需要设置为data模式。 model 如果你只是用ray client,则使用model即可。            该文件修订时间： 2021-01-12 10:27:47 "},"python/py_paralell.html":{"url":"python/py_paralell.html","title":"Python并行度你所需要知道的","keywords":"","body":"Python并行度你所需要知道的 在MLSQL中，你可以直接使用Python对表进行处理。你可能已经注意到， 我们经常会在处理前，使用!tableRepartition 对数据进行重新切分。 比如： !tableRepartition _ -i simpleDataTemp -num 3 -o simpleData; !ray on simpleData py.wow1 named mlsql_temp_table2; 这里，在处理表 simpleDataTemp 之前，我们将其重新划分成3个分区的新表 simpleData。 之后才交给python代码进行处理。 那么做这件事的理由是什么呢？ 资源 以上面的分区数3为例： 如果你没有使用Ray,那么你的集群需要 3+1 个 核,才能让代码正常运行。这意味着如果你的分区过大， 而集群比较繁忙或者资源较少，很可能无法正确运行。 如果你使用了Ray,那么需要保证Ray至少有3个CPU资源。集群需要保证至少 3+3 个资源。否则也可能无法正常的运行任务。 效率 理论上分区越大速度越快。但是他会受到资源的制约。 自动化 尽管你可能设置了3个分区，但集群不保证一定按三个分区运行，除非资源足够。所谓足够是指，集群剩余可使用资源核数 > 3 * 2. 假设你的剩余资源为N，你设置的分区数的> N/2，那么系统会调整为 N/2-1, 如果你的设置分区数小于 N/2,则会按照你的设置运行。            该文件修订时间： 2021-01-12 10:27:47 "},"algs/":{"url":"algs/","title":"MLSQL内置算法","keywords":"","body":"MLSQL内置算法 前面章节我们介绍了对Python的集成，他在集群模式下，需要做一些特殊配置，并且需要每个节点都安装conda。但这个章节里的算法都是内置的， 并且没有特殊依赖就可以运行。我们会重点讲解四个，其他都是大同小异。 RandomForest XGBoost ALS LDA            该文件修订时间： 2021-01-12 10:27:46 "},"algs/kmeans.html":{"url":"algs/kmeans.html","title":"KMeans","keywords":"","body":"KMeans KMeans 属于聚类算法。 首先我们新增一些数据。 set jsonStr=''' {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0}, {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.4,2.9,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.7,3.2,1.3,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} '''; load jsonStr.`jsonStr` as data; select vec_dense(features) as features from data as data1; 聚类算法属于无监督算法，所以没有Label的概念。接着，我们可以训练了： train data1 as KMeans.`/tmp/alg/kmeans` where k=\"2\" and seed=\"1\"; 训练时可以设置的参考包括： k 聚类数，默认为2 (这样得到三个分类0,1,2) initMode 初始化模式： 'random' and 'k-means||' initSteps kmeans步数,必须大于0 批量预测 无 API预测 训练完成后，可以注册模型为函数，进行预测： register KMeans.`/tmp/alg/kmeans` as kcluster; select kcluster(features) as catagory from data1 as output;            该文件修订时间： 2021-01-12 10:27:46 "},"algs/naive_bayes.html":{"url":"algs/naive_bayes.html","title":"NaiveBayes","keywords":"","body":"NaiveBayes NaiveBayes 是一个分类算法。 -- create test data set jsonStr=''' {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0}, {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.4,2.9,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.7,3.2,1.3,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} '''; load jsonStr.`jsonStr` as data; select vec_dense(features) as features ,label as label from data as data1; -- use RandomForest train data1 as NaiveBayes.`/tmp/model` where -- once set true,every time you run this script, MLSQL will generate new directory for you model keepVersion=\"true\" -- specicy the test dataset which will be used to feed evaluator to generate some metrics e.g. F1, Accurate and evaluateTable=\"data1\" -- specify group 0 parameters and `fitParam.0.featuresCol`=\"features\" and `fitParam.0.labelCol`=\"label\" and `fitParam.0.smoothing`=\"0.5\" -- specify group 1 parameters and `fitParam.1.featuresCol`=\"features\" and `fitParam.1.labelCol`=\"label\" and `fitParam.1.smoothing`=\"0.2\" ; 最后输出结果如下： name value --------------------------------- modelPath /tmp/model/_model_10/model/1 algIndex 1 alg org.apache.spark.ml.classification.NaiveBayes metrics f1: 0.7625000000000001 weightedPrecision: 0.8444444444444446 weightedRecall: 0.7999999999999999 accuracy: 0.8 status success startTime 20180913 59:15:32:685 endTime 20180913 59:15:36:317 trainParams Map(smoothing -> 0.2,featuresCol -> features, labelCol -> label) --------------------------------- modelPath /tmp/model/_model_10/model/0 algIndex 0 alg org.apache.spark.ml.classification.NaiveBayes metrics f1:0.7625000000000001 weightedPrecision: 0.8444444444444446 weightedRecall: 0.7999999999999999 accuracy: 0.8 status success startTime 20180913 59:1536:318 endTime 20180913 59:1538:024 trainParams Map(smoothing -> 0.2, featuresCol -> features, labelCol -> label) 对于大部分内置算法而言，都支持如下几个特性： 可以通过keepVersion 来设置是否保留版本。 通过fitParam.数字序号 配置多组参数，设置evaluateTable后系统自动算出metrics. 批量预测 predict data1 as NaiveBayes.`/tmp/model`; 结果如下： features label rawPrediction probability prediction {\"type\":1,\"values\":[5.1,3.5,1.4,0.2]} 0 {\"type\":1,\"values\":[16.28594461094461,3.7140553890553893]} {\"type\":1,\"values\":[0.8142972305472306,0.18570276945276948]} 0 {\"type\":1,\"values\":[5.1,3.5,1.4,0.2]} 1 {\"type\":1,\"values\":[16.28594461094461,3.7140553890553893]} {\"type\":1,\"values\":[0.8142972305472306,0.18570276945276948]} 0 API预测 register NaiveBayes.`/tmp/model` as rf_predict; -- you can specify which module you want to use: register NaiveBayes.`/tmp/model` as rf_predict where algIndex=\"0\"; -- you can specify which metric the MLSQL should use to get best model register NaiveBayes.`/tmp/model` as rf_predict where autoSelectByMetric=\"f1\"; select rf_predict(features) as predict_label, label from data1 as output; algIndex可以选择我用哪组参数得到的算法。我们也可以让系统自动选择，前提是我们在训练时配置了evalateTable， 这个只要使用autoSelectByMetric即可。 最后，就可以像使用一个函数一样对一个feature进行预测了。            该文件修订时间： 2021-01-12 10:27:46 "},"algs/als.html":{"url":"algs/als.html","title":"ALS","keywords":"","body":"ALS ALS在协同算法里面很流行。通过它可以很方便的搭建一个推荐系统。 他的数据格式比较简单，需要userCol,itemCol,ratingCol三个。 set jsonStr=''' {\"a\":1,\"i\":2,\"rate\":1}, {\"a\":1,\"i\":3,\"rate\":1}, {\"a\":2,\"i\":2,\"rate\":1}, {\"a\":2,\"i\":7,\"rate\":1}, {\"a\":1,\"i\":2,\"rate\":1}, {\"a\":1,\"i\":6,\"rate\":1}, '''; load jsonStr.`jsonStr` as data; 现在我们可以使用ALS进行训练了： train data as ALSInPlace.`/tmp/model` where -- the first group of parameters `fitParam.0.maxIter`=\"5\" and `fitParam.0.regParam` = \"0.01\" and `fitParam.0.userCol` = \"a\" and `fitParam.0.itemCol` = \"i\" and `fitParam.0.ratingCol` = \"rate\" -- the sencond group of parameters and `fitParam.1.maxIter`=\"1\" and `fitParam.1.regParam` = \"0.1\" and `fitParam.1.userCol` = \"a\" and `fitParam.1.itemCol` = \"i\" and `fitParam.1.ratingCol` = \"rate\" -- compute rmse and evaluateTable=\"data\" and ratingCol=\"rate\" -- size of recommending items for user and `userRec` = \"10\" -- size of recommending users for item -- and `itemRec` = \"10\" and coldStartStrategy=\"drop\"; 在这里，我们配置了两组参数，并且使用rmse来评估效果，最后的结果是给每个用户10条内容。如果需要给每个内容推荐10个用户则设置itemRec参数即可。 最后的结果如下： name value modelPath /tmp/model/_model_14/model/0 algIndex 0 alg org.apache.spark.ml.recommendation.ALS metrics rmse: -0.011728744197876936 status success startTime 20190112 19:18:59:826 endTime 20190112 20:18:02:280 trainParams Map(ratingCol -> rate, itemCol -> i, userCol -> a, regParam -> 0.01, maxIter -> 5) ........ 你可以看看最后的预存结果： load parquet.`/tmp/model/data/userRec` as userRec; select * from userRec as result; 结果如下： a recommendations 1 [{\"i\":2,\"rating\":0.9975529},{\"i\":3,\"rating\":0.9835032},{\"i\":6,\"rating\":0.9835032},{\"i\":7,\"rating\":0.805835}] 2 [{\"i\":2,\"rating\":0.9970016},{\"i\":7,\"rating\":0.9838716},{\"i\":3,\"rating\":0.82125527},{\"i\":6,\"rating\":0.82125527}] 预测 该算法不支持批量预测以及APi预测            该文件修订时间： 2021-01-12 10:27:46 "},"algs/random_forest.html":{"url":"algs/random_forest.html","title":"RandomForest","keywords":"","body":"RandomForest RandomForest是一个分类算法。 -- create test data set jsonStr=''' {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0}, {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.4,2.9,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.7,3.2,1.3,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} '''; load jsonStr.`jsonStr` as data; select vec_dense(features) as features ,label as label from data as data1; -- use RandomForest train data1 as RandomForest.`/tmp/model` where -- once set true,every time you run this script, MLSQL will generate new directory for you model keepVersion=\"true\" -- specicy the test dataset which will be used to feed evaluator to generate some metrics e.g. F1, Accurate and evaluateTable=\"data1\" -- specify group 0 parameters and `fitParam.0.labelCol`=\"features\" and `fitParam.0.featuresCol`=\"label\" and `fitParam.0.maxDepth`=\"2\" -- specify group 1 parameters and `fitParam.1.featuresCol`=\"features\" and `fitParam.1.labelCol`=\"label\" and `fitParam.1.maxDepth`=\"10\" ; 最后输出结果如下： name value --------------------------------- modelPath /tmp/model/_model_10/model/1 algIndex 1 alg org.apache.spark.ml.classification.RandomForestClassifier metrics f1: 0.7625000000000001 weightedPrecision: 0.8444444444444446 weightedRecall: 0.7999999999999999 accuracy: 0.8 status success startTime 20180913 59:15:32:685 endTime 20180913 59:15:36:317 trainParams Map(maxDepth -> 10) --------------------------------- modelPath /tmp/model/_model_10/model/0 algIndex 0 alg org.apache.spark.ml.classification.RandomForestClassifier metrics f1:0.7625000000000001 weightedPrecision: 0.8444444444444446 weightedRecall: 0.7999999999999999 accuracy: 0.8 status success startTime 20180913 59:1536:318 endTime 20180913 59:1538:024 trainParams Map(maxDepth -> 2, featuresCol -> features, labelCol -> label) 对于大部分内置算法而言，都支持如下几个特性： 可以通过keepVersion 来设置是否保留版本。 通过fitParam.数字序号 配置多组参数，设置evaluateTable后系统自动算出metrics. 批量预测 predict data1 as RandomForest.`/tmp/model`; 结果如下： features label rawPrediction probability prediction {\"type\":1,\"values\":[5.1,3.5,1.4,0.2]} 0 {\"type\":1,\"values\":[16.28594461094461,3.7140553890553893]} {\"type\":1,\"values\":[0.8142972305472306,0.18570276945276948]} 0 {\"type\":1,\"values\":[5.1,3.5,1.4,0.2]} 1 {\"type\":1,\"values\":[16.28594461094461,3.7140553890553893]} {\"type\":1,\"values\":[0.8142972305472306,0.18570276945276948]} 0 API预测 register RandomForest.`/tmp/model` as rf_predict; -- you can specify which module you want to use: register RandomForest.`/tmp/model` as rf_predict where algIndex=\"0\"; -- you can specify which metric the MLSQL should use to get best model register RandomForest.`/tmp/model` as rf_predict where autoSelectByMetric=\"f1\"; select rf_predict(features) as predict_label, label from data1 as output; algIndex可以选择我用哪组参数得到的算法。我们也可以让系统自动选择，前提是我们在训练时配置了evalateTable， 这个只要使用autoSelectByMetric即可。 最后，就可以像使用一个函数一样对一个feature进行预测了。            该文件修订时间： 2021-01-12 10:27:46 "},"algs/lda.html":{"url":"algs/lda.html","title":"LDA","keywords":"","body":"LDA 主题模型是我非常喜欢的一个模型。为什么呢？ 首先它是无监督，其次他可以给词和内容都可以算出N个主题的概率分布，从而使得词和内容都可以计算了。 下面看看如何使用： set jsonStr=''' {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0}, {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.4,2.9,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.7,3.2,1.3,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} '''; load jsonStr.`jsonStr` as data; select vec_dense(features) as features ,label as label from data as data1; train data1 as LDA.`/tmp/model` where -- k: number of topics, or number of clustering centers k=\"3\" -- docConcentration: the hyperparameter (Dirichlet distribution parameter) of article distribution must be >1.0. The larger the value is, the smoother the predicted distribution is and docConcentration=\"3.0\" -- topictemperature: the hyperparameter (Dirichlet distribution parameter) of the theme distribution must be >1.0. The larger the value is, the more smooth the distribution can be inferred and topicConcentration=\"3.0\" -- maxIterations: number of iterations, which need to be fully iterated, at least 20 times or more and maxIter=\"100\" -- setSeed: random seed and seed=\"10\" -- checkpointInterval: interval of checkpoints during iteration calculation and checkpointInterval=\"10\" -- optimizer: optimized calculation method currently supports \"em\" and \"online\". Em method takes up more memory, and multiple iterations of memory may not be enough to throw a stack exception and optimizer=\"online\" ; 上面大部分参数都不需要配置。训练完成后会返回状态： --------------- ------------------ modelPath /tmp/model/_model_21/model/0 algIndex 0 alg org.apache.spark.ml.clustering.LDA metrics status success startTime 20190112 36:18:02:057 endTime 20190112 36:18:06:873 trainParams Map() 批量预测 predict data1 as LDA.`/tmp/model` ; 结果如下： features label topicDistribution {\"type\":1,\"values\":[5.1,3.5,1.4,0.2]} 0 {\"type\":1,\"values\":[0.9724967882100011,0.01374292627483604,0.01376028551516305]} 目前批量预测还不支持词的主题分布计算。 API预测 目前只支持spark 2.3.x register LDA.`/tmp/model` as lda; select label,lda(4) topicsMatrix,lda_doc(features) TopicDistribution,lda_topic(label,4) describeTopics from data as result; 同样的当你注册lda函数事，会给隐式生成多个函数: lda 接受一个词 lda_doc 接受一个文档 lda_topic 接受一个主题，以及显示多少词            该文件修订时间： 2021-01-12 10:27:46 "},"algs/xgboost.html":{"url":"algs/xgboost.html","title":"XGBoost","keywords":"","body":"XGBoost XGBoostExt 基于 xgboost4j-spark. 开发而成，相关参数大家可以参考相应的官方文档。 目前只支持spark 2.3.x XGBoost使用上和其他内置算法完全一样： set jsonStr=''' {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0}, {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.4,2.9,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.7,3.2,1.3,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} '''; load jsonStr.`jsonStr` as data; select vec_dense(features) as features ,label as label from data as data1; train data1 as XGBoostExt.`/tmp/model`; 输出结果： name value --------------- ------------------ modelPath /tmp/model/_model_24/model/0 algIndex 0 alg ml.dmlc.xgboost4j.scala.spark.WowXGBoostClassifier metrics status true startTime 20190112 50:18:49:395 endTime 20190112 50:18:54:088 trainParams Map() 批量预测 -- batch predict predict data1 as XGBoostExt.`/tmp/model`; API 预测 -- api predict register XGBoostExt.`/tmp/model` as npredict; select npredict(features) from data as output;            该文件修订时间： 2021-01-12 10:27:46 "},"dl/":{"url":"dl/","title":"基于Java的深度学习框架集成","keywords":"","body":"深度学习 【文档更新日志：2020-04-07】 Note: 本文档适用于MLSQL Engine 1.6.0-SNAPSHOT/1.6.0 及以上版本。对应的Spark版本应该为2.4.5,不低于2.4.3。 MLSQL除了可以集成Tensorflow等具备Python接口的框架以外，也提供了一个深度学习算法框架BigDL的插件。使用该框架，你无需任何配置即可使用。 在使用前，你需要安装该插件： !plugin app add - \"mlsql-bigdl-2.4\"; 除此之外，你还需要在启动Engine时，设置如下参数： -streaming.bigdl.enable true 然后就可以使用BigDL的所有功能了。核心功能我们会在后续的章节介绍： 如何加载和处理你的图片 如何创建模型并且进行训练，然后预测            该文件修订时间： 2021-01-12 10:27:46 "},"dl/load_image.html":{"url":"dl/load_image.html","title":"加载图片数据","keywords":"","body":"加载图片数据 其实MLSQL内置了好几个图片处理ET(Esitmator/Transformer),不过我们今天只会接受 ImageLoaderExt. 我们来看一段示例： set imageDir=\"/Users/allwefantasy/Downloads/jack\"; run command as ImageLoaderExt.`${imageDir}` where code=''' def apply(params:Map[String,String]) = { Resize(256, 256) -> CenterCrop(224, 224) -> MatToTensor() -> ImageFrameToSample() } ''' as images; select imageName from images limit 1 as output; ImageLoaderExt支持传递Scala处理代码对图片进行处理。我们看看里面的代码片段,首先方法是apply,这个是默认的，而且是必须。其次里面的params参数其实是where条件里的其他参数。 接着方法主体是一个DSL: Resize(256, 256) -> CenterCrop(224, 224) -> MatToTensor() -> ImageFrameToSample() 对每张图片做了四个操作： 把图片缩放为256*256大小 进行裁切为224*224 图片转化为张量 最后转化为后续深度学习可以高效处理的结构（其实就是Image DataFrame） 通常加载和处理图片是一个比较慢的过程 其中3,4两部分每次你都带上就好。那我还有哪些可以操作呢？ Brightness(deltaLow: Double, deltaHigh: Double) Hue(deltaLow: Double, deltaHigh: Double) Saturation(deltaLow: Double, deltaHigh: Double) Contrast(deltaLow: Double, deltaHigh: Double) ChannelOrder() //随机修改图片channel顺序 ColorJitter(brightnessProb: Double = 0.5, brightnessDelta: Double = 32, contrastProb: Double = 0.5, contrastLower: Double = 0.5, contrastUpper: Double = 1.5, hueProb: Double = 0.5, hueDelta: Double = 18, saturationProb: Double = 0.5, saturationLower: Double = 0.5, saturationUpper: Double = 1.5, randomOrderProb: Double = 0, shuffle: Boolean = false) Resize(resizeH: Int, resizeW: Int, resizeMode: Int = Imgproc.INTER_LINEAR, useScaleFactor: Boolean = true) AspectScale(scale: Int, scaleMultipleOf: Int = 1, maxSize: Int = 1000) /* image channel normalize meanR mean value in R channel meanG mean value in G channel meanB mean value in B channel stdR std value in R channel stdG std value in G channel stdB std value in B channel */ ChannelNormalize(meanR: Float, meanG: Float, meanB: Float, stdR: Float = 1, stdG: Float = 1, stdB: Float = 1) PixelNormalizer(means: Array[Float]) CenterCrop(cropWidth: Int, cropHeight: Int, isClip: Boolean = true) RandomCrop(cropWidth: Int, cropHeight: Int, isClip: Boolean = true) FixedCrop(x1: Float, y1: Float, x2: Float, y2: Float, normalized: Boolean, isClip: Boolean = true) DetectionCrop(roiKey: String, normalized: Boolean = true) /* expand image, fill the blank part with the meanR, meanG, meanB meansR means in R channel meansG means in G channel meansB means in B channel minExpandRatio min expand ratio maxExpandRatio max expand ratio */ Expand(meansR: Int = 123, meansG: Int = 117, meansB: Int = 104, minExpandRatio: Double = 1, maxExpandRatio: Double = 4.0) /* Fill part of image with certain pixel value startX start x ratio startY start y ratio endX end x ratio endY end y ratio value filling value */ Filler(startX: Float, startY: Float, endX: Float, endY: Float, value: Int = 255) HFlip() RandomTransformer(transformer: FeatureTransformer, maxProb: Double)            该文件修订时间： 2021-01-12 10:27:46 "},"dl/cifar10.html":{"url":"dl/cifar10.html","title":"Cifar10示例","keywords":"","body":"Cifar10示例 这个章节我们会通过操作Cifar10来演示如何 图片准备 首先你要下载图片集，网络上大部分都不是原生图片了，都是已经转化为向量的。大家可以通过这个链接 cifar.tgz 获得图片。 解压后应该有两个目录，一个是train(约6万张)，一个是test.这次示例我们只用train目录下的。 加载图片 上一个章节我们已经掌握了如何加载图片数据。也就是通过如下代码，我们已经获得了图片： set labelMappingPath = \"/tmp/si\"; set imageConvertPath = \"/tmp/cifar_train_data\"; set modelPath = \"/tmp/cifar-model\"; set imageDir=\"/Users/allwefantasy/Downloads/jack\"; run command as ImageLoaderExt.`${imageDir}` where code=''' def apply(params:Map[String,String]) = { Resize(256, 256) -> CenterCrop(224, 224) -> MatToTensor() -> ImageFrameToSample() } ''' as images; 抽取分类 通常，我们需要对图片路径也进行处理，这样才好得到分类。这可以通过如下方式： -- convert image path to number label select split(split(imageName,\"_\")[1],\"\\\\.\")[0] as labelStr,features from data as newdata; train newdata as StringIndex.`${labelMappingPath}` where inputCol=\"labelStr\" and outputCol=\"labelIndex\" as newdata1; predict newdata as StringIndex.`${labelMappingPath}` as newdata2; select (cast(labelIndex as int) + 1) as label,features from newdata2 as newdata3; 保存处理好的图片数据，方便下次训练时使用 -- save image processing result. save overwrite newdata3 as parquet.`${imageConvertPath}`; 训练 load parquet.`${imageConvertPath}` as newdata3; select array(cast(label as float)) as label,features from newdata3 as newdata4; --train with LeNet5 model train newdata4 as BigDLClassifyExt.`${modelPath}` where fitParam.0.featureSize=\"[3,28,28]\" and fitParam.0.classNum=\"10\" and fitParam.0.maxEpoch=\"50\" and fitParam.0.code=''' def apply(params:Map[String,String])={ val model = Sequential() model.add(Reshape(Array(3, 28, 28), inputShape = Shape(28, 28, 3))) model.add(Convolution2D(6, 5, 5, activation = \"tanh\").setName(\"conv1_5x5\")) model.add(MaxPooling2D()) model.add(Convolution2D(12, 5, 5, activation = \"tanh\").setName(\"conv2_5x5\")) model.add(MaxPooling2D()) model.add(Flatten()) model.add(Dense(100, activation = \"tanh\").setName(\"fc1\")) model.add(Dense(params(\"classNum\").toInt, activation = \"softmax\").setName(\"fc2\")) } ''' ; 和其他内置的普通算法一样，我们可以配置多组参数从而进行多组参数的训练。 featureSize, 描述张量形状，通道在前 classNum 分类数目， cifar10是10个分类 maxEpoch进行多少个epoch code DSL 部分，该部分主要进行模型的构建。 和数据加载一样，它也是一段基于scala语法的DSL,可以让你以Keras的形态描述网络结构。 在这里我们构建一个leNet网络。 训练时间会比较长。我们建议修改spark日志，将org.spark.spark相关的日志都设置成WARN.因为日志太多了。 批预测 predict newdata4 as BigDLClassifyExt.`${modelPath}` as predictdata; 预测结果的分类数字需要+1才是正确的分类。 API预测 -- deploy with api server register BigDLClassifyExt.`/tmp/bigdl` as cifarPredict; select vec_argmax(cifarPredict(vec_dense(features))) as predict_label, label from data as output;            该文件修订时间： 2021-01-12 10:27:46 "},"api_deploy/":{"url":"api_deploy/","title":"部署算法API服务","keywords":"","body":"部署算法API服务 能够不做任何开发，就可以把数据特征工程和模型预测迁移到API服务上是MLSQL最强悍的地方。 这个章节我们会为您揭秘相关原理。            该文件修订时间： 2021-01-12 10:27:46 "},"api_deploy/design.html":{"url":"api_deploy/design.html","title":"设计和原理","keywords":"","body":"设计和原理 使用MLSQL完成模型训练后，这个时候，我们肯定想迫不及待的把模型部署然后提供API服务。 通常，模型使用的场景有三个： 批处理。 比如对历史数据做统一做一次预测处理。 流式计算。 希望把模型部署在流式程序里。 API服务。 希望通过API 对外提供模型预测服务。（这是一种最常见的形态） 在MLSQL中，所以的特征工程ET(Estimator/Transformer)都可以被注册成UDF函数，同时所有的模型也可以被注册UDF函数， 这样只要API Server 支持注册这些函数，我们就可以通过这些函数的组合，完成一个端到端的预测服务了。 下面是原理图： 训练阶段： 文本集合 ------ TF/IDF 向量(TFIDFInplace) ----- 随机森林(RandomForest) | | 产出 model model | | register register | | 预测服务 单文本 ------- udf1 --------------- udf2 ---> 预测结果 所有训练阶段产生的model都可以被API Server注册，然后使用，从而无需开发便可实现端到端的预测了。            该文件修订时间： 2021-01-12 10:27:46 "},"api_deploy/case.html":{"url":"api_deploy/case.html","title":"部署流程","keywords":"","body":"部署流程 我们对MLSQL的Local模式做了优化，从而实现毫秒级的预测效果。 只要以local模式启动MLSQL，通常你可以认为这是一个标准的Java应用： ./bin/spark-submit --class streaming.core.StreamingApp \\ --master local[2] \\ --name predict_service \\ streamingpro-mlsql-x.x.xjar \\ -streaming.name predict_service \\ -streaming.platform spark \\ -streaming.rest true \\ -streaming.driver.port 9003 \\ -streaming.spark.service true \\ -streaming.thrift false \\ -streaming.enableHiveSupport true \\ -streaming.deploy.rest.api true 其中最后一行-streaming.deploy.rest.api true开启了优化，可以让MLSQL示例跑的更快。 访问 http://127.0.0.1:9003/run/script 接口动态注册在训练阶段生成的模型： register TfIdfInPlace.`/tmp/tfidf_model` as tfidf_predict; register RandomForest.`/tmp/rf_model` as bayes_predict; 访问：http://127.0.0.1:9003/model/predict进行预测请求： 请求参数为： dataType=row data=[{\"feature\":[1,2,3...]}] sql=select bayes_predict(vec_dense(feature)) as p Property Name Default Meaning dataType vector data字段的数据类型，目前只支持vector/string/row data [] 你可以传递一个或者多个vector/string/json,必须符合json规范 sql None 用sql的方式调用模型，其中如果是vector/string,则模型的参数feature是固定的字符串，如果是row则根据key决定 pipeline None 用pipeline的方式调用模型，写模型名，然后逗号分隔，通常只能支持vector/string模式 完整例子 启动MLSQL集群，进行训练： --NaiveBayes set jsonStr=''' {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0}, {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.4,2.9,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[4.7,3.2,1.3,0.2],\"label\":1.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} {\"features\":[5.1,3.5,1.4,0.2],\"label\":0.0} '''; load jsonStr.`jsonStr` as data; select vec_dense(features) as features ,label as label from data as data1; -- use RandomForest train data1 as RandomForest.`/tmp/model` where -- once set true,every time you run this script, MLSQL will generate new directory for you model keepVersion=\"true\" -- specicy the test dataset which will be used to feed evaluator to generate some metrics e.g. F1, Accurate and evaluateTable=\"data1\" -- specify group 0 parameters and `fitParam.0.labelCol`=\"features\" and `fitParam.0.featuresCol`=\"label\" and `fitParam.0.maxDepth`=\"2\" -- specify group 1 parameters and `fitParam.1.featuresCol`=\"features\" and `fitParam.1.labelCol`=\"label\" and `fitParam.1.maxDepth`=\"10\" ; 按前面启动一个MLSQL java程序，然后注册前面的模型： register RandomForest.`/tmp/model` as rf_predict; 接着就可以外部调用API使用了,需要传递两个参数： dataType=row data=[{\"feature\":[1,2,3...]}] sql=select bayes_predict(vec_dense(feature)) as p 最后的预测结果为： { \"p\": { \"type\": 1, \"values\": [ 1, 0 ] } } 另外，大部分模块都是可以通过这种方式进行注册，不仅仅是算法。这就实现了端到端的部署，允许你将预处理逻辑也部署上。 通过sql select 语法，你还可以完成一些较为复杂的预测逻辑。            该文件修订时间： 2021-01-12 10:27:46 "},"udf/":{"url":"udf/","title":"MLSQL动态创建UDF/UDAF","keywords":"","body":"动态创建UDF/UDAF MLSQL 支持使用 Python和 Scala写UDF/UDAF。而且无需编译和打包或者重启，可以即时生效， 可以极大的方便用户增强SQL的功能。 这里对于Python UDF的支持值得注意的有如下几点： python不支持任何native库，比如numpy. python可能会有类型问题的坑，同时需要指定返回值。 我们提供了专门的交互式python语法以及大规模数据处理的python语法，用以弥补python UDF的不足。在Python专门章节 我们会提供更详细的介绍。 所以我们建议对于python尽可能只做简单的文本解析处理，以及使用原生自带的库。 目前MLSQL使用的是jython 2.7.1,更多细节可参考官网。            该文件修订时间： 2021-01-12 10:27:47 "},"udf/python_udf.html":{"url":"udf/python_udf.html","title":"Python UDF","keywords":"","body":"Python UDF 下面是一个完整的示例： -- using set statement to hold your python script -- Notice that the first parameter of function you defined should be self. set echoFun=''' def apply(self,m): return m '''; -- load script as a table, every thing in mlsql should be table which -- can be processed more conveniently. load script.`echoFun` as scriptTable; -- register `apply` as UDF named `echoFun` register ScriptUDF.`scriptTable` as echoFun options -- specify which script you choose and lang=\"python\" -- As we know python is not strongly typed language, so -- we should manually spcify the return type. -- map(string,string) means a map with key is string type,value also is string type. -- array(string) means a array with string type element. -- nested is support e.g. array(array(map(string,array(string)))) and dataType=\"map(string,string)\" ; -- create a data table. set data=''' {\"a\":1} {\"a\":1} {\"a\":1} {\"a\":1} '''; load jsonStr.`data` as dataTable; -- using echoFun in SQL. select echoFun(map('a','b')) as res from dataTable as output; 我们前面说到，对于Python UDF,我们需要显示的指定返回数据类型，在示例中是 and dataType=\"map(string,string)\" 这表示返回值是一个字典，并且key和value都是string. Python UDF并不能返回任意类型，只能是如下类型以及对应的组合: string float double integer short date binary map array 其中map 和array 为复杂类型，里面对应的值只能罗列的这些类型。所以我们也可以做比较复杂的结果返回： array(array(map(string,array(string)))) 上面的类型描述也是合法的。 上面的做法方便做代码分割，udf申明可以放在单独文件，注册动作可以放在另外的文件，之后都通过include来完成整合。我们也有更简单的模式，比如 register ScriptUDF.`` as echoFun options and lang=\"python\" and dataType=\"map(string,string)\" and code='''' def apply(self,m): return m ''''; 把代码直接写在了code里，而不是先set 再load，最后引用。 如果函数名称不是apply,那么你需要通过methodName 来进行应用，大致如下： register ScriptUDF.`` as echoFun options and lang=\"python\" and dataType=\"map(string,string)\" and methodName=\"jack\" and code='''' def jack(self,m): return m ''''; 另外，我们可以写多个函数，然后通过methodName指定需要哪个。            该文件修订时间： 2021-01-12 10:27:47 "},"udf/scala_udf.html":{"url":"udf/scala_udf.html","title":"Scala UDF","keywords":"","body":"Scala UDF 下面是一个典型例子： set plusFun=''' def apply(a:Double,b:Double)={ a + b } '''; -- load script as a table, every thing in mlsql should be table which -- can be process more convenient. load script.`plusFun` as scriptTable; -- register `apply` as UDF named `plusFun` register ScriptUDF.`scriptTable` as plusFun ; -- create a data table. set data=''' {\"a\":1} {\"a\":1} {\"a\":1} {\"a\":1} '''; load jsonStr.`data` as dataTable; -- using echoFun in SQL. select plusFun(1,2) as res from dataTable as output; 我们也可以简化： register ScriptUDF.`` as plusFun where and lang=\"scala\" and udfType=\"udf\" code=''' def apply(a:Double,b:Double)={ a + b } ''';            该文件修订时间： 2021-01-12 10:27:47 "},"udf/scala_udaf.html":{"url":"udf/scala_udaf.html","title":"Scala UDAF","keywords":"","body":"Scala UDAF 下面是一个典型例子： set plusFun=''' import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction} import org.apache.spark.sql.types._ import org.apache.spark.sql.Row class SumAggregation extends UserDefinedAggregateFunction with Serializable{ def inputSchema: StructType = new StructType().add(\"a\", LongType) def bufferSchema: StructType = new StructType().add(\"total\", LongType) def dataType: DataType = LongType def deterministic: Boolean = true def initialize(buffer: MutableAggregationBuffer): Unit = { buffer.update(0, 0l) } def update(buffer: MutableAggregationBuffer, input: Row): Unit = { val sum = buffer.getLong(0) val newitem = input.getLong(0) buffer.update(0, sum + newitem) } def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = { buffer1.update(0, buffer1.getLong(0) + buffer2.getLong(0)) } def evaluate(buffer: Row): Any = { buffer.getLong(0) } } '''; --加载脚本 load script.`plusFun` as scriptTable; --注册为UDF函数 名称为plusFun register ScriptUDF.`scriptTable` as plusFun options className=\"SumAggregation\" and udfType=\"udaf\" ; set data=''' {\"a\":1} {\"a\":1} {\"a\":1} {\"a\":1} '''; load jsonStr.`data` as dataTable; -- 使用plusFun select a,plusFun(a) as res from dataTable group by a as output; 为了方便使用，我们也可以定义多个方法，然后分别注册： set plusFun=''' class A { def apply(a:Double,b:Double)={ a + b } def hello(a:String)={ s\"hello: ${a}\" } } '''; load script.`plusFun` as scriptTable; register ScriptUDF.`scriptTable` as plusFun where methodName=\"apply\" and className=\"A\"; register ScriptUDF.`scriptTable` as helloFun options methodName=\"hello\" and className=\"A\"; -- using echoFun in SQL. select plusFun(1,2) as plus, helloFun(\"jack\") as jack as output;            该文件修订时间： 2021-01-12 10:27:47 "},"udf/java_udf.html":{"url":"udf/java_udf.html","title":"Java UDF","keywords":"","body":"Java UDF 下列是一个Java运行时编译UDF的例子： set echoFun=''' import java.util.HashMap; import java.util.Map; public class UDF { public Map apply(String s) { Map m = new HashMap<>(); Integer[] arr = {1}; m.put(s, arr); return m; } } '''; load script.`echoFun` as scriptTable; register ScriptUDF.`scriptTable` as funx options lang=\"java\" ; -- create a data table. set data=''' {\"a\":\"a\"} '''; load jsonStr.`data` as dataTable; select funx(a) as res from dataTable as output; 由于java语言的特殊性，我们需要注意几点： 传递的代码必须是一个java类，并且默认系统会寻找UDF.apply()做为运行的udf，如果需要特殊类名和方法名，需要在register时，声明options，例如例子2。 不支持包名。 例子2: set echoFun=''' import java.util.HashMap; import java.util.Map; public class Test { public Map test(String s) { Map m = new HashMap<>(); m.put(s, s); return m; } } '''; load script.`echoFun` as scriptTable; register ScriptUDF.`scriptTable` as funx options lang=\"java\" and className = \"Test\" and methodName = \"test\" ; -- create a data table. set data=''' {\"a\":\"a\"} '''; load jsonStr.`data` as dataTable; select funx(a) as res from dataTable as output;            该文件修订时间： 2021-01-12 10:27:47 "},"security/sum.html":{"url":"security/sum.html","title":"安全与隐私","keywords":"","body":"安全与隐私 MLSQL 提供了很多工具用于加强引擎的和数据的安全访问控制。 目前，我们会集中在如下两个主题进行探讨： MLSQL Engine 接口访问控制 授权用户对数据的访问控制            该文件修订时间： 2021-01-12 10:27:47 "},"security/":{"url":"security/","title":"MLSQL Engine 接口访问控制","keywords":"","body":"MLSQL Engine访问安全加固 MLSQL 默认使用Http协议对外提供服务。通常，MLSQL Engine应该部署在一个可信的环境（比如内网）。不过MLSQL Engine还是提供了多种方式 允许用户开启访问限制，最大限度保证用户的应用与数据安全。            该文件修订时间： 2021-01-12 10:27:47 "},"security/token_control.html":{"url":"security/token_control.html","title":"接口访问Token设置","keywords":"","body":"接口访问Token设置 MLSQL Engine 对外提供HTTP服务默认是没有权限验证的，原因是我们假设他是在一个内网安全环境，只有特定应用才能访问该服务。MLSQL Engine从2.0.0版本开始对接口访问提供了Token验证。 设置启动Token 在spark-submit 命令行里，添加 --conf spark.mlsql.auth.access_token=your-token-string 最后如果用户要访问Engine接口，必须在请求参数里带上 your-token-string才能访问。 对于MLSQL Console,则需要在建立Engine的时候，填写 token:            该文件修订时间： 2021-01-12 10:27:47 "},"security/custom_control.html":{"url":"security/custom_control.html","title":"自定义接口访问策略","keywords":"","body":"自定义接口访问策略 除了前面提到的Token验证，用户也可以开发自己的验证逻辑，比如请求验证服务器等。 可以在启动脚本里配置如下参数： --jars your-jar-contains-auth-class --conf spark.mlsql.auth.custom=your-auth-class-name 现阶段（2.0.0）版本，你需要用scala开发，保证验证auth-class里有如下函数： def auth(params: Map[String, String]): (Boolean, String) params会包含owner,sql等各种相关信息。 第一个返回值是布尔值，true表示通过验证，false表示没有通过验证。第二个返回值是当第一个返回值为false的情况下，给出的原因说明。            该文件修订时间： 2021-01-12 10:27:47 "},"security/sum2.html":{"url":"security/sum2.html","title":"授权用户对数据的访问控制","keywords":"","body":"授权用户对数据的访问控制 MLSQL对任何接入的数据源，任何形式的资源访问都可以抽象成库表，并且使用[库-表-字段-操作]形式实现权限认证。 指的注意的是，这种认证直接集成在语言层面。 库表是资源的表现形式，列则表达了MLSQL对数据访问控制的粒度。 而在操作上，我们将其分成两类： 数据的操作，比如你是不是能对一张表进行load/save/directQuery/create/drop/insert/update/select/set 语言工具。比如你是不是能够使用TfIdfInPlace工具，如果你没有被授权使用TfIdfInPlace工具,你就无法使用该工具在MLSQL中做任何事情。 HDFS如何抽象成库表 HDFS 可以理解是一个default数据库。对应的每个路径则是一个表。默认用户主目录下的内容都是可以被访问的。对于任何非主目录则需要单独进行授权。 比如，如果你被授权了 /tmp/work 目录,那么你对/tmp/work以及所有的子目录具有读或者写权限。 结构化数据库如何抽象成库表 对于 MySQL、Hive、ES、HBase 等数据源默认就是库表形态的。 MySQL 和 Hive 数据源：数据库表名对应权限认证的库表名 HBase 数据源：namespace 名称对应库名，HBase 表名对应表名 ES 数据源：ES 索引名称对应库名， ES 索引下的不同类型对应表名 [info] 一些特例情况 考虑到ES经常会拆分索引，ES数据源应当支持正则匹配权限认证： 比如说您拥有索引名为「es_index_test」的权限，那么您同时拥有「es_index_test-2020.09.27」与「es_index_test-2020-09-27」的权限（具体日期随意） 授权规则 得益于MLSQL的良好语法，比如，当我们授权 hive表 db1.table1 的load权限给user1时，此时用户运行如下语句就会被通过： load hive.`db1.table1` as table1; 否则就不会得到通过。MLSQL还可以支持到列级别的控制，尽管我们授权 hive表 db1.table1 的load权限给user1，我们也可以同时限制他能够看到哪些列，亦或是对列进行脱敏。 实现机制 MLSQL会在两个阶段收集校验信息。第一个是做语法预处理的时候，他会抽取出用户所要访问的资源，并且将其发送给指定的权限校验服务。 第二个阶段是，他会在解析运行时执行校验，这主要服务于列权限。系统在执行Load的时候，会根据权限服务器，过滤掉不符合权限要求的列。同样在解析运行时 执行校验的还有ET组件，譬如我们前面提到的TfIdfInPlace工具。 权限服务可以是用户自己开发的，也可以是官方提供的。            该文件修订时间： 2021-01-12 10:27:47 "},"resources/dynamic_resource.html":{"url":"resources/dynamic_resource.html","title":"动态资源调整","keywords":"","body":"资源动态调整 当你在实际的生产环境使用MLSQL Stack之后，大量的MLSQL Engine会因为各种场景被部署： 有共分析师，业务探索使用的，有供作为API使用的，有做ETL引擎使用的，有做流使用的，有做机器学习平台使用的，这对服务提供方带来了大量的资源压力。比如业务探索类的，典型的上班时间资源需求大，下班时间基本上没怎么有人用，ETL则是反之，API也有一定方位。而且业务探索类的其实对响应有一定的实时需求，比如一个复杂的脚本使用者期待10s内返回，如果使用Spark 的DRA反复的资源释放和申请，可能临时申请资源的时间就已经超过10s，更不用提计算了。 这个时候我们更希望的是通过统计分析，自己制定一个资源调整策略，从而节省系统资源，减轻压力。MLSQL 为此提出这个MPIP,管理员只要通过简单的两个三个命令就可以添加或者删除CPU/内存资源。 !resource add 10c; !resource remove 10c; !resource set 40c; 第一个表示给当前正在运行的MLSQL Engine添加10个核，第二个表示减少10核，第三个则表示将当前的MLSQL Engine的资源设置为40核。 那么内存呢？内存会根据启动时配置的CPU和内存的比例，进行相应的增加或者删减。比如我们启动时，CPU和内存为1:4，那么我们添加10c,相应的，系统会自动增加40G的内存。 有了这个功能后，我们完全可以开发一个资源控制策略，对于探索类应用，每天上班时，自动增加资源，下班时剔除资源，用户也可以主动向管理员临时申请更多资源，实现真正的弹性，整个过程无需重启。            该文件修订时间： 2021-01-12 10:27:47 "},"store/":{"url":"store/","title":"MLSQL 脚本商店和插件","keywords":"","body":"MLSQL 脚本商店和插件 MLSQL 作为脚本语言，提供了很多可复用特性，所以用户可以将自己写的一些脚本提供出来，供其他用户使用。 MLSQL脚本商店目前就是做这个用途的。 如果觉得自己的MLSQL脚本很酷，欢迎到MLSQL Github ISSUE中黏贴出来，我们会放到官方仓库里。 脚本类型的插件其实有很多天然优势： 无需重启或者部署。修改完即可生效； 可以多个版本同时存在，只需要使用不同的名字即可。 结合python代码或者是scala/java UDF/UDAF，也可以实现很多复杂的功能。 与此同时，MLSQL也支持使用Scala/Java编写的系统插件。我们在下面章节会一一进行介绍。            该文件修订时间： 2021-01-12 10:27:47 "},"store/save_excel.html":{"url":"store/save_excel.html","title":"脚本插件save_excel","keywords":"","body":"脚本插件 save_excel 用户可以通过安装比较重的数据源插件来完成相同的功能。参考网络安装插件。 在前文中，我们通过动态安装jar包的方式，使得我们能够操作excel数据源。 对于读取，我们可以通过参数maxRowsInMemory很好的控制内存。但是对于写入，该数据源插件则没有相关控制内存的使用参数，这导致保存较大的excel文件时， 内存使用过大，影响整个Engine的稳定性。 通过 脚本插件，save_excel，我们不仅可以避免影响系统稳定性，同时完全不需要安装插件，只需要使用include 语法引用即可。 参看如下使用代码： select 1 as a, 2 as b as mockTable; --如果是在demo站点，加上下面的配置，因为该插件需要python支持 -- !python env \"PYTHON_ENV=source /usr/local/miniconda/bin/activate dev\"; set inputTable = \"mockTable\"; set outputPath = \"/tmp/jack.xlsx\"; include store.`william/save_excel`; 这里，我们随意模拟了一张表 mockTable,然后通过set 语法配置两个参数： inputTable outputPath 接着 include 商店里的save_excel插件即可。 点击执行，就可以在/tmp目录里看到jack.xlsx数据啦。 如果用户么有办法访问公网，可以通过如下python代码在有网络的环境下获取脚本内容： import requests import json res = requests.post(\"http://store.mlsql.tech/run\", params={ \"action\":\"getPlugin\", \"pluginName\":'''william/save_excel''', \"version\": \"0.1.0\",\"pluginType\":\"MLSQL_SCRIPT\" }) json.loads(json.loads(res.content)[0][\"extraParams\"])[\"content\"] 然后拷贝出对应的代码，替换include store.save_excel;这一行，可以取得一样的效果。            该文件修订时间： 2021-01-12 10:27:47 "},"plugin/":{"url":"plugin/","title":"MLSQL Engine插件","keywords":"","body":"插件 目前大部分还没有为 spark 3.0 做适配。 MLSQL支持插件机制。官方插件位于MLSQL Plugins。 插件分成四种类型： ET 插件 DataSource 插件 Script 插件 App 插件 如果从数据处理的角度而言，DataSource插件可以让你扩展MLSQL访问你想要的数据源，而ET插件则可以完成数据处理相关的工作，甚至构建一个机器学习集群， 比如TF Cluster 实际上就是一个ET插件。 Script 插件则是一个更高层次的插件，是可复用的MLSQL代码。 为了使用该功能，你需要启动MLSQL Engine是设置 -streaming.datalake.path 参数，并确保运行MLSQL Engine的账号有权限读写该目录。            该文件修订时间： 2021-01-12 10:27:47 "},"plugin/online_install.html":{"url":"plugin/online_install.html","title":"网络安装插件","keywords":"","body":"网络安装插件 如果你内网（也可以通过自己设置代理）可以访问 http://store.mlsql.tech，那么你可以直接使用命令行方式在Console里安装。 比如如果需要安装excel支持，一行命令在MLSQL Console里即可搞定： !plugin ds add - \"mlsql-excel-2.4\"; 接着就可以用读取和保存excel格式数据了： load excel.`/tmp/upload/example_en.xlsx` where useHeader=\"true\" and maxRowsInMemory=\"100\" and dataAddress=\"A1:C8\" as data; select * from data as output; 更多可用插件到这里来看mlsql-plugins            该文件修订时间： 2021-01-12 10:27:47 "},"plugin/offline_install.html":{"url":"plugin/offline_install.html","title":"离线安装插件","keywords":"","body":"离线安装插件 2.0.1-SNAPSHOT、2.0.1 及以上版本支持。 考虑到很多场景，我们需要引擎启动的时候就具备某个插件的功能，亦或是我们没办法访问外部网络，这个时候就可以通过离线方式安装插件。 下载Jar包并且上传到你的服务器 首先，手动下载相应的Jar包(我这里用一段python代码下载) targetPath = \"./__mlsql__/mlsql-excel-2.4_2.11.jar\" with requests.get(\"http://store.mlsql.tech/run\",params={ \"action\":\"downloadPlugin\", \"pluginType\":'''MLSQL_PLUGIN''', \"pluginName\": \"mlsql-excel-2.4\", \"version\":\"0.1.0-SNAPSHOT\" }, stream=True) as r: r.raise_for_status() downloadSize = 0 with open(targetPath,\"wb\") as f: for chunk in r.iter_content(chunk_size=8192): downloadSize += 8192 print(str(downloadSize/1024)+\"\\n\") f.write(chunk) 值得注意的是，在上面的参数中，唯一需要根据场景修改的是pluginName和version. 启动时配置jar包以及启动类 在启动脚本里，配置插件主类(这里以Excel插件安装为例)： -streaming.plugin.clzznames tech.mlsql.plugins.ds.MLSQLExcelApp 同时使用 --jars 将我们上一个步骤的jar包带上。 这个时候你就可以使用excel数据源了。            该文件修订时间： 2021-01-12 10:27:47 "},"api/sum.html":{"url":"api/sum.html","title":"API on（Console/Engine）介绍","keywords":"","body":"API on（Console/Engine）介绍 MLSQL Console/Engine 都对外提供http协议的API服务。            该文件修订时间： 2021-01-12 10:27:46 "},"developer/":{"url":"developer/","title":"MLSQL Console API指南","keywords":"","body":"MLSQL Console API指南 一般而言，企业都有自己的账户体系， 亦或是我们需要对接已有的调度系统。MLSQL Console提供API以及多种扩展方式，允许用户完成这些对接。            该文件修订时间： 2021-01-12 10:27:46 "},"developer/script_api.html":{"url":"developer/script_api.html","title":"脚本获取接口","keywords":"","body":"脚本获取接口 1.7.0-SNAPSHOT/1.7.0 及以上版本可用 MLSQL Console本身提供了脚本管理功能，你可以书写，调试以及运行脚本。 在一些特定场景中，我们需要其他程序能够访问这些脚本，然后将这些脚本发送给引擎执行。典型场景如调度服务。 接口1： http://[mlsql-console-url]/api_v1/script_file/get 参数： 参数名 类型 含义 owner 字符串 拥有脚本的用户名 id 数字 脚本id 因为安全方面的原因，你还需要在header里传递access-token. access-token 对应的值在 MLSQL Console的application.yml配置文件里。 这个token具有很高的权限，请勿泄露。 接口2： http://[mlsql-console-url]/api_v1/script_file/include 参数： 参数名 类型 含义 owner 字符串 拥有脚本的用户名 path 脚本路径 同样，因为安全方面的原因，你还需要在请求header里传递access-token. access-token 对应的值在 MLSQL Console的application.yml配置文件里。 MLSQL Console内部include语法内容获取，也是基于这个接口完成的。            该文件修订时间： 2021-01-12 10:27:46 "},"developer/script_run_api.html":{"url":"developer/script_run_api.html","title":"脚本执行代理接口","keywords":"","body":"脚本执行代理接口 1.7.0-SNAPSHOT/1.7.0 及以上版本可用 我们可以通过提交脚本给MLSQL Console,然后Console会提交给后端引擎执行。在提交过程中，我们可以控制后端是异步还是同步返回。 使用MLSQL Console作为代理接口有诸多好处，因为Console会自动填写很多配置，简化了提交的复杂度。 接口 http://[mlsql-console-url]/api_v1/run/script 参数名 类型 含义 owner String 选择执行的用户 engineName string 可以选择后端执行的引擎 sql string 待执行脚本 jobName string 任务名称。建议保持不重复，比如使用uuid或者脚本id async boolean 是否异步提交给后端引擎。默认false,需要搭配callback使用 callback string 引擎执行完成后，会通过该URL进行回调 sessionPerRequest boolean 默认false,一个用户如果可以并发提交脚本，那么该选项一定要打开 access-token string 请求头参数，该token可以在MLSQL Console的配置文件中找到，名字叫： auth_secret 通过脚本获取接口,然后再通过该接口完成脚本提交，就可以实现调度所需的功能了。 下面是一段示例python代码： import requests res = requests.post(\"http://127.0.0.1:9002/api_v1/run/script\", params={ \"owner\":\"allwefantasy@gmail.com\", \"sql\":\"select 1 as a as b;\", \"jobName\": \"wow\" },headers={\"access-token\":\"xxxx\"}) res.content 结果如下：            该文件修订时间： 2021-01-12 10:27:46 "},"api/run-script.html":{"url":"api/run-script.html","title":"脚本执行接口","keywords":"","body":"/run/script 接口 该接口用来执行MLSQL语句。 参数列表 参数 说明 示例值 sql 需要执行的MLSQL内容 owner 当前发起请求的租户 jobType 任务类型 script/stream/sql 默认script executeMode 如果是执行MLSQL则为query,如果是为了解析MLSQL则为analyze。很多插件会提供对应的executeMode从而使得用户可以通过HTTP接口访问插件功能 jobName 任务名称，一般用uuid或者脚本id,最好能带上一些信息，方便更好的查看任务 timeout 任务执行的超时时间 单位毫秒 silence 最后一条SQL是否执行 默认为 false sessionPerUser 按用户创建sesison 默认为 true sessionPerRequest 按请求创建sesison 默认为 false,一般如果是调度请求，务必要将这个值设置为true async 请求是不是异步执行 默认为 false callback 如果是异步执行，需要设置回调URL skipInclude 禁止使用include语法 默认false skipAuth 禁止权限验证 默认true skipGrammarValidate 跳过语法验证 默认true includeSchema 返回的结果是否包含单独的schema信息 默认false fetchType take/collect, take在查看表数据的时候非常快 默认collect defaultPathPrefix 所有用户主目录的基础目录 context.__default__include_fetch_url__ Engine获取include脚本的地址 context.__default__console_url__ console地址 context.__default__fileserver_url__ 下载文件服务器地址，一般默认也是console地址 context.__default__fileserver_upload_url__ 上传文件服务器地址，一般默认也是console地址 context.__auth_client__ 数据访问客户端的class类 默认是streaming.dsl.auth.meta.client.MLSQLConsoleClient context.__auth_server_url__ 数据访问验证服务器地址 context.__auth_secret__ engine回访请求服务器的密钥。比如console调用了engine，需要传递这个参数， 然后engine要回调console,那么需要将这个参数带回            该文件修订时间： 2021-01-12 10:27:46 "},"api/code_suggest.html":{"url":"api/code_suggest.html","title":"代码提示接口","keywords":"","body":"代码提示接口 代码提示接口复用了脚本执行接口. 下面是核心参数说明： 参数 说明 示例值 executeMode autoSuggest sql 当前编辑器里的所有内容 lineNum 当前光标所在行，从1开始计数 1 columnNum 当前光标所在列,从1开始计数 1 目前接口已经支持load/select语法的提示了，而且具备非常强的跨语句提示能力。 如果无法提示的，可能会报错，用户只要关注200的返回即可。 下面是一段使用JS调用该接口的例子： export default class CodeIntellegence { static async getSuggestList(sql,lineNum,columnNum){ const restClient = new ActionProxy() const res = await restClient.post(\"/run/script\",{ executeMode: \"autoSuggest\", sql: sql, lineNum: lineNum +1, columnNum: columnNum, isDebug: false, queryType: \"robot\" }) if(res && res.status === 200){ const wordList = res.content return wordList }else { return [] } } }            该文件修订时间： 2021-01-12 10:27:46 "},"resources/liveness.html":{"url":"resources/liveness.html","title":"MLSQL Engine Liveness探针","keywords":"","body":"Liveness探针 MLSQL Engine 2.1.0-SNAPSHOT及以上可用 MLSQL 支持K8s的liveness探针。对应接口为 http://.../health/liveness 如果处于可用状态，返回200,结果如下： // 20201029141025 // http://127.0.0.1:9003/health/liveness { \"status\": \"UP\", \"components\": { \"livenessProbe\": { \"status\": \"UP\" } } } 如果系统不可用，返回500,结果如下： // 20201029141025 // http://127.0.0.1:9003/health/liveness { \"status\": \"DOWN\", \"components\": { \"livenessProbe\": { \"status\": \"DOWN\" } } }            该文件修订时间： 2021-01-12 10:27:47 "},"resources/readiness.html":{"url":"resources/readiness.html","title":"MLSQL Engine Readness探针","keywords":"","body":"Readiness探针 MLSQL Engine 2.1.0-SNAPSHOT及以上可用 MLSQL 支持K8s的Readiness探针。对应接口为 http://..../health/readiness 如果已经初始化完成，处于可用状态，返回200,结果如下： // 20201029141214 // http://127.0.0.1:9003/health/readiness { \"status\": \"IN_SERVICE\", \"components\": { \"readinessProbe\": { \"status\": \"IN_SERVICE\" } } } 如果系统还未初始化完成，返回503,结果如下： // 20201029141214 // http://127.0.0.1:9003/health/readiness { \"status\": \"OUT_OF_SERVICE\", \"components\": { \"readinessProbe\": { \"status\": \"OUT_OF_SERVICE\" } } }            该文件修订时间： 2021-01-12 10:27:47 "},"qa/":{"url":"qa/","title":"常见问题","keywords":"","body":"常见问题 本章节会介绍用户在使用中常见的一些问题。 文件上传失败怎么办 个人主目录是个啥 会话隔离/并发执行/调试执行/定时任务 CDH/HDP 怎么运行MLSQL Engine 排查错误，三个系统的日志你都要看 听说mlsql-cluster暂时不更新了，mlsql-cluster是个啥？ MLSQL K8s部署，镜像环境如何制作 写SQL如何避免拷贝黏贴 如何将SQL封装成命令调用            该文件修订时间： 2021-01-12 10:27:47 "},"qa/upload_file.html":{"url":"qa/upload_file.html","title":"文件上传失败怎么办","keywords":"","body":"文件上传 MLSQL Console文件上传的路口在这里： 通常文件上传会遇到两个问题： 上传失败 上传成功，但找不到文件 上传失败，可以按如下方式检查： 主目录是否存在或者Engine有读写权限么？ 临时目录大小是否超过配额 Engine/Console之间是否确认互通 其中临时目录配额默认是125M，是MLSQL Console用于中转文件的目录。通常用户上传的文件，先要上传到MLSQL Console,然后MLSQL Console再发指令 给Engine,拉取到HDFS目录。 临时目录的文件会保留2个小时。也就是说假设你第一次传了一个120M的文件，接着你再上传需要等两个小时。 你也可以通过修改MLSQL Console启动脚本start.sh 来修改配置： java -cp .:${MLSQL_CONSOLE_JAR} tech.mlsql.MLSQLConsole \\ -mlsql_engine_url ${MLSQL_ENGINE_URL} \\ -my_url ${MY_URL} \\ -user_home ${USER_HOME} \\ -enable_auth_center ${ENABLE_AUTH_CENTER:-false} \\ -single_user_upload_bytes 1073741824 \\ -config ${MLSQL_CONSOLE_CONFIG_FILE} 通过参数 single_user_upload_bytes 我们将用户临时上传目录修改为1G. 如果是上传成功，但是在Console的 FileSystem里没看到，或者通过命令!hdfs -ls /tmp/upload 也没有发现文件。这个时候住哟啊是检查下 主目录 是不是被创建了。            该文件修订时间： 2021-01-12 10:27:47 "},"qa/home.html":{"url":"qa/home.html","title":"个人主目录是个啥","keywords":"","body":"个人主目录 MLSQL Engine每个实例要求有一个主目录，该主目录里会为每个登录的账号设置一个子目录。 主目录通常是你在注册引擎时设置： 我们以test2为例，此时主目录是/data/mlsql/homes（并且假设用户使用了分布式存储HDFS）,那么用户需要手工创建该目录（启动Engine的账号需要能够读写该目录）。 如果有用户A save文件，比如: save overwrite table1 as parquet.`/tmp/table1`; 此时在HDFS实际的存储目录为： /data/mlsql/homes/A/tmp/table1 同理，A用户上传的文件也会存储在 /data/mlsql/homes/A/tmp/upload 中。 尽管实际路径如此，但是在MLSQL中，使用者是无需关心这个前缀的。比如如果你要列出所有上传的文件，可以使用： !hdfs -ls /tmp/upload; 而不是完整路径。            该文件修订时间： 2021-01-12 10:27:47 "},"qa/session_isolated.html":{"url":"qa/session_isolated.html","title":"会话隔离/并发执行/调试执行/定时任务","keywords":"","body":"会话隔离/并发执行/调试执行/定时任务 标题有点长。我们知道，在Console中，我们是可以选中几条语句执行的。并且系统会记住之前执行语句的结果（表名）。 这显然是很方便我们调试的。在Console中，用户之间写的临时表名都是隔离的。 比如 A用户执行了如下语句： select \"a\" as userName as table1; 同时，B用户执行了如下语句： select \"b\" as userName as table1; 尽管他们用了相同的表名table1,但是并不会互相影响。 但是，如果你写了很多定时执行的脚本，亦或是对外提供API服务，这个时候一般都是适用同一个用户名，这个时候就会使得脚本之间的表名互相影响。 MLSQL Engine的HTTP接口提供了两个参数控制隔离： 如果sessionPerUser设置为true 按用户进行隔离 如果sessionPerRequest设置为true,那么会按请求隔离。 所以如果你使用定时任务那么请将这两个参数都设置为true。更多参数请参看：MLSQL Engine Rest接口详解            该文件修订时间： 2021-01-12 10:27:47 "},"qa/cdh_hdp.html":{"url":"qa/cdh_hdp.html","title":"CDH/HDP 怎么运行MLSQL Engine","keywords":"","body":"CDH/HDP 怎么运行MLSQL Engine CDH/HDP默认都会自带一个Spark发行版，但这个自带的要么太旧，要么太新，甚至还有魔改。 通常，我们建议大家直接使用社区的版本。 那么怎么将Spark社区版运行在CDH/HDP上呢？版本有什么要求么？ 选择Spark社区版时，你要考虑如下几个需求： 你CDH/HDP的Hadoop版本是多少？ 2.7还是3.x.根据对应的版本选择合适的Spark社区版。 MLSQL目前测试过2.4.3/3.0.0两个版本，所以最好你的Spark版本也要是这两个版本。 这两个因素决定了你现在的Spark社区发行版。 你可以在任何一台和CDH/DP集群网络互通的机器上，下载Spark发行版和MLSQL发行版。然后，你唯一需要做的就是： 将Hadoop的配置文件，如core-site.xml,hdfs.xml,hive.xml等文件放到SPARK_HOME/conf目录里 有了这些文件，Spark才知道如何连接Hadoop集群。 接着，就可以参考安装部署文档去部署了。 可能存在的问题 CDH/HDP 根据版本不同，会在Yarn节点上默默的带上一些默认Jar包，可能会发生Jar包冲突。 对于HDP大家可以参考这篇文章Ambari hdp Spark多版本兼容获得一些思路. 对于Spark运行在如 CDH,比如比较早的版本如5.13.1，可能需要解决一些问题，大家可以根据实际的错误按图索骥： HDFS Kerberos token过期问题 错误信息大概是这样： spark hadoop.security.token.SecretManager token can't be found in cache 原因是打包使用hadoop–2.7官方是没有解决这方面问题的(线上也是使用cdh的hadoop) 解决方法：打包指定cdh版本的hadoop依赖，指定-Dhadoop.version=2.6.0-cdh5.13.1 HDFS native snappy library not available问题 错误信息大概是这样： native snappy library not available: this version of libhadoop was built without snappy support. 解决方法：在SPARK_HOME/conf下的spark-env.sh 中 指定JAVA_LIBRARY_PATH为native so文件路径 export JAVA_LIBRARY_PATH=$JAVA_LIBRARY_PATH:\\ /opt/cloudera/parcels/CDH/lib/hadoop/lib/native export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:\\ /opt/cloudera/parcels/GPLEXTRAS/lib/hadoop/lib/native:\\ /opt/cloudera/parcels/CDH/lib/hadoop/lib/native export SPARK_DIST_CLASSPATH=/etc/hadoop/conf:\\ /opt/cloudera/parcels/CDH/lib/hadoop/lib/*:\\ /opt/cloudera/parcels/CDH/lib/hadoop/lib/native/*:\\ /opt/cloudera/parcels/GPLEXTRAS/lib/hadoop/lib/*:: executor端,启动脚本添加java.library.path为native so文件路径 ENV SPARK_EXECUTOR_NATIVE_JAVA_OPTS=\"-Djava.library.path=/opt/cloudera/parcels/CDH/lib/hadoop/lib/native/\" executor) shift 1 CMD=( ${JAVA_HOME}/bin/java \"${SPARK_EXECUTOR_NATIVE_JAVA_OPTS[@]}\" \"${SPARK_EXECUTOR_JAVA_OPTS[@]}\" -Xms$SPARK_EXECUTOR_MEMORY -Xmx$SPARK_EXECUTOR_MEMORY -cp \"$SPARK_CLASSPATH:$SPARK_DIST_CLASSPATH\" org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url $SPARK_DRIVER_URL --executor-id $SPARK_EXECUTOR_ID --cores $SPARK_EXECUTOR_CORES --app-id $SPARK_APPLICATION_ID --hostname $SPARK_EXECUTOR_POD_IP ) ;; https访问异常 错误信息大概是这样：PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException 原因是：访问https服务需要指定证书 解决方法： 将安全证书安装后， 添加至本地Jre security中， cd /Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/security 目录下执行: keytool -import -v -alias xxxxx -keystore \"/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/security/cacerts\" -file \"/xxx/xxxx/xxxxx.cer\" Spark3和hadoop兼容问题 Spark 3.0 官方默认支持的Hadoop最低版本为2.7, Hive最低版本为 1.2。我们平台使用的CDH 5.13,对应的版本分别为hadoop-2.6.0, hive-1.1.0。 所以在编译yarn模块会报错： 具体是/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala的 298行logAggregationContext.setRolledLogsIncludePattern(includePattern) 300行logAggregationContext.setRolledLogsExcludePattern(excludePattern)问题 由于这两个方法是hadoop2.6.4添加的，如果你的hadoop版本低于2.6.4，那么编译就会报错。 解决方法：参考 https://github.com/apache/spark/pull/16884/files Spark 3.0 整合hive异常(高版本hive忽略) 错误信息大概是这样： hive.ql.metadata.HiveException: Unable to fetch table Invalid method name: get_table_req 原因是spark3源码默认使用2.3.7版本的hive，里面的getTable方法调用了get_table_req,而hive客户端的版本 解决方法：自己手动编译Spark，指定hive的版本为1.2.1，打包编译Spark的时候指定-Phive-1.2            该文件修订时间： 2021-01-12 10:27:47 "},"qa/debug_log.html":{"url":"qa/debug_log.html","title":"排查错误，三个系统的日志你都要看","keywords":"","body":"排查错误，三个系统的日志你都要看 通常第一次安装Console/Engine后，然后欢欢喜喜的执行示例，然后发现Console一直在转圈圈，界面上没有显示具体错误。 这个时候别慌，我们看看怎么排查问题出在哪。而且掌握这篇文章的技巧后，也特别方便在MLSQL群里提问呢，帮助我们更好的帮到您。 在整个过程中，有三个系统的日志可以看： Console日志 Engine日志 Spark UI以及里面的日志 Console日志是在Console的安装目录里的logs目录下。 Engine的日志是在Engine安装目录下的logs目录下，通常名称是mlsql_engine.log。 (如果你发现没有，可以将这个log4j.properties 拷贝到SPARK_HOME/conf目录里)。 说说他们之间的关系 我们可以在Console里配置Engine地址，在配置Engine地址的过程中，还需要配置诸如consoleUrl, fileServerUrl ,authServerUrl等。 Engine会通过consoleUrl访问Console,比如通知Console任务执行状态等。fileServerUrl ,authServerUrl 则分别和文件上传，权限控制有关。默认都填写 consoleUrl的地址即可，因为Console实现了这些功能的接口。 所以实际上，Console和Engine是要互通的，也就是Engine需要访问Console，Console也需要访问Engine。            该文件修订时间： 2021-01-12 10:27:47 "},"qa/mlsql_cluster.html":{"url":"qa/mlsql_cluster.html","title":"听说mlsql-cluster暂时不更新了，mlsql-cluster是个啥？","keywords":"","body":"听说mlsql-cluster暂时不更新了，mlsql-cluster是个啥？ MLSQL Cluster其实蛮有误导性的。MLSQL Cluster其实就是一个代理，可以管理多个MLSQL Engine实例。 典型的功能如实现负载均衡。当然了，MLSQL Cluster还有很多负载均衡策略可选，比如尽量将请求转发给最清闲的Engine实例， 亦或是将请求发送给绝对资源剩余最多的实例，亦或是发送给所有引擎。 同时，MLSQL Cluster还能实现一些读写分离的工作。尽管他功能强大，但是考虑到部署的复杂度，我们还是暂时停止了对它的更新。 Console目前已经能管理多个Engine，并且在使用时选择需要的引擎，大部分场景是已经满足的。            该文件修订时间： 2021-01-12 10:27:47 "},"qa/mlsql-docker-image.html":{"url":"qa/mlsql-docker-image.html","title":"MLSQL K8s部署，镜像环境如何制作","keywords":"","body":"MLSQL K8s部署，镜像环境如何制作 通常我会制作两个镜像： 基础镜像，该镜像需要满足如下几点： JDK Conda(Python相关环境) Spark发型包 我们以Spark 3.0为例，假设我们镜像名称叫： spark:v3.0.0-hadoop3.2 注： 该镜像文件需要在Spark发行版根目录执行。 ARG java_image_tag=14.0-jdk-slim FROM openjdk:${java_image_tag} ARG spark_uid=185 RUN set -ex && \\ apt-get update && \\ ln -s /lib /lib64 && \\ apt install -y bash tini libc6 libpam-modules krb5-user libnss3 && \\ mkdir -p /opt/spark && \\ mkdir -p /opt/spark/examples && \\ mkdir -p /opt/spark/work-dir && \\ touch /opt/spark/RELEASE && \\ rm /bin/sh && \\ ln -sv /bin/bash /bin/sh && \\ echo \"auth required pam_wheel.so use_uid\" >> /etc/pam.d/su && \\ chgrp root /etc/passwd && chmod ug+rw /etc/passwd && \\ rm -rf /var/cache/apt/* RUN apt-get update \\ && apt-get install -y \\ git \\ wget \\ cmake \\ build-essential \\ curl \\ unzip \\ libgtk2.0-dev \\ zlib1g-dev \\ libgl1-mesa-dev \\ && apt-get clean \\ && echo 'export PATH=/opt/conda/bin:$PATH' > /etc/profile.d/conda.sh \\ && wget \\ --quiet \"https://repo.anaconda.com/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh\" \\ -O /tmp/anaconda.sh \\ && /bin/bash /tmp/anaconda.sh -b -p /opt/conda \\ && rm /tmp/anaconda.sh \\ && /opt/conda/bin/conda install -y \\ libgcc python=3.6.9 \\ && /opt/conda/bin/conda clean -y --all \\ && /opt/conda/bin/pip install \\ flatbuffers \\ cython==0.29.0 \\ numpy==1.15.4 RUN /opt/conda/bin/conda create --name dev python=3.6.9 -y \\ && source /opt/conda/bin/activate dev \\ && pip install pyarrow==0.10.0 \\ && pip install ray==0.8.0 \\ && pip install aiohttp psutil setproctitle grpcio pandas xlsxwriter watchdog requests click uuid sfcli pyjava COPY jars /opt/spark/jars COPY bin /opt/spark/bin COPY sbin /opt/spark/sbin COPY kubernetes/dockerfiles/spark/entrypoint.sh /opt/ COPY examples /opt/spark/examples COPY kubernetes/tests /opt/spark/tests COPY data /opt/spark/data ENV SPARK_HOME /opt/spark WORKDIR /opt/spark/work-dir RUN chmod g+w /opt/spark/work-dir ENTRYPOINT [ \"/opt/entrypoint.sh\" ] USER ${spark_uid} 接着，我们会基于该镜像，打包MLSQL相关的依赖： FROM spark:v3.0.0-hadoop3.2 COPY streamingpro-mlsql-spark_3.0_2.12-2.0.1-SNAPSHOT.jar /opt/spark/work-dir/ WORKDIR /opt/spark/work-dir ENTRYPOINT [ \"/opt/entrypoint.sh\" ] 通常基础镜像不太用变化。MLSQL倒是可能会升级较为频繁。            该文件修订时间： 2021-01-12 10:27:47 "},"qa/load_jdbc.html":{"url":"qa/load_jdbc.html","title":"加载JDBC(如MySQL，Oracle)数据常见困惑","keywords":"","body":"加载JDBC(如MySQL，Oracle)数据常见困惑 MLSQL可以使用Load语法加载支持JDBC协议的数据源。比如MySQL,Oracle. 通常使用方式如下(例子来自官方文档)： set user=\"root\"; connect jdbc where url=\"jdbc:mysql://127.0.0.1:3306/wow?characterEncoding=utf8&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false\" and driver=\"com.mysql.jdbc.Driver\" and user=\"${user}\" and password=\"${password}\" as db_1; load jdbc.`db_1.table1` as table1; select * from table1 as output; 其中，connect看着好像是去链接一个数据库，但其实并不是，该语句并不会执行真正的连接动作， 而是仅仅记下了数据库的连接信息，并且给这个真实的数据库（在上面的例子中是DB wow）取了一个别名db_1. 之后，如果你需要加载wow库的表，就可以直接使用db_1引用了。 load会把数据都加载到MLSQL引擎的内存里么？ 答案是不会。引擎会批量到MySQL拉取数据进行计算。同一时刻，只有一部分数据在引擎内存里。 load的时候可以加载过滤条件么 可以，但是没有必要。用户可以把条件直接在后续接select语句中，select 语句里的where条件会被下推给存储做过滤，避免大量数据传输。 count非常慢，怎么办？ 比如用户执行如下语句想查看下表的数据条数,如果表比较大，可能会非常慢，甚至有可能导致Engine有节点挂掉 load jdbc.`db_1.tblname` as tblname; select count(*) from tblname as newtbl; 原因是引擎需要拉取全量数据（批量拉取，并不是全部加载到内存），然后计数，而且默认是单线程，一般数据库对全表扫描都比较慢，所以 没有where条件的count可能会非常慢，甚至跑不出来。 如果用户仅仅是为了看下表的大小，推荐用directQuery模式，directQuery会把查询直接发给数据库执行，然后把得到的计算结果返回给引擎,所以非常快。 具体操作方式如下： load jdbc.`db_1.tblname` where directQuery=''' select count(*) from tblname ''' as newtbl; 在select语句中加了where条件也很慢（甚至引擎挂掉） 虽然你加了where条件，但是过滤效果可能并不好，引擎仍然需要拉取大量的数据进行计算，引擎默认是单线程的。我们可以配置多线程的方式去数据库拉取数据，可以避免单线程僵死。 核心参数有如下： partitionColumn 按哪个列进行分区 lowerBound, upperBound, 分区字段的最小值，最大值（可以使用directQuery获取） numPartitions 分区数目。一般8个线程比较合适。 能够进行分区的字段要求是数字类型，推荐使用自增id字段。 多线程拉取还是慢，有办法进一步加速么 你可以通过上面的方式将数据保存到delta/hive中，然后再使用。这样可以一次同步，多次使用。如果你没办法接受延迟，那么可以使用MLSQL把MySQL实时同步到 Delta中，可以参考MySQL Binlog同步 有么有深入原理的文章？ 可以参考这篇MLSQL加载JDBC数据源深度剖析 结束语 对无论是JDBC类数据源，还是Hive等传统数仓类数据源，MLSQL未来会统一提供索引服务来进行加速，敬请期待。            该文件修订时间： 2021-01-12 10:27:47 "},"dev_guide/engine/":{"url":"dev_guide/engine/","title":"MLSQL Engine开发环境设置","keywords":"","body":"MLSQL Engine开发环境设置 MLSQL主要使用Java/Scala开发，所以我们推荐使用Idea IntelliJ 社区版进行开发。 MLSQL Engine内部的执行引擎是Spark，尽管在设计的时候可以使用其他引擎替换（如Flink），但目前Spark为其唯一实现。 通常而言，MLSQL会支持Spark最近的两个大版本,截止到本文写作时间，我们支持2.4.x,3.0.x。 经过测试的为准确版本号为2.4.3/3.0.0/3.0.1。 因为要同时支持两个版本，但是Spark不同版本的API往往发生变化，我们使用Maven的profile机制切换不同的模块支持。 红色框选部分，展示了我们对spark 2.3,2.4,3.0进行了适配。实际上2.3已经不再维护了。在未来3.1版本适配完成后，会删除2.3这个适配。 用户可以通过git 下载源码： git clone https://github.com/allwefantasy/mlsql.git . 这是一个Maven项目，用户只要是有用idea 按Maven项目打开即可。在后续章节里，我们会详解介绍如何基于Spark 2.4.3 和 3.0.0开发 MLSQL .            该文件修订时间： 2021-01-12 10:27:46 "},"dev_guide/engine/spark_2_4_3.html":{"url":"dev_guide/engine/spark_2_4_3.html","title":"Spark 2.4.3开发环境","keywords":"","body":"Spark 2.4.3开发环境 Profile设置 项目导入后，核心在于Profile的设置。在右侧Maven选项里，对如下Profile 进行勾选即可： 勾选完成后，项目结构如下： 在IDE中启动调试 streamingpro-mlsql 模块是主模块，他依赖其他所有模块。实际上，external里部分模块对streamingpro-mlsq模块有循环依赖的问题，我们通过在external里设置为Provided 来解决这个问题。 所以要在IDE中直接进行启动和调试，只要在streamingpro-mlsql新建一个启动类即可。下面是我使用的一个启动类： package streaming.core /** * 2019-03-20 WilliamZhu(allwefantasy@gmail.com) */ object WilliamLocalSparkServiceApp { def main(args: Array[String]): Unit = { StreamingApp.main(Array( \"-streaming.master\", \"local[*]\", \"-streaming.name\", \"god\", \"-streaming.rest\", \"true\", \"-streaming.thrift\", \"false\", \"-streaming.platform\", \"spark\", \"-spark.mlsql.enable.runtime.directQuery.auth\", \"true\", // \"-streaming.ps.cluster.enable\",\"false\", \"-streaming.enableHiveSupport\",\"false\", \"-spark.mlsql.datalake.overwrite.hive\", \"true\", \"-spark.mlsql.auth.access_token\", \"mlsql\", //\"-spark.mlsql.enable.max.result.limit\", \"true\", //\"-spark.mlsql.restful.api.max.result.size\", \"7\", // \"-spark.mlsql.enable.datasource.rewrite\", \"true\", // \"-spark.mlsql.datasource.rewrite.implClass\", \"streaming.core.datasource.impl.TestRewrite\", //\"-streaming.job.file.path\", \"classpath:///test/init.json\", \"-streaming.spark.service\", \"true\", \"-streaming.job.cancel\", \"true\", \"-streaming.datalake.path\", \"/data/mlsql/datalake\", \"-streaming.plugin.clzznames\",\"tech.mlsql.plugins.ds.MLSQLExcelApp\", // scheduler \"-streaming.workAs.schedulerService\", \"false\", \"-streaming.workAs.schedulerService.consoleUrl\", \"http://127.0.0.1:9002\", \"-streaming.workAs.schedulerService.consoleToken\", \"mlsql\", // \"-spark.sql.hive.thriftServer.singleSession\", \"true\", \"-streaming.rest.intercept.clzz\", \"streaming.rest.ExampleRestInterceptor\", // \"-streaming.deploy.rest.api\", \"true\", \"-spark.driver.maxResultSize\", \"2g\", \"-spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\", // \"-spark.sql.codegen.wholeStage\", \"true\", \"-spark.ui.allowFramingFrom\",\"*\", \"-spark.kryoserializer.buffer.max\", \"2000m\", \"-streaming.driver.port\", \"9003\" // \"-spark.files.maxPartitionBytes\", \"10485760\" //meta store // \"-streaming.metastore.db.type\", \"mysql\", // \"-streaming.metastore.db.name\", \"app_runtime_full\", // \"-streaming.metastore.db.config.path\", \"./__mlsql__/db.yml\" // \"-spark.sql.shuffle.partitions\", \"1\", // \"-spark.hadoop.mapreduce.job.run-local\", \"true\" //\"-streaming.sql.out.path\",\"file:///tmp/test/pdate=20160809\" //\"-streaming.jobs\",\"idf-compute\" //\"-streaming.driver.port\", \"9005\" //\"-streaming.zk.servers\", \"127.0.0.1\", //\"-streaming.zk.conf_root_dir\", \"/streamingpro/jack\" )) } } 用户可以随时自己设置很多参数。点击右键即可Run/Debug。 注意，在IDE调试，并不需要Spark环境。 自助构建发行版 用户可以拷贝黏贴（修改）如下脚本完成发行版的构建。 export LC_ALL=zh_CN.UTF-8 export LANG=zh_CN.UTF-8 # 依赖的Spark版本 export MLSQL_SPARK_VERSION=2.4 # scala版本 export SCALA_VERSION=2.11 # 项目根目录 export MLSQL_M_HOME=/Users/allwefantasy/CSDNWorkSpace/streamingpro-spark-2.4.x # 版本号，保持和maven一致 export VERSION=\"2.1.0-SNAPSHOT\" # 构建 export TEMP_DIR=/tmp/mlsql-engine_${MLSQL_SPARK_VERSION}-${VERSION} export UPLOAD=false export ENABLE_CHINESE_ANALYZER=false ./dev/package.sh # 在tmp目录生成发行包 rm -rf ${TEMP_DIR} mkdir -p ${TEMP_DIR}/libs cp streamingpro-mlsql/target/streamingpro-mlsql-spark_${MLSQL_SPARK_VERSION}_${SCALA_VERSION}-${VERSION}.jar ${TEMP_DIR}/libs if [[ \"${ENABLE_CHINESE_ANALYZER}\" == \"true\" ]]; then echo \"cp -r lib/*.jar ${TEMP_DIR}/libs/\" cp -r lib/*.jar ${TEMP_DIR}/libs/ fi ## 生成启动脚本等 cp dev/start-local.sh ${TEMP_DIR} cat \"${TEMP_DIR}/start-default.sh\" if [[ -z \"\\${SPARK_HOME}\" ]]; then echo \"===SPARK_HOME is required===\" exit 1 fi SELF=\\$(cd \\$(dirname \\$0) && pwd) cd \\$SELF ./start-local.sh EOF cat \"${TEMP_DIR}/README.md\" 1. Configure env SPARK_HOME 2. Run ./start-default.sh EOF chmod u+x ${TEMP_DIR}/*.sh cd ${TEMP_DIR} && cd .. tar czvf mlsql-engine_${MLSQL_SPARK_VERSION}-${VERSION}.tar.gz mlsql-engine_${MLSQL_SPARK_VERSION}-${VERSION}            该文件修订时间： 2021-01-12 10:27:46 "},"dev_guide/engine/spark_3_0_0.html":{"url":"dev_guide/engine/spark_3_0_0.html","title":"Spark 3.0.0开发环境","keywords":"","body":"Spark 3.0.0开发环境 Profile设置 项目导入后，核心在于Profile的设置。在右侧Maven选项里，对如下Profile 进行勾选即可： 勾选完成后，项目结构如下： 修改配置 因为Maven自身动态不足，以及IDE有可能对Maven支持的还是不够好，所以在做完Profile选择后，用户需要在项目根目录下执行如下两条指令： ./dev/change-scala-version.sh 2.12 python ./dev/python/convert_pom.py 第一个是修改scala版本为 2.12. 第二个是修改默认properties. 在IDE中启动调试 streamingpro-mlsql 模块是主模块，他依赖其他所有模块。实际上，external里部分模块对streamingpro-mlsq模块有循环依赖的问题，我们通过在external里设置为Provided 来解决这个问题。 所以要在IDE中直接进行启动和调试，只要在streamingpro-mlsql新建一个启动类即可。下面是我使用的一个启动类： package streaming.core /** * 2019-03-20 WilliamZhu(allwefantasy@gmail.com) */ object WilliamLocalSparkServiceApp { def main(args: Array[String]): Unit = { StreamingApp.main(Array( \"-streaming.master\", \"local[*]\", \"-streaming.name\", \"god\", \"-streaming.rest\", \"true\", \"-streaming.thrift\", \"false\", \"-streaming.platform\", \"spark\", \"-spark.mlsql.enable.runtime.directQuery.auth\", \"true\", // \"-streaming.ps.cluster.enable\",\"false\", \"-streaming.enableHiveSupport\",\"false\", \"-spark.mlsql.datalake.overwrite.hive\", \"true\", \"-spark.mlsql.auth.access_token\", \"mlsql\", //\"-spark.mlsql.enable.max.result.limit\", \"true\", //\"-spark.mlsql.restful.api.max.result.size\", \"7\", // \"-spark.mlsql.enable.datasource.rewrite\", \"true\", // \"-spark.mlsql.datasource.rewrite.implClass\", \"streaming.core.datasource.impl.TestRewrite\", //\"-streaming.job.file.path\", \"classpath:///test/init.json\", \"-streaming.spark.service\", \"true\", \"-streaming.job.cancel\", \"true\", \"-streaming.datalake.path\", \"/data/mlsql/datalake\", \"-streaming.plugin.clzznames\",\"tech.mlsql.plugins.ds.MLSQLExcelApp\", // scheduler \"-streaming.workAs.schedulerService\", \"false\", \"-streaming.workAs.schedulerService.consoleUrl\", \"http://127.0.0.1:9002\", \"-streaming.workAs.schedulerService.consoleToken\", \"mlsql\", // \"-spark.sql.hive.thriftServer.singleSession\", \"true\", \"-streaming.rest.intercept.clzz\", \"streaming.rest.ExampleRestInterceptor\", // \"-streaming.deploy.rest.api\", \"true\", \"-spark.driver.maxResultSize\", \"2g\", \"-spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\", // \"-spark.sql.codegen.wholeStage\", \"true\", \"-spark.ui.allowFramingFrom\",\"*\", \"-spark.kryoserializer.buffer.max\", \"2000m\", \"-streaming.driver.port\", \"9003\" // \"-spark.files.maxPartitionBytes\", \"10485760\" //meta store // \"-streaming.metastore.db.type\", \"mysql\", // \"-streaming.metastore.db.name\", \"app_runtime_full\", // \"-streaming.metastore.db.config.path\", \"./__mlsql__/db.yml\" // \"-spark.sql.shuffle.partitions\", \"1\", // \"-spark.hadoop.mapreduce.job.run-local\", \"true\" //\"-streaming.sql.out.path\",\"file:///tmp/test/pdate=20160809\" //\"-streaming.jobs\",\"idf-compute\" //\"-streaming.driver.port\", \"9005\" //\"-streaming.zk.servers\", \"127.0.0.1\", //\"-streaming.zk.conf_root_dir\", \"/streamingpro/jack\" )) } } 用户可以随时自己设置很多参数。点击右键即可Run/Debug。 注意，在IDE调试，并不需要Spark环境。 自助构建发行版 用户可以拷贝黏贴（修改）如下脚本完成发行版的构建。 export LC_ALL=zh_CN.UTF-8 export LANG=zh_CN.UTF-8 # 依赖的Spark版本 export MLSQL_SPARK_VERSION=3.0 # scala版本 export SCALA_VERSION=2.12 # 项目根目录 export MLSQL_M_HOME=/Users/allwefantasy/CSDNWorkSpace/mlsql-3.0 # 版本号，保持和maven一致 export VERSION=\"2.1.0-SNAPSHOT\" # 构建 export TEMP_DIR=/tmp/mlsql-engine_${MLSQL_SPARK_VERSION}-${VERSION} export UPLOAD=false export ENABLE_CHINESE_ANALYZER=false ./dev/package.sh # 在tmp目录生成发行包 rm -rf ${TEMP_DIR} mkdir -p ${TEMP_DIR}/libs cp streamingpro-mlsql/target/streamingpro-mlsql-spark_${MLSQL_SPARK_VERSION}_${SCALA_VERSION}-${VERSION}.jar ${TEMP_DIR}/libs if [[ \"${ENABLE_CHINESE_ANALYZER}\" == \"true\" ]]; then echo \"cp -r lib/*.jar ${TEMP_DIR}/libs/\" cp -r lib/*.jar ${TEMP_DIR}/libs/ fi ## 生成启动脚本等 cp dev/start-local.sh ${TEMP_DIR} cat \"${TEMP_DIR}/start-default.sh\" if [[ -z \"\\${SPARK_HOME}\" ]]; then echo \"===SPARK_HOME is required===\" exit 1 fi SELF=\\$(cd \\$(dirname \\$0) && pwd) cd \\$SELF ./start-local.sh EOF cat \"${TEMP_DIR}/README.md\" 1. Configure env SPARK_HOME 2. Run ./start-default.sh EOF chmod u+x ${TEMP_DIR}/*.sh cd ${TEMP_DIR} && cd .. tar czvf mlsql-engine_${MLSQL_SPARK_VERSION}-${VERSION}.tar.gz mlsql-engine_${MLSQL_SPARK_VERSION}-${VERSION}            该文件修订时间： 2021-01-12 10:27:46 "},"dev_guide/engine/plugin/ET_README.html":{"url":"dev_guide/engine/plugin/ET_README.html","title":"ET插件开发","keywords":"","body":"ET插件开发 ET 插件是MLSQL里最重要的一种插件，能完成非常多的复杂任务，包括： 无法用SQL实现的特定的数据处理 实现各种可复用的复杂的算法模型以及特征工程工具 提供各种便利工具，比如发送邮件，生成图片等各种必须的工具 比如,计算父子关系 就没办法很好的用SQL表达， 这个时候我们就可以用ET来实现： -- 准备模拟数据 set jsonStr = ''' {\"id\":0,\"parentId\":null} {\"id\":1,\"parentId\":null} {\"id\":2,\"parentId\":1} {\"id\":3,\"parentId\":3} {\"id\":7,\"parentId\":0} {\"id\":199,\"parentId\":1} {\"id\":200,\"parentId\":199} {\"id\":201,\"parentId\":199} '''; load jsonStr.`jsonStr` as data; -- 对模拟数据开始构建父子关系 run data as TreeBuildExt.`` where idCol=\"id\" and parentIdCol=\"parentId\" and treeType=\"nodeTreePerRow\" as result; 在MLSQL中，所有以run/train/predict开头的语句，都是通过ET插件实现的。在上边的示例中，其语句的内在含义为： 对表data进行处理，处理的模块是TreeBuildExt,where 条件后面是该模块处理data数据需要配置的一些参数，最后模块处理完成后得到一张新表，叫result. 可见 TreeBuildExt本质实现了从表到表的数据转换。 对于算法而言，例子就更多了，几乎所有高阶内置算法都是通过ET插件实现的，譬如随机森林. 发送邮件的例子： set EMAIL_TITLE = \"这是邮件标题\"; set EMAIL_BODY = `select download_url from t1` options type = \"sql\"; set EMAIL_TO = \"\"; select \"${EMAIL_BODY}\" as content as data; run data as SendMessage.`` where method=\"mail\" and to = \"${EMAIL_TO}\" and subject = \"${EMAIL_TITLE}\" and smtpHost = \"xxxxxxxx\"; MLSQL还允许将这些模块封装成命令使用，比如常见的!hdfs命令其实就是一个HDFS ET模块。只是使用命令行代替了run语法而已。 在下面的章节中，我们会分别介绍如何开发ET组件以及如何将ET组件封装成命令行使用。            该文件修订时间： 2021-01-12 10:27:46 "},"dev_guide/engine/plugin/et.html":{"url":"dev_guide/engine/plugin/et.html","title":"ET插件开发","keywords":"","body":"ET插件开发 开发ET插件有三种方式： 直接修改MLSQL源码 独立成模块作为内置插件使用 独立成项目作为外置插件使用 直接修改MLSQL源码 让我们先来实现一个没啥用的插件，叫 EmptyTable,具体用法如下： select 1 as col as table1; run table1 as EmptyTable.`` as outputTable; EmptyTable啥也不干，返回一个空表,在上面的例子中，空表的名称为outputTable. 具体实现代码如下： package tech.mlsql.plugins.ets import org.apache.spark.ml.util.Identifiable import org.apache.spark.sql.expressions.UserDefinedFunction import org.apache.spark.sql.{DataFrame, SparkSession} import streaming.dsl.mmlib.SQLAlg import streaming.dsl.mmlib.algs.param.WowParams class EmptyTable(override val uid: String) extends SQLAlg with WowParams { def this() = this(Identifiable.randomUID(\"tech.mlsql.plugins.ets.EmptyTable\")) override def train(df: DataFrame, path: String, params: Map[String, String]): DataFrame = { df.sparkSession.emptyDataFrame } override def batchPredict(df: DataFrame, path: String, params: Map[String, String]): DataFrame = train(df, path, params) override def load(sparkSession: SparkSession, path: String, params: Map[String, String]): Any = ??? override def predict(sparkSession: SparkSession, _model: Any, name: String, params: Map[String, String]): UserDefinedFunction = ??? } 可以看到，EmptyTable只需要继承SQLAlg和混入WowParams即可（实际上还有很多其他的接口可以混入，比如权限接口等等）。然后覆盖实现相应的方法即可。在MLSQL里，train语法对应的是ET里的train方法。 run/predict对应的是batchPredict方法。register则对应的是load/predict 方法。 我们希望在train/run语法里都可以用，只需要实现train/batchPredict方法即可。 train方法签名也比较简单，给定你一个dataframe(在示例中是table1)，以及一些参数（where条件里的参数），返回一个新的dataframe接口。 -- table1 就是train的第一个参数df run table1 as EmptyTable.`` -- where 在train方法里可以通过params拿到 where ... -- outputTable 就是train的返回值 as outputTable; 到目前为止，我们就实现了一个没啥用的ET插件了。那么如何注册到MLSQL引擎中呢？如果是作为内置插件，我们只要添加如下一行代码到tech.mlsql.ets.register.ETRegister即可： register(\"EmptyTable\", \"tech.mlsql.ets.EmptyTable\") 现在，你启动IDE，就可以使用这个模块了。 独立成模块作为内置插件使用 如果你希望这个插件是一个独立的模块，并且内置在MLSQL中，那么你需要在external目录下新建一个模块，我们已经非常多的例子了。 除了上面这个文件以外，你还需要提供一个EmptyTableApp的类（类名随便取）,内容如下： package tech.mlsql.plugins.ets import tech.mlsql.dsl.CommandCollection import tech.mlsql.ets.register.ETRegister import tech.mlsql.version.VersionCompatibility /** * 6/8/2020 WilliamZhu(allwefantasy@gmail.com) */ class EmptyTableApp extends tech.mlsql.app.App with VersionCompatibility { override def run(args: Seq[String]): Unit = { //注册ET组件 ETRegister.register(\"EmptyTable\", classOf[EmptyTable].getName) } override def supportedVersions: Seq[String] = Seq(\"1.5.0-SNAPSHOT\", \"1.5.0\", \"1.6.0-SNAPSHOT\", \"1.6.0\") } object EmptyTableApp { } 然后在tech.mlsql.runtime.PluginHook 添加该内置插件： object PluginHook extends Logging { private val apps = List( \"tech.mlsql.plugins.app.pythoncontroller.PythonApp\", \"tech.mlsql.plugins.mlsql_watcher.MLSQLWatcher\", \"tech.mlsql.plugins.sql.profiler.ProfilerApp\", \"tech.mlsql.autosuggest.app.MLSQLAutoSuggestApp\", \"tech.mlsql.plugins.ets.ETApp\", \"tech.mlsql.plugins.healthy.App\", \"tech.mlsql.plugins.ets.EmptyTableApp\" ) 当然，你还需要在streamingpro-mlsql 添加该该模块依赖。通常添加在profile/streamingpro-spark-2.4.0-adaptor 和profile/streamingpro-spark-3.0.0-adaptor 中都要添加。如果你这个模块只兼容其中一个，添加一个即可。 作为外置插件使用 如果你想作为外置插件使用，也就是单独成一个项目开发和维护，可以参考项目 :https://github.com/allwefantasy/mlsql-plugins 模式和内置插件一样，然后打成jar包，使用离线安装的方式安装 MLSQL 外置插件可以动态安装，但是如果要更新，则需要重启服务。            该文件修订时间： 2021-01-12 10:27:46 "},"dev_guide/engine/plugin/et_command.html":{"url":"dev_guide/engine/plugin/et_command.html","title":"命令行开发","keywords":"","body":"命令行开发 在上一篇文章里，我们开发了一个没啥用的EmptyTable,正常使用是这样的： -- table1 就是train的第一个参数df run table1 as EmptyTable.`` -- where 在train方法里可以通过params拿到 where ... -- outputTable 就是train的返回值 as outputTable; 如果我希望像下面这么使用怎么办？ !emptyTable _ -i table1 -o outputTable; 在EmptyApp里加上一句话即可： package tech.mlsql.plugins.ets import tech.mlsql.dsl.CommandCollection import tech.mlsql.ets.register.ETRegister import tech.mlsql.version.VersionCompatibility /** * 6/8/2020 WilliamZhu(allwefantasy@gmail.com) */ class EmptyTableApp extends tech.mlsql.app.App with VersionCompatibility { override def run(args: Seq[String]): Unit = { //注册ET组件 ETRegister.register(\"EmptyTable\", classOf[EmptyTable].getName) } //注册命令,注意，语句最后没有分号 CommandCollection.refreshCommandMapping(Map(\"saveFile\" -> \"\"\" |run ${i} as EmptyTable.`` as ${o}\" |\"\"\".stripMargin)) override def supportedVersions: Seq[String] = Seq(\"1.5.0-SNAPSHOT\", \"1.5.0\", \"1.6.0-SNAPSHOT\", \"1.6.0\") } object EmptyTableApp { } 这个时候，你既可以用run语法，也可以用命令行了。MLSQL还支持比较复杂的脚本化方式。前面的例子是使用命名参数，用户也可以使用占位符： run {0} as EmptyTable.`` as {-1:next(named,uuid())}\" 这个时候语法是这样的： !emptyTable table1 named outputTable; 其中{0}表示第一个参数。 {-1}表示不使用占位，而是使用匹配，匹配规则是named字符串后面的值，在这里是outputTable,如果没有则使用uuid()函数随机生成一个。 如果不想事先确定用户会填写什么参数，可以这么写： run command as EmptyTable.`` where parameters='''{:all}\" 不过需要修改下获取参数的代码，你可以在EmptyTable插件中通过如下方式获得一个字符串数组： val command = JSONTool.parseJson[List[String]](params(\"parameters\")) 然后自己匹配命令。            该文件修订时间： 2021-01-12 10:27:46 "},"extra/commands.html":{"url":"extra/commands.html","title":"常见宏命令","keywords":"","body":"常见宏命令 MLSQL内置了非常多的宏命令，可以帮助用户实现更好的交互。 !show 该命令可以展示系统很多信息。 查看当前引擎版本： !show version; 显示show支持的所有子命令： !show commands; 列出所有的表： !show tables; 从指定db罗列所有表 !show tables from [DB名称]; 列出所有当前正在运行的任务： !show jobs; 列出某个任务的相关信息： !show \"jobs/v2/[jobGroupId]\"; !show \"jobs/[jobGroupId]\"; !show \"jobs/get/[jobGroupId]\"; 三者显示的内容不同，用户可以自己尝试下结果。 列出所有可用的数据源： !show datasources; 列出所有Rest接口： !show \"api/list\"; 列出所有支持的配置参数（不全,以文档为主）: !show \"conf/list\"; 查看日志： !show \"log/[文件偏移位置]\"; 列出数据源的参数： !show \"datasources/params/[datasource name]\"; 列出当前系统资源： !show resource; 列出所有的ET组件： !show et; 列出某个ET组件的信息： !show \"et/[ET组件名称]\"; 列出所有函数： !show functions; 列出某个函数： !show \"function/[函数名称]\"; !hdfs !hdfs 主要用来查看文件系统。支持大部分HDFS查看命令。 查看帮助： !hdfs -help; !hdfs -usage; 下面为一些常见操作： 罗列某个目录所有文件： !hdfs -ls /tmp; 删除文件目录： !hdfs -rmr /tmp/test; 拷贝文件： !hdfs -cp /tmp/abc.txt /tmp/dd; !kill 该命令主要用来杀任务。 !kill [groupId或者Job Name]; !desc 查看表结构。 !desc [表名]; !cache/!unCache 对表进行缓存。 !cache [表名] [缓存周期]; 其中缓存周期有三种选择： script session application 手动释放缓存： !unCache [表名]; !if/!elif/!then/!else/!fi 这五者配合使用，可以实现条件分支语句。参看：MLSQL 支持条件分支语句. !println 打印文本： !println '''文本内容'''; !runScript 将一段文本当做MLSQL脚本执行： !runScript ''' select 1 as a as b; ''' named output; !last 将上一条命令的输出取一个表名，方便后续使用： !hdfs -ls /tmp; !last named table1; select * from table1 as output; !lastTableName 记住上一个表的名字，然后可以在下次获取： select 1 as a as table1; !lastTableName; select \"${__last_table_name__}\" as tableName as output; 输出结果为 table1; !tableRepartition 对标进行分区： !tableRepartition _ -i [表名] -num [分区数] -o [输出表名]; !saveFile 如果一个表只有一条记录，并且该记录只有一列，并且该列是binary格式，那么我们可以将该列的内容保存成一个文件。比 !saveFile _ -i [表名] -o [保存路径]; !emptyTable 比如有的时候我们并不希望有输出，可以在最后一句加这个语句： !emptyTable; !profiler 执行原生SQL: !profiler sql ''' select 1 as a ''' ; 查看所有spark内核的配置： !profiler conf; 查看一个表的执行计划： !profiler explain [表名或者一条SQL]; !python 可以通过该命令设置一些Python运行时环境。 参考: 配置Python以及使用Python代码处理数据 !ray 支持python代码集成。参看 MLSQL Python支持 !delta 显示帮助： !delta help; 列出所有delta表： !delta show tables; 版本历史： !delta history [db/table]; 表信息： !delta info [db/table]; 文件合并： !delta compact [表路径] [版本号] [文件数] [是否后台运行]; !delta compact db/tablename 100 3 background; 上面表示对db/table 100之前的版本的文件进行合并，每个目录只保留三个文件。 !withWartermark 参考 window/watermark的使用 !plugin 插件安装和卸载. 参看：插件 !kafkaTool kafka相关的小工具。参看： MLSQL Kafka小工具集锦 !callback 流式Event事件回调。参看： 如何设置流式计算回调            该文件修订时间： 2021-01-12 10:27:46 "}}